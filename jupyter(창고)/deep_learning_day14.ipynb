{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd099ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머(이어서)\n",
    "# 사실 어려운 개념들이다. 대학원의 연구실에서 발표하고 공부하는 분야이다.\n",
    "# 트랜스포머는 싱글 블랙박스 모델이다. \n",
    "# 인코더와 디코더로 나뉜다. 그리고 요소들은 웨이트를 공유하지 않는 것이 특징이다. \n",
    "# RNN과 LSTM은 웨이트가 공유가 된다. 하지만 여기는 공유되지 않는다. \n",
    "# 어느 단어가 있으면 단어를 인코딩하기 위해 다른 단어들과의 관계를 확인한다.\n",
    "# 피드 포워드 뉴럴 네트워크로 셀프어텐션 단계를 지난 정보들이 들어간다. \n",
    "# 인코더는 셀프어텐션 -> 피드포워드 순이다. 디코더는 셀프어텐션 ->인코더,디코더 어텐션 ->피드포워드 순으로 진행된다.\n",
    "# 각각의 단어가 다른 단어들과의 관계를 확인한다. \n",
    "# 디코더가 가장 관련있는 단어에 집중할 수 있도록 해준다.\n",
    "# 차원은 너무 고차원이다. 공간의 낭비를 줄이기 위해 임베딩을 했다는 사실을 기억할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303c6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt는 1초에 조 단위로 연산을 한다. \n",
    "# 우리가 다루는 연산문제들은 경사하강법을 쓴다. 역행렬을 사용하지 않는다.\n",
    "# gpt같은 경우는 경사하강법을 쓸필요가 없다. 1초당 웨이트 변수들이 수 조개가 변한다. \n",
    "# 이제는 gpt 내부의 변화를 설명할 수 없다. 우리는 어떤 식으로 AI가 구동을 하는지 알 수 없기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b6513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터는 쿼리, 키, 벨류벡터가 얻어졌다. \n",
    "# 검색어와 영상의 연관성은 코사인 유사도로 계산한다. 코사인 유사도 이외에도 다른 유사도 계산방법으로 활용 가능하다.\n",
    "# 포지션 임베딩은 위치에 따라 단어의 뜻을 달리 한다. \n",
    "# '난 널 사랑한다. 그는 사랑하지 않는다.' 라고 하면 사랑이 두 번 나와도 둘의 쓰임새는 아예 다르다. \n",
    "# 밸류값은 맨 마지막에 적재된다.\n",
    "# 멀티헤드는 리니어들을 합치는 방식이다. 셀프 어텐션을 여러 번 할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8dd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트나 gpt나 트랜스포머를 기반으로 만들어진 인공지능이다. \n",
    "# 먼저 훈련된 모델을 사용하는 방식이다. \n",
    "# gpt3.5 부터는 파인튜닝을 한다면 토큰단위로 돈을 받을 것이다. 카드가 연동되어있어야 한다. \n",
    "# 모델을 만들기 위해서는 훈련을 시켜야 한다. 학습과정을 최소로 하면서 사용할 수 있어야 한다. \n",
    "# cls가 먼저 투입될 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d36085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2는 다음 단어를 예측하는 시스템이다. \n",
    "# 인공지능은 퍼러미터 양의 법칙이 적용된다. 퍼러미터가 많으면 대체로 성능이 좋아진다. \n",
    "# 버트는 인코더 부분에 해당하고 GPT는 디코더부분에 해당한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 초기에는 인코더블록은 512개까지 가져올 수 있게 만들어졌다. \n",
    "# 토큰보다 적게 만들어져 있는 경우 패딩이 되어진다. \n",
    "# 디코더 블록같은 경우 인코더의 구조를 살짝 변경해놓은 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c9f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt3는 대규모 언어모델이다. \n",
    "# 퍼러미터의 갯수가 굉장히 많아졌다. \n",
    "# 훈련을 하게되면 훈련된 모델에서는 텍스트를 생성할 것이다. \n",
    "# 선택적으로 생성된 텍스트를 다시 입력으로 넣을 수 있다.\n",
    "# 당연히 출력에 영향을 주게 된다. 생성된 결과를 입력으로 집어넣게 되는 것이다.\n",
    "# 여기서 문제가 발생한다. 정확도가 매우 높은 모델이 있다고 할 때 0.01%의 오류가 있다고 하면 그 오류도 다시 입력을 할 것이다.\n",
    "# 문제는 수행하다보면 0.01%의 정확도가 반복되어버리기에 오류율이 점점 늘어나게 된다. 제곱이 늘어날 수록 증가하는 것이다.\n",
    "# 나중에 값이 0에 가까워지는 심각한 상황이 발생한다. \n",
    "# 즉 이번에는 100%가 나와야 한다는 소리이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcaadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT는 파인튜닝하는 방법이 간단하다. 소위 가스라이팅이라고 부른다. \n",
    "# API키 발급받아서 사용할 수 있다. \n",
    "# 예측된 결과가 사실이 아닐 가능성이 높다. \n",
    "# ai훈련은 질보다 양이다. 엄청난 양의 데이터로 연산을 하는 것이다. \n",
    "# 특히 프로그래밍, 개발자는 이러한 GPT를 정면으로 이길 수는 없다. 초급 개발자로서는 아무것도 할 수 없다. \n",
    "# 데이터를 바라보는 방법을 알아야 한다. 그게 신세대 프로그래머의 소양이다. \n",
    "# 어중간한 코딩은 GPT가 알아서 해 줄 것이다. \n",
    "# 개발자라는 직업이 사라지는 것은 아니지만 GPT를 잘 활용하는 사람. 융화능력이 중요할 것이다.\n",
    "# 화이트칼라 직업일 수록 이러한 현상을 겪기 쉬울 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4bb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt 파인튜닝\n",
    "# 우리가 수행하는 분야에 대해 튜닝을 할 수 있다. 기존에 훈련된 모델을 가지고 특정 업무를 학습한다.\n",
    "# 우리는 먼저 만들어진 모델을 이용해서 학습할 것이다. \n",
    "# https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "# 설명서이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer    \n",
    "from transformers import TFGPT2LMHeadModel\n",
    "# 파인튜닝은 특정 목적을 가지고 한다. 우리가 원하는 목적을 얻기 위해서 하는 것이다.\n",
    "# 특정 문장이나 특정 결과가 생성되는 것을 목적으로 한다. \n",
    "# 허깅페이스를 방문한 적이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\",from_pt=True)\n",
    "# KO-gpt는 한국어 버젼으로 파인튜닝 한 것이다. 한국어 GPT2 모델이다. \n",
    "# 사실 나온지 2년밖에 되지 않았다. \n",
    "# BPE를 배운 적이 있을 것이다. OOV문제를 해결하기 위한 과정에서 설명했었다.\n",
    "# 이런것도 사용이 된다. \n",
    "# 파이토치로 구현된 것이 많다. 코드 내부의 수정을 위해서 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "# 토크나이저는 한 세트이다. 하나의 묶음이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='프로젝트를 성공적으로 수행하기 위해서는'\n",
    "# gpt에게 물어볼 내용을 적어보자. \n",
    "# 인코딩작업을 해야한다. 토크나이저가 할 일이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(sent) \n",
    "# 리스트로 인코딩 되었을 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f338de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor([input_ids])\n",
    "# 주의사항은 2차원 구조로 만들어야 한다. 그래야 투입을 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=128, repetition_penalty=2.0,use_cache = True)\n",
    "# 문장을 생성하는 명령이다. \n",
    "# 기존의 언어 투입이 첫번째이다.\n",
    "# 최대 길이 출력의 길이이다. \n",
    "# 리피티션 패널티는 한번 나온 단어를 다시 나오지 않게 하는 속성이다. 기본값이 1이다. 작게 하면 반복적인 말이 자주 나온다. \n",
    "# 캐시 사용을 활성화하였다. \n",
    "# 아웃풋은 텐서로 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = output.numpy().tolist()[0]# 단일 리스트화 시켜보자. 토크나이저의 디코드 함수로 전달하기 위한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output_ids) # 출력을 해봤다. 신문기사가 출력되었다. 앞에 있는 단어로 다음 단어를 유추한 것이다. \n",
    "# 처음 훈련시킨 상태에서는 대화를 하지 않는다. 그저 과거의 질문을 기억해내는 것 뿐이다. \n",
    "# 그리고 이전 글과 다음 글은 이어지지 않을 것이다. 상당히 골치아프다. \n",
    "# 질문에 대한 답변이 많은데 그 중에 하나를 선택한다. 즉 확률로 계산이 되어진다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids)\n",
    "output.logits\n",
    "# (1,6,51200)이 출력된다. 만약 맨 마지막에 있는 것을 출력하기 위해서는\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits[0,-1] # 이런 식으로 출력할 수 있을 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10=tf.math.top_k(output.logits[0,-1],k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d23045",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(top10.indices.numpy())\n",
    "# 탑 10에 해당하는 다음 연관단어가 나왔다. \n",
    "# 다음 단어에 들어올 것을 유추해서 연결하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f734b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '인공지능 전문가가 되기 위해서'\n",
    "input_ids = tokenizer.encode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9586611",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(input_ids)<50):\n",
    "     output = model(np.array([input_ids]))           # 아웃풋을 모델로부터 얻는다. \n",
    "     top5 = tf.math.top_k(output.logits[0, -1], k=5) # 출력결과에서 상위 5개의 단어를 가지고 온다. \n",
    "     token_id = random.choice(top5.indices.numpy())  # 다음 단어를 5개 중에 랜덤하게 선택한다. \n",
    "     input_ids.append(token_id)                      # 토큰아이디를 리스트에 추가한다. \n",
    "tokenizer.decode(input_ids)                          # 디코드를 한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt3.5는 2022년 1월까지 정보를 가지고 있다. 이후에는 정보가 존재하지 않는다.\n",
    "# 그러한 데이터들을 업데이트 할 수 있다. \n",
    "# 파인튜닝을 하기 위해서 기존의 학습된 모델을 특정한 데이터셋을 이용해서 학습을 진행하라고 한다. \n",
    "# 즉 형식을 맞춰야 한다. 그렇다면 원하는 종류의 챗봇이 될 수 있다. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e530b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AI 사이트에서 API를 선택할 수 있다. 파인튜닝을 선택할 수 있다.\n",
    "# 파인튜닝 설명서도 다 나와있다.\n",
    "# 1. 모델을 선택한다.\n",
    "# 2. 훈련 데이터셋을 준비한다.\n",
    "# 3. 데이터셋은 role user content, role system content와 같이 특정 구조를 가지고 있다. \n",
    "#    시스템은 시스템의 특성을 결정하는 칸이다. 역할부여도 가능하다.\n",
    "#    최근에는 데이터가 10건 이상 반복되어야지 파인튜닝이 되도록 설계되어있다. \n",
    "#    그래서 jsonl형식으로 저장해서 jsonl파일을 올려서 파인튜닝을 한다. \n",
    "#    Json형식으로 되어있다. 키와 밸류값으로 구성되어있다. 정확하게는 Jsonl형식으로 저장해야한다. \n",
    "# 4. 파일 내부엔 프롬프트라고 해서 있다. 새로 업로드 하거나 기존것을 업로드 할 수도 있다. \n",
    "# 5. API키는 카드번호와 같다. 잘못 유출되면 큰일난다. 키는 파인튜닝을 하기 위해 선결제를 해야 한다. 그 돈이 차감되면서 파인튜닝이 이루어진다.\n",
    "# https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "# 번역기 돌리고 코랩에서 파인튜닝을 진행하자. \n",
    "# https://wikidocs.net/200282\n",
    "# 파인튜닝 설명서이다. \n",
    "# 하나의 입력과 해당되는 출력으로 구성된 학습예제세트가 필요하다. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41fde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
