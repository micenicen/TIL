{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce18a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 2일차\n",
    "# 오늘은 파이토치. 버트 파인튜닝을 배울 것이다. \n",
    "# 텐서플로는 이후 정리 예정이다. \n",
    "# 웹 어플리케이션은 html, javascript, jango,flask등을 공부할 것이다. \n",
    "# 깃헙에 포트폴리오 관리를 할 때에는 이력서, 회사 제출할 때 우리가 했던 것을 보여줄 수 있어야 한다. \n",
    "# GAN, 자연어처리를 특강기에 할 예정이다. 할 것이 많긴 하다. \n",
    "# SQL쪽도 크루드도 할 듯 하다. \n",
    "# 기업은 GPT를 자체적으로 제작/활용하는 것을 중요시한다. \n",
    "# 프롬프트 엔지니어링은 CNN,RNN을 기본으로 알아야 한다. \n",
    "# 정성적 평가에서 최고기술자에 대해서 동적 계획법을 이해하고 동적 계획법을 다룰 수 있는 사람을 말한다. \n",
    "# 즉 알고리즘을 잘 다뤄야 한다. \n",
    "# 강화학습은 맞는 분야가 존재한다. 즉각적인 대응이 필요한 분야에 대해서 강화학습을 하는 것이다. \n",
    "# 어디에다 바둑알을 두었을 경우 대국에서 이길 확률이 높은가를 계산하는 것이다.\n",
    "# 그래서 빠른 대응이 필요한 곳에 필요하다. 농업에는 적절한지는 의문이다. \n",
    "# 코랩과 연동해서 진행할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3caeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치는 기본적으로 클래스를 구현하여 모델을 제작한다. \n",
    "# 파이토치 관련 코드를 보면 이러한 경우가 상당히 많이 있을 것이다. \n",
    "# 지난 시간에는 단순 선형 모델을 이용해서 만들었을 것이다. 인풋 - 출력 - 아웃풋 디멘젼 순으로 제작했을 것이다. \n",
    "# 그러한 모델을 클래스로 구현해보는 것이다. \n",
    "# 규칙은 nn.모듈을 상속받는 것 부터 시작한다. \n",
    "# 객체지향적 언어프로그래밍의 핵심 요소가 상속이다. 물려주는 것을 말한다. 부모가 자식에게 물려주는 것이다.\n",
    "# 클래스에도 부모가 있고 자식이 있다. \n",
    "# 부모 클래스와 자식 클래스가 있고 둘 사이의 관계는 상속관계가 있다. \n",
    "# 예를 들어 붕어빵 기계(클래스)를 잉어빵 기계로 만들려고 한다. \n",
    "# 붕어빵 기계를 만들었던 템플릿을 버리고 잉어빵 기계의 템플릿을 버리기에는 시간, 비용, 연구인력 등 많은 것을 소모한다.\n",
    "# 상속은 이러한 문제를 해결하기 위해 만들어졌다. 부모 클래스(붕어빵 기계)와 자식 클래스(잉어빵 기계)는 상속관계이다.\n",
    "# 클래스는 내부 클래스라고 한다. 이 것을 물려주는 행위를 상속이라고 한다. \n",
    "# 크기, 너비, 높이, 내용물, 동작 등등 속성과 함수등 여러 특징들이 존재한다. 그 특징을 바꾸는 것이다.\n",
    "# 부모 클래스의 디폴트 세팅을 바꿔서 자식 클래스는 다른 기능을 가질 수 있게 된다. \n",
    "# 붕어빵 기계를 만든 이후 여러 번에 걸쳐 유지보수를 했을 것이다.(릴리즈)\n",
    "# 개발 방법론중에 에자일(agile)기법이 있다. 완성도가 떨어져도 빠르게 만드는 것이다. 바로 배포하는 것이다. (time to market)\n",
    "# 에자일 기법은 배포를 빠르게 하고 순환적으로 설계/개발을 반복하는 것이다. \n",
    "# 기능을 반복해서 개선하는 것으로 보면 된다. \n",
    "# 소프트웨어는 요구사항을 분석하고 설계하고 구현하고 테스트하고 배포하고 유지보수하는 단계를 진행한다. \n",
    "# 유지보수가 60~70%정도를 소모한다. 개발에서 쓰는 자원은 생각보다 얼마 안된다.\n",
    "# 그래서 유지보수할 때 비용이 적게 들도록 설계를 하게 된다. 설계가 중요한 이유이다. 확장성을 고려해서 설계해야 한다. \n",
    "# 확장하기에 용이한 방향으로 설계해야한다. (추상화) \n",
    "# 객체지향 설계는 추상화가 굉장히 중요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e078b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 설계를 프로젝트를 하나 만들었다. 동물 클래스이다. \n",
    "# 동물 ----파충류 -- 거북이 \n",
    "#                 --     뱀\n",
    "#        --양서류 -- 개구리\n",
    "#        --포유류 -- 호랑이\n",
    "#        --조류   -- 까마귀\n",
    "# 위로 올라갈수록 조상이라고 생각하면 된다. 가장 높은 클래스(루트 클래스, 자바 오브젝트 클래스)가 동물이다. \n",
    "# 속성들은 밑으로 계속 전달된다. 하지만 동물이라는 공통적인 특징을 가지는 것은 계속 상속받는다. \n",
    "# 다만 여기서 공부한다라는 속성을 동물에 부여하면 개구리와 호랑이는 대학에 다녀야 한다.\n",
    "# 코를 골며 자면 고래는 바다에서 바로 죽었을 것이다.\n",
    "# 상위 클래스로 올라가면 올라갈 수록 추상적이게 특징이 정해진다. 그리고 밑으로 내려갈수록 구체화 된다. \n",
    "# 동사형은 대부분 함수에 해당한다. 매서드이다. 여기서 매서드는 자식클래스로 갈수록 매서드가 달라진다. \n",
    "# 이걸 매서드 재정의(overRiding)이라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf859e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f4918db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스는 먼저 작성하고 괄호를 치고 부모 클래스를 기술할 수 있다. \n",
    "# 자녀가 모듈 클래스인 리니어 회귀모델에 상속되었을 것이다. \n",
    "class LinearRegressionModel(nn.Module):  # 함수를 생성한다. \n",
    "    def __init__(self):                  # init 함수는 LinearRegressionModel 객체가 생성되어지는 시점에 자동으로 호출되는 함수이다. \n",
    "                                         # 생성자는 객체가 생성자가 만들어질 당시 객체가 가지는 특징을 초기화하는 목적을 가진다. \n",
    "        super().__init__()               # super는 부모객체를 말한다. nn.module이 super이다. 부모객체도  __init__가 있을 것이다. \n",
    "                                         # 여기서 부모 클래스 객체가 만들어졌다. nn.Module안에도 __init__가 있다. \n",
    "                                         # 부모클래스가 가진 속성으로 초기화 하는 것이다. 즉 초기화 조건이 다른 클래스의 초기조건이다. \n",
    "#        print('호출됨')                 # 호출된 것을 확인하기 위한 용도의 프린트 함수  \n",
    "        self.linear = nn.Linear(1,1)     # 입력 디멘전은 1, 출력 디멘전도 1이다. 단순 선형회귀모델이다. \n",
    "                                         # 만약 다중선형회귀일 경우 1,1을 다르게 바꾸면 된다. \n",
    "    def foward(self,x):                  # 데이터를 전달받음과 동시에\n",
    "        return self.linear(x)            # 데이터를 호출하는 것이다.  리니어라는 구조에 따라 [1,1]이 입출력 될 것이다. \n",
    "                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4f112b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " LinearRegressionModel()                # 생성자 매서드가 호출되었다. \n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1086fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]]) # 입력데이터는 1차원 구조이고 데이터가 3건이 존재한다. \n",
    "y_train = torch.FloatTensor([[2], [4], [6]]) # 출력데이터는 입력값에 대한 예측값이다. 정답값으로 1차원 구조이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b544f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()              # 모델을 생성했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f8bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(                 # 점을 찍었을 때 다른 것들이 있다. 그것들은 상속받은 것이다. \n",
    "    model.parameters(),lr=0.01)              # 즉 선대 조상으로부터 받았기에 점을 찍고 탭을 누르면 여러 가지가 나온 것이다.             \n",
    "                                             # 퍼러미터는 경사하강법을 말한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e770a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 14.385373\n",
      "Epoch  100/2000 Cost: 0.009960\n",
      "Epoch  200/2000 Cost: 0.006155\n",
      "Epoch  300/2000 Cost: 0.003803\n",
      "Epoch  400/2000 Cost: 0.002350\n",
      "Epoch  500/2000 Cost: 0.001452\n",
      "Epoch  600/2000 Cost: 0.000897\n",
      "Epoch  700/2000 Cost: 0.000555\n",
      "Epoch  800/2000 Cost: 0.000343\n",
      "Epoch  900/2000 Cost: 0.000212\n",
      "Epoch 1000/2000 Cost: 0.000131\n",
      "Epoch 1100/2000 Cost: 0.000081\n",
      "Epoch 1200/2000 Cost: 0.000050\n",
      "Epoch 1300/2000 Cost: 0.000031\n",
      "Epoch 1400/2000 Cost: 0.000019\n",
      "Epoch 1500/2000 Cost: 0.000012\n",
      "Epoch 1600/2000 Cost: 0.000007\n",
      "Epoch 1700/2000 Cost: 0.000005\n",
      "Epoch 1800/2000 Cost: 0.000003\n",
      "Epoch 1900/2000 Cost: 0.000002\n",
      "Epoch 2000/2000 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model.foward(x_train)     # 학습데이터이다. 전달받아서 포워드 연산을 진행하게 된다. \n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))                            \n",
    "# 어제랑 매커니즘이 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5049704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 한줄을 관측치라고 한다. 샘플이라고도 한다. 샘플은 5개이다. \n",
    "# 인스턴스라고도 부르기도 한다. \n",
    "# 전체 데이터는 5건이 있다. \n",
    "# 만약 데이터 규모가 크면 전체 데이터에 대한 그레디언트 디센트 알고리즘을 수행한다면 \n",
    "# 메모리 부족이라든지 결과가 한참 걸릴수도 있다. 그래서 데이터를 나누어 처리하는 것을 미니배치하고 한다. \n",
    "# 대규모의 데이터를 작은 단위로 나누어 연산하는 것이다. \n",
    "# 배치사이즈는 한번에 학습하기 위한 데이터를 말한다. \n",
    "# 미니배치의 갯수만큼 경사하강법이 수행된다. \n",
    "# 예를 들어 데이터가 100건이 있다고 하자. 1에폭은 100건의 데이터를 모두 학습한 것을 말한다. \n",
    "# 배치사이즈를 20으로 설정했다. 배치사이즈는 한번에 20건의 데이터를 읽어서 학습하는 것을 말한다. \n",
    "# 5미니배치는 20배치사이즈로 데이터를 5번 가져와서 학습했다는 뜻이다. 총 100건의 데이터학습이 이루어졌다. \n",
    "# 배치의 수는 미니배치를 얼마로 설정했느냐에 따라 다르다. 경사하강법은 5번이 이루어졌다. \n",
    "# 문제는 한번 계산할 때마다 계산시간이 어마어마하게 들어간다. 백프라퍼게이션을 하면서 일일히 미분 업데이트를 하는것이다.\n",
    "# 여기서 20개의 데이터에 대해 한꺼번에 오차를 구하는 것이다. 만약 배치사이즈가 1이면 1개의 데이터마다 오차를 구할 것이다. \n",
    "# 용어중에 반복적으로 데이터를 가져오는 것을 이터레이션이라고 말한다. 1에폭에서 발생하는 W와 b의 업데이트 횟수를 말한다.\n",
    "# 한번에 20개씩의 데이터를 읽어서 학습하는 것이기에 매개변수 업데이트는 5번 일어난다. 전체 데이터/배치사이즈 이다. \n",
    "# 전체 데이터에 대해 한번에 배치와 함께 업데이트를 수행하는 경우를 배치학습이라고 한다. \n",
    "# 미니배치 학습은 전체 데이터에 대해 여러 번 배치를 하여 여러 번 업데이트를 하는 것을 말한다. \n",
    "# 우리는 미니배치 학습을 보고있는 것이다. 일반적으로 데이터가 대규모라 배치학습은 잘 안한다. 미니배치학습을 한다. \n",
    "#\n",
    "# 파이토치에는 데이터셋(DataSet)과 데이터로더(DataLoader)라고 하는 것이 있다. \n",
    "# 이것들을 이용해서 데이터를 쉽게 배치하고 학습할 수 있다. (셔플, 미니배치 학습, 병렬처리등의 연산을 빠르게 할 수 있다.)\n",
    "# 텐서 형태로 데이터를 받아서 데이터셋으로 변환하는 것을 먼저 시행해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c487aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a374093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dca592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a95a5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 15517.335938\n",
      "Epoch  100/2000 Cost: 2.525305\n",
      "Epoch  200/2000 Cost: 2.401519\n",
      "Epoch  300/2000 Cost: 2.284253\n",
      "Epoch  400/2000 Cost: 2.173163\n",
      "Epoch  500/2000 Cost: 2.067915\n",
      "Epoch  600/2000 Cost: 1.968257\n",
      "Epoch  700/2000 Cost: 1.873792\n",
      "Epoch  800/2000 Cost: 1.784342\n",
      "Epoch  900/2000 Cost: 1.699620\n",
      "Epoch 1000/2000 Cost: 1.619332\n",
      "Epoch 1100/2000 Cost: 1.543295\n",
      "Epoch 1200/2000 Cost: 1.471278\n",
      "Epoch 1300/2000 Cost: 1.403039\n",
      "Epoch 1400/2000 Cost: 1.338388\n",
      "Epoch 1500/2000 Cost: 1.277154\n",
      "Epoch 1600/2000 Cost: 1.219148\n",
      "Epoch 1700/2000 Cost: 1.164209\n",
      "Epoch 1800/2000 Cost: 1.112142\n",
      "Epoch 1900/2000 Cost: 1.062835\n",
      "Epoch 2000/2000 Cost: 1.016131\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca6db0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로드 작업\n",
    "# 텐서 데이터셋, 데이터로드를 볼 것이다. \n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더ㅁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c9718b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2043c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train,y_train)  # 텐서 데이터를 전달받아 데이터셋 형태로 변환해주는 것이다. \n",
    "                                          # 데이터셋의 정확한 의미는 텐서플로에서 데이터들을 좀더 쉽게 병렬처리/셔플을 할 수 있게\n",
    "                                          # 3차원 텐서와 1차원 텐서를 특별한 형태로 변환시킨 것이다. \n",
    "                                          # 이제 데이터 로더를 사용할 수 있다. 텐서 데이터를 읽어서 데이터로드를 사용할 수 있다. \n",
    "                                          # 미니배치의 크기를 지정해 줄 수 있다. \n",
    "                                          # 훈련 데이터를 가지고 트레이닝을 할 때 셔플옵션으로 데이터를 섞어줄 수 있다. \n",
    "                                          # 시계열 데이터는 절대 셔플하면 안된다. 일반적인 데이터에는 섞는 것이 낫다. \n",
    "                                          # 매 에폭마다 섞어버린다. 배치단위로 섞기도 한다. 그것이 성능이 더 좋다. \n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "682ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=2,shuffle=True)  # 데이터셋을 디폴트가 1이다. 배치사이즈는 정수로 입력이 가능하다. 셔플은 안한다. \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad7921fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49b70edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 22263.300781\n",
      "Epoch    0/20 Batch 2/3 Cost: 7324.471680\n",
      "Epoch    0/20 Batch 3/3 Cost: 2734.815430\n",
      "Epoch    1/20 Batch 1/3 Cost: 526.284668\n",
      "Epoch    1/20 Batch 2/3 Cost: 298.402924\n",
      "Epoch    1/20 Batch 3/3 Cost: 75.410629\n",
      "Epoch    2/20 Batch 1/3 Cost: 29.897415\n",
      "Epoch    2/20 Batch 2/3 Cost: 0.275766\n",
      "Epoch    2/20 Batch 3/3 Cost: 3.261801\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.239504\n",
      "Epoch    3/20 Batch 2/3 Cost: 7.676706\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.437944\n",
      "Epoch    4/20 Batch 1/3 Cost: 5.092645\n",
      "Epoch    4/20 Batch 2/3 Cost: 2.564964\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.115805\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.325948\n",
      "Epoch    5/20 Batch 2/3 Cost: 0.484260\n",
      "Epoch    5/20 Batch 3/3 Cost: 11.555705\n",
      "Epoch    6/20 Batch 1/3 Cost: 4.613418\n",
      "Epoch    6/20 Batch 2/3 Cost: 2.473148\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.007362\n",
      "Epoch    7/20 Batch 1/3 Cost: 4.886775\n",
      "Epoch    7/20 Batch 2/3 Cost: 0.501963\n",
      "Epoch    7/20 Batch 3/3 Cost: 1.313272\n",
      "Epoch    8/20 Batch 1/3 Cost: 5.564356\n",
      "Epoch    8/20 Batch 2/3 Cost: 0.931730\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.139153\n",
      "Epoch    9/20 Batch 1/3 Cost: 5.593845\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.915278\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.134815\n",
      "Epoch   10/20 Batch 1/3 Cost: 5.432393\n",
      "Epoch   10/20 Batch 2/3 Cost: 1.871661\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.793458\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.189683\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.541753\n",
      "Epoch   11/20 Batch 3/3 Cost: 11.801200\n",
      "Epoch   12/20 Batch 1/3 Cost: 3.564377\n",
      "Epoch   12/20 Batch 2/3 Cost: 2.467189\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.521573\n",
      "Epoch   13/20 Batch 1/3 Cost: 5.005827\n",
      "Epoch   13/20 Batch 2/3 Cost: 1.153988\n",
      "Epoch   13/20 Batch 3/3 Cost: 1.008134\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.259810\n",
      "Epoch   14/20 Batch 2/3 Cost: 0.514745\n",
      "Epoch   14/20 Batch 3/3 Cost: 11.579370\n",
      "Epoch   15/20 Batch 1/3 Cost: 3.165733\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.557637\n",
      "Epoch   15/20 Batch 3/3 Cost: 9.541760\n",
      "Epoch   16/20 Batch 1/3 Cost: 3.845111\n",
      "Epoch   16/20 Batch 2/3 Cost: 3.188715\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.832946\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.605067\n",
      "Epoch   17/20 Batch 2/3 Cost: 5.866021\n",
      "Epoch   17/20 Batch 3/3 Cost: 0.018002\n",
      "Epoch   18/20 Batch 1/3 Cost: 0.854021\n",
      "Epoch   18/20 Batch 2/3 Cost: 5.179920\n",
      "Epoch   18/20 Batch 3/3 Cost: 0.725965\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.932016\n",
      "Epoch   19/20 Batch 2/3 Cost: 5.892582\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.003100\n",
      "Epoch   20/20 Batch 1/3 Cost: 4.596215\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.492987\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.321300\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "            cost.item()\n",
    "            ))\n",
    "# 20배치를 도는데 3번의 배치가 도는 것이다. 5건의 데이터 중 2건씩 쓰기에 나머지 1건은 0번 인덱스 것을 쓰거나 랜덤하게 쓴다. \n",
    "# 전체적으로 봤을 때는 떨어지는 추세인 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ba3cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var =  torch.FloatTensor([[73, 80, 75]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f97588e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model(new_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d8d7428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[148.8328]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee0612bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d514237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터셋\n",
    "# 우리가 가진 데이터를 이용하여 커스텀 데이터를 만드는 경우도 종종 있다.\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):                    # 데이터를 전처리하는 과정이다. \n",
    "\n",
    "    def __len__(self):                     # 데이터의 길이와 데이터의 갯수를 말한다. \n",
    "\n",
    "    def __getitem__(self, idx):            # 데이터 1개를 리턴해주는 기능이다.\n",
    "# 부모클래스가 물려준 함수를 내가 원하는 방식으로 재정의(오버라이딩)를 하여 구현한다. \n",
    "# 기존에 코드를 작성했던 방식과 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2bc4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):                              # 데이터셋 클래스를 상속받아서 만든 클래스이다. 매우 추상적이다. \n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]  # 그냥 처음부터 데이터를 투입하였다. \n",
    "\n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):                                     # 데이터의 갯수를 확인하였다. \n",
    "        return len(self.x_data)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.x_data[idx])            # 바로 바꿔버린다. \n",
    "        y = torch.FloatTensor(self.y_data[idx])            # \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "964b966f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x2148a7e68d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset()                                  # 실행하는 순간 'Dataset'클래스에 함수를 상속받는다. \n",
    "                                                           # 여기는 클래스가 구체회된다. \n",
    "                                                           # 위의 Dataset은 추상 메서드이다. 수행하는 것이 추상적이다.\n",
    "                                                           # 동작에 대해서는 추상적이고 새로 정의된 함수에서 구체화되는 구조이다.\n",
    "                                                           # 추상클래스는 추상 메서드로 구성된 클래스를 말한다. \n",
    "                                                           # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c86de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc4f4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5709251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 45416.046875\n",
      "Epoch    0/20 Batch 2/3 Cost: 13926.115234\n",
      "Epoch    0/20 Batch 3/3 Cost: 7262.937500\n",
      "Epoch    1/20 Batch 1/3 Cost: 1121.990845\n",
      "Epoch    1/20 Batch 2/3 Cost: 311.391998\n",
      "Epoch    1/20 Batch 3/3 Cost: 131.426193\n",
      "Epoch    2/20 Batch 1/3 Cost: 43.081085\n",
      "Epoch    2/20 Batch 2/3 Cost: 14.055125\n",
      "Epoch    2/20 Batch 3/3 Cost: 0.139301\n",
      "Epoch    3/20 Batch 1/3 Cost: 4.553526\n",
      "Epoch    3/20 Batch 2/3 Cost: 6.146994\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.403586\n",
      "Epoch    4/20 Batch 1/3 Cost: 2.954335\n",
      "Epoch    4/20 Batch 2/3 Cost: 5.038210\n",
      "Epoch    4/20 Batch 3/3 Cost: 1.076577\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.404117\n",
      "Epoch    5/20 Batch 2/3 Cost: 7.896728\n",
      "Epoch    5/20 Batch 3/3 Cost: 5.531641\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.093210\n",
      "Epoch    6/20 Batch 2/3 Cost: 8.941119\n",
      "Epoch    6/20 Batch 3/3 Cost: 4.888919\n",
      "Epoch    7/20 Batch 1/3 Cost: 5.877522\n",
      "Epoch    7/20 Batch 2/3 Cost: 1.796899\n",
      "Epoch    7/20 Batch 3/3 Cost: 3.478986\n",
      "Epoch    8/20 Batch 1/3 Cost: 3.318721\n",
      "Epoch    8/20 Batch 2/3 Cost: 5.276530\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.923929\n",
      "Epoch    9/20 Batch 1/3 Cost: 2.029536\n",
      "Epoch    9/20 Batch 2/3 Cost: 4.532216\n",
      "Epoch    9/20 Batch 3/3 Cost: 4.394083\n",
      "Epoch   10/20 Batch 1/3 Cost: 3.173558\n",
      "Epoch   10/20 Batch 2/3 Cost: 5.232466\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.910717\n",
      "Epoch   11/20 Batch 1/3 Cost: 5.142117\n",
      "Epoch   11/20 Batch 2/3 Cost: 2.113605\n",
      "Epoch   11/20 Batch 3/3 Cost: 4.803331\n",
      "Epoch   12/20 Batch 1/3 Cost: 4.029474\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.307208\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.217780\n",
      "Epoch   13/20 Batch 1/3 Cost: 2.720594\n",
      "Epoch   13/20 Batch 2/3 Cost: 4.769362\n",
      "Epoch   13/20 Batch 3/3 Cost: 1.251963\n",
      "Epoch   14/20 Batch 1/3 Cost: 1.113295\n",
      "Epoch   14/20 Batch 2/3 Cost: 2.922723\n",
      "Epoch   14/20 Batch 3/3 Cost: 9.385823\n",
      "Epoch   15/20 Batch 1/3 Cost: 4.964393\n",
      "Epoch   15/20 Batch 2/3 Cost: 1.782483\n",
      "Epoch   15/20 Batch 3/3 Cost: 7.748513\n",
      "Epoch   16/20 Batch 1/3 Cost: 3.796657\n",
      "Epoch   16/20 Batch 2/3 Cost: 4.365985\n",
      "Epoch   16/20 Batch 3/3 Cost: 8.375484\n",
      "Epoch   17/20 Batch 1/3 Cost: 2.812106\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.418007\n",
      "Epoch   17/20 Batch 3/3 Cost: 10.779423\n",
      "Epoch   18/20 Batch 1/3 Cost: 3.794390\n",
      "Epoch   18/20 Batch 2/3 Cost: 4.799230\n",
      "Epoch   18/20 Batch 3/3 Cost: 3.480238\n",
      "Epoch   19/20 Batch 1/3 Cost: 3.915905\n",
      "Epoch   19/20 Batch 2/3 Cost: 6.491597\n",
      "Epoch   19/20 Batch 3/3 Cost: 2.641418\n",
      "Epoch   20/20 Batch 1/3 Cost: 1.659396\n",
      "Epoch   20/20 Batch 2/3 Cost: 2.285486\n",
      "Epoch   20/20 Batch 3/3 Cost: 10.729983\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "            cost.item()))\n",
    "# 이게 파이토치의 기본 문법이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "877e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 파인튜닝\n",
    "# https://wikidocs.net/book/2155\n",
    "# 한국어임베딩을 저술한 저자는 블로그로 유명하다. \n",
    "# 구조에 대한 설명을 상당히 자세히 설명한다. \n",
    "# 실무에서 적용하기 위해서는 파인튜닝을 반드시 거쳐야 한다. 하고자 하는 업무, 작업등을 고려하여 파인튜닝을 하는 것이다.\n",
    "# 파인튜닝은 대규모 택스트데이터를 학습시킨 거대언어모델(LLM)을 이용한다.\n",
    "# 훈련방식은 다음 단어를 예측하는 방식으로 언어모델을 만든 것이다.\n",
    "# 모델을 그대로 사용한다면 두 문장을 입력을 받아서 유사한 정도를 구해서 서로 관련이 있는지 없는지를 분류하고 싶은 것이다.\n",
    "# 두 문장의 유사도를 구할 때 기존에는 유클리디안, 코사인유사도 등을 사용해왔다.\n",
    "# 초 거대 언어모델을 사용해서 두 문장의 유사도를 구하는 것에는 기존 방식은 알맞지 않았다.\n",
    "# 마스크드 언어모델이 있었고(MLM) 두 문장을 비교하는 언어모델이 있었다.(NLP)\n",
    "# 그리고 원래 목적은 스팸메일을 걸러내기 위한 목적으로 만든 모델도 아니다. \n",
    "# 레이블이 없는 모델을 가지고 오되 레이블이 있는 다른 작업에서 추가 훈련과 함께 하이퍼퍼러미터를 재조정한다.\n",
    "# 레이블이 없는 모델가지고 레이블이 있는 데이터를 학습시켰는데 모델의 성능이 더 좋아지는 것이다. \n",
    "# 모델 위에 분류작업을 위한 신경망을 추가하여 굉장히 분류를 잘하게 만든 것이다. \n",
    "# 이미 가지고 있는 지식을 기반으로 스팸 메일을 분류하기 때문에 성능이 더 좋아진 것이다. \n",
    "# 트랜스포머의 구조는 인코더와 디코더, 어텐션헤드, 피드포워드 뉴럴네트워크가 들어가있다.\n",
    "# 디코더에는 마스크드 멀티헤드가 들어있다. \n",
    "# 버트는 문맥을 반영할 수 있다. \n",
    "# 책에 나와있는 것은 버트 모델을 응용할 수 있게 만든 것이다. \n",
    "# klue/bert-base는 한국어로 사용할 수 있는 버트 모델이다. 트랜스포머 패키지를 받은 이후 사용 가능하다.\n",
    "# from transformers import TFBertForMaskedLM\n",
    "# from transformers import AutoTokenizer\n",
    "# 불러온 다음에 다음과 깉이 입력하면\n",
    "# model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# 한국어 기본 버트를 불러와서 사용이 가능하다. \n",
    "# 모델은 서로 매핑관계를 유지하고 있어야 한다. 그것은 항상 핵심이다. 서로가 항상 한 쌍이라는 것은 계속 강조한다.\n",
    "# 입력을 하는 것은 다음과 같이 할 수 있다. \n",
    "# 센턴스버트는 문장 임베딩을 얻을 수 있는 버트이다. \n",
    "# 문장벡터를 얻는 방법은 여러가지가 존재하지만 사전학습된 모델에 CLS 토큰의 출력벡터를 이용한다. \n",
    "# CLS 토큰은 출력벡터의 평균을 매겨서 풀링하게 된다. 평균 풀링(mean polling)을 통해 평균을 구하는 것이다.\n",
    "# 센턴스버트는 문장 임베딩 성능을 우수하게 개선시킨 모델이다. \n",
    "# 센턴스버트는 문장을 두개를 줘서 유사도를 측정하는 방식을 취한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트는 굉장히 무거우면서 학습속도가 느리다. 그래서 TPU를 쓰는 경우가 일반적이다.\n",
    "# TPU는 코드설정을 따로 해야하는 번거로움이 존재한다. 맨 처음에 사용해야 한다. \n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# TPU를 쓰기 위해 이 코드를 실행시켜야 하는 번거로움이 존재한다. 항상 상수처럼 사용해야 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e297c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경설정 끝나고 입력해야한다.\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "# CPU는 기존 계산을 하는데 쓰였다. 그리고 그래픽, 병렬계산을 목적으로 GPU를 사용하게 되었다. \n",
    "# 연산장치가 수천개나 되기에 GPU는 연산이 매우 빠르다.\n",
    "# TPU는 텐서 프로세싱 유닛이다. 구글이 만든 칩이다. 벡터나 텐서에 최적화된 특징을 가진다. \n",
    "# 즉 위의 코드들은 분산처리와 관련된 API이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80629401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():                # 함수에 아예 시퀸셜을 넣은 것이다. \n",
    "    return tf.keras.Sequential(\n",
    "        [tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)])\n",
    "# 시퀸셜을 생성하고 시퀸셜 객체가 리턴된다. 지금 모델이 리턴된 것이다.\n",
    "# 모델이 생긴것을 보아하니 CNN모델이다. \n",
    "# 기존에는 model = Sequential()과 같이 작성했다.  사실 위와 같이 작성하는것도 가능하다. \n",
    "# 리스트로 담아버린 다음 시퀸셜 객체로 만들어서 리턴한 것이다. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():          # 함수를 불러오는 동시에 모델 컴파일을 하게 된다. TPU사용할 때는 이런 방식으로 사용한다. \n",
    "  model = create_model()        # TPU를 사용할 때에는 컴파일 할때 with구문을 써야한다. \n",
    "  model.compile(optimizer='adam',# 모델 제작, 컴파일을 동시에 한다는 뜻이다. TPU를 사용할 때 통용되는 약속이다. \n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "# 위의 줄은 일종의 분산학습을 위해서 사용한다. \n",
    "# 일반적으로 모델은 3가지로 나온다. 다대 일, 다대 다 , 질의응답유형 등 3가지로 나뉜다. \n",
    "# 재밌는 것은 일반적인 명사가 아닌 경우 해석이 어려울 수 있다. \n",
    "# 문장속의 특정 단어에 대해 개체로서 인식이 되어질 수 있도록 하는 기법이 개채명 인식기술이다. \n",
    "# OKT, morphs를 썼던 기억이 날 것이다. 여기서 이름을 인식하기 어려울 것이다. \n",
    "# 파인튜닝을 잘 하면 일대 일 / 다대 다 분류기를 만들어낼 수 있다. \n",
    "# 사실 질의응답과 다대 일 유형이 가장 많이 쓰인다. 파인튜닝을 통해 기계독해에 활용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 트랜스포머를 설치하고 시작하자. \n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "# 이미 두 개의 파일로 제작된 것을 불러온 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee301948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력 - 15만개\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력 - 5만개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5] # 아이디는 쓸데없고 다큐멘트는 내용, 라벨은 긍정과 부정으로 나눠져있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c694117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')    # Null 값이 존재하는 행 제거\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.isnull().values.any())         # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3649e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05227fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 건의 데이터가 삭제된 것을 알 수 있다.\n",
    "# 중복 데이터 삭제작업도 해야 할 것이다. \n",
    "# 버트 토크나이저를 가져와보자. \n",
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# 다운로드가 진행될 것이다. \n",
    "# 한국어 모델은 다양한 기관으로부터 연구가 되었다. \n",
    "# 처음에는 대규모 데이터를 이용한 자연어처리 연구가 많이 진행되었다.\n",
    "# 방대한 말뭉치 데이터를 이용하여 비지도학습을 진행하여 만들어졌다. \n",
    "# 외국어는 언어모델 연구가 매우 활발하다. 한국은 그렇지 않다.\n",
    "# 그래서 영어를 기반으로 학습되어지다보니 한국어가 처리하기 어려운 것은 어쩔 수 없었다. 성능문제도 발생했다. \n",
    "# 한국어 자연어처리 연구가 진행되는 곳은 에트리(공공기관), 네이버, 카카오등이 있다. \n",
    "# 네이버는 클로바, ko-gpt, kor-bert등이 있다. \n",
    "# 계열을 나누면 인코더 모델, 디코더 모델계열이 있다. GPT는 디코더 계열이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bbf41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 GPT 모델을 간략하게 소개해보자.\n",
    "# 한국어는 에트리에서 kor-bert를 먼저 만들었다. 사전학습된 한국어 버트 모델이었다. \n",
    "# 한국어 백과사전과 뉴스기사를 통해 만들었다. 사이즈는 상당히 크다. 20기가바이트 가량이라고 한다. \n",
    "# 심지어는 3만개 이상의 한국어 단어를 사용했다고 한다. \n",
    "# 그냥 bert는 구글에서 만든것이다. 110개국 버젼으로 존재한다. 한국것이 성능이 시원치 않았기에 에트리에서 파인튜닝 한 것이다. \n",
    "# 기존의 버트보다는 낫다고 한다. \n",
    "# skt는 ko-bert를 만들었다. \n",
    "# ko-gpt도 skt에서 만들었다. \n",
    "# kobart도 skt에서 만들었는데 이름센스가 참 특이하다.\n",
    "# 삼성에서는 koreALBERT라는 것을 만들었다. 이름 참 특이하다. 나무위키에서도 긁어모은 탓에 40기가라는 매우 큰 용량을 가진다. \n",
    "# 클로바는 hyperCLOVA라고 불리며 네이버에서 제작되었다. 네이버의 글을 사용하여 제작되었다. 학습된 데이터가 매우 크다. \n",
    "# 지금 쓰는 klue-bert도 나무위키, 청와대, 뉴스에서 긁어온 케이스이다. \n",
    "# 카카오는 ko-gpt라는 것을 만들었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "# 샾이 두 개씩 붙어있는 문자들이 매우 많다. \n",
    "# 아마 토크나이즈는 모델마다 다를 것이다. 알고 있는 단어는 전부 다를 것이다. \n",
    "# 코퍼스 안에 이미 단어가 있었던 것이다. 하지만 없는 것은 전부 샾이 붙는다. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dce18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "# 맨 앞 토큰인 2와 맨 뒤 토큰인 3이 있다. cls, sep토큰인 듯 하다. \n",
    "print(tokenizer.decode(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\")))\n",
    "# [CLS] 보는내내 그대로 들어맞는 예측 카리스마 없는 악역 [SEP]\n",
    "# 이러한 결과가 출력된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73748b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패드 토큰도 존재하는데 아마 0번에 할당되었을 것이다. \n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827218e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 물론 패딩길이도 원하는 대로 조절하는 것도 가능하다. \n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d216377",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_result = tokenizer.encode(\" 전율을         일으키는         영화 . 다시         보고싶은   영화 \" \n",
    ", max_length=max_seq_len, pad_to_max_length=True)\n",
    "encoded_result\n",
    "# 128개로 패딩이 된 광경을 볼 수 있을 것이다. \n",
    "# 버트모델로 투입하기 위하여 정수 인코딩은 항상 해 줘야 한다.\n",
    "# 또한 세그먼트 인코딩(문장 하나일 때는 0 두개 이상일때는 0과 1로 표현), 어텐션 마스크(가려지는 단어)가 있어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
    "print(valid_num * [1] + (max_seq_len - valid_num) * [0]) # 길이만큼을 쓰고 나머지는 0으로 표현한 모양이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer): # 댓글, 긍/부정/최대길이, 사용 토크나이저\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []# 먼저 빈 리스트를 선언한다. \n",
    "\n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):# 예시, 레이블을 한꺼번에 보내기, \n",
    "        # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True) # 댓글을 최대 길이만큼 시퀀스화, 패딩\n",
    "\n",
    "        # attention_mask는 실제 단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스.\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)                      # 패딩 갯수 \n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count # 마스크 씌우기\n",
    "\n",
    "        # token_type_id는 세그먼트 임베딩을 위한 것으로 이번 예제는 문장이 1개이므로 전부 0으로 통일.\n",
    "        token_type_id = [0] * max_seq_len\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        # 예외사항에 대해서 해결방식\n",
    "        \n",
    "        input_ids.append(input_id)             \n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels    # 나온 값은 훈련 데이터이다. 이게 입력으로 들어간다. \n",
    "# 이러한 방식으로 실행하자. \n",
    "# 방금까지 공부했던 것을 실행해주는 함수이다. \n",
    "# 단어들에 대한 정수 인코딩을 해야 한다. 그 작업을 해주는 함수이다. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "363e47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = convert_examples_to_features(train_data['document'], train_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "# 이거 시간이 매우 많이 걸릴 느낌이다. 그래도 코랩은 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_X) # (3, 146182, 128)로 결과가 나온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = convert_examples_to_features(test_data['document'], test_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "# 테스트 데이터도 같은 방식으로 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e049e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0][0] \n",
    "# 첫번째 문장에대한 input_id일 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[1][0] \n",
    "# 어텐션 마스크일 것이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[2][0] \n",
    "# 세그먼트 아이디일 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = train_X[0][0]\n",
    "attention_mask = train_X[1][0]\n",
    "token_type_id = train_X[2][0]\n",
    "label = train_y[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n",
    "print('레이블 :',label)\n",
    "# 그냥 다 정리해놓았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3884ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertModel.from_pretrained(\"klue/bert-base\", from_pt=True)   # 사전 학습된 버트 모델이다. \n",
    "# 이제 모델을 불러올 시간이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "# 최대길이는 128로 설정했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "token_type_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "                                                                                   # 아까 훈련된 3개의 것들을 모양과 타입을 맞춰놓았다. \n",
    "outputs = model([input_ids_layer, attention_masks_layer, token_type_ids_layer])    # 모델 구조에 맞게 변환한 다음 모델안에 집어넣었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47093fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가학습 이후 모델의 훈련결과가 나올 것이다. \n",
    "# 먼저 아웃풋은 (None, 128,768이 존재한다. 단어 벡터이다.)\n",
    "# None이 자료설명에 적혀있는데 배치크기가 아직 정해지지 않은 모양이다. \n",
    "# 768차원 벡터가 128개가 있다는 뜻이다.  \n",
    "# 만약 요소 하나하나를 본다면 배치크기, 단어벡터만 나온다. 그냥 단어인 듯 하다. \n",
    "# 기존 아웃풋은 입력도 128이고 출력도 128이다. 단어는 1개만 출력되는 듯 하다. \n",
    "# 만들고자 하는 모델이 다:다 구조면 outputs[0]을 사용하면 된다.\n",
    "# 만들고자 하는 모델이 다:1 구조면 outputs[1]을 사용하면 된다.\n",
    "# 우리는 다대 1 구조이다. 그래서 outputs[1]을 사용할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "003072dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForSequenceClassification(tf.keras.Model):                     # 모델을 사용하는 것이다. \n",
    "    def __init__(self, model_name):                                        # 모델 이름이 통째로 넘어갔다.                \n",
    "        super(TFBertForSequenceClassification, self).__init__()            # 부모클래스 객체를 생성했을 것이다.\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)  # 사전 훈련된 모델을 불러온다는 것이다. \n",
    "        self.classifier = tf.keras.layers.Dense(1,                         # 이것은 분류기인듯 하다. \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='sigmoid',      # 출력값이 하나기 때문이다. 만약 여러개면 소프트맥스다.\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):                                                # 콜 함수는 호출하는게 안보일 것이다.  \n",
    "        input_ids, attention_mask, token_type_ids = inputs                 # 얘는 알아서 호출된다. 아까 3개 만든것들이 넘어간다.\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]                                             # 다대 1구조라서 아웃풋 1로 쓴 것이다. \n",
    "        prediction = self.classifier(cls_token)                            # \n",
    "\n",
    "        return prediction                                                  # 예측값이 리턴된다. 현재는 0 아니면 1이 리턴된다.\n",
    "# 다대 1 구조를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU 작동을 위한 코드 TPU 작동을 위한 코드\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403cf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = TFBertForSequenceClassification(\"klue/bert-base\")\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  loss = tf.keras.losses.BinaryCrossentropy()\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
    "# 여기까지가 준비과정이다. 파인튜닝을 이제부터 하면 된다. \n",
    "# 훈련데이터를 사용해야 한다. \n",
    "# 이제 이 과정에서 시간이 어마어마하게 걸린다. \n",
    "# 파인튜닝은 이런 것이다. 이런 방식이 유사하게 진행된다. \n",
    "# 에폭을 2번 돌렸는데도 잘 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5284acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y, epochs=2, batch_size=64, validation_split=0.2)\n",
    "# 상당히 오래걸릴 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdc46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_X, test_y, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)\n",
    "# 모델 평가하는 코드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b153c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478be44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
