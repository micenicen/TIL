{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271f0614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "ERROR: Invalid requirement: '#'\n",
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers                          # 트랜스포머 다운로드\n",
    "!pip install accelerate                            # 엑셀러레이트 다운로드\n",
    "!pip install sentencepiece                         # 센텐스피스 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75657049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\user\\anaconda3\\lib\\site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub            # 허깅페이스 모듈 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd9af74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c31dcc41e9e4ae493c1e48feee93fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\.cache\\\\huggingface\\\\hub\\\\models--google--pegasus-xsum\\\\snapshots\\\\8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b\\\\config.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법 1 : 다운로드하려는 파일(페가수스)의 저장소 ID와 파일 이름 설정\n",
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136f6eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8087e3530fba46f6ac4d39ad295b4702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\.cache\\\\huggingface\\\\hub\\\\models--google--pegasus-xsum\\\\snapshots\\\\4d33b01d79672f27f001f6abade33f22d993b151\\\\config.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법 2 : 다운로드 하려는 파일(페가수스)의 특정 버전을 다운로드하려면 \n",
    "# revision매개변수를 사용하여 분기 이름, 태그 또는 커밋 해시를 지정해야함. \n",
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(\n",
    "    repo_id=\"google/pegasus-xsum\", \n",
    "    filename=\"config.json\", \n",
    "    revision=\"4d33b01d79672f27f001f6abade33f22d993b151\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab998f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f378b2bcc7f4f3eb2df7996716e2cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login                      # 허깅페이스 로그인. https://huggingface.co/settings/tokens의 토큰 이용\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccb11ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf52cc35eb1a4423bcce499306933667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, how are you?\\n\\nAlice: Hello, how are you?\\n\\nBob: I'm doing well. How are you?\\n\\nAlice: I'm doing well, thank you. How about you?\\n\\nBob\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"RWKV/rwkv-raven-1b5\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)                        # 훈련된 모델의 토크나이저 불러오기\n",
    "language_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)            # 훈련된 LM모델 불러오기\n",
    "\n",
    "# CPU로 모델 이동\n",
    "language_model.to(\"cpu\")                                                     # 모델은 cpu에서 가동. 디폴트는 GPU\n",
    "\n",
    "# 텍스트 생성 파이프라인 생성\n",
    "text_generation_pipeline = TextGenerationPipeline(                           # 텍스트 파이프라인 설정\n",
    "    model=language_model,                                                    # 모델 설정\n",
    "    tokenizer=tokenizer,                                                     # 토크나이저 설정\n",
    "    device=-1                                                                # CPU 사용을 위해 -1 설정. GPU사용은 0이 설정값\n",
    ")\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "generated_text = text_generation_pipeline(\"Hello, how are you?\", max_length=50) # 텍스트 형성 \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2477caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def md(t):\n",
    "    display(Markdown(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc00cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Completion(prompt):\n",
    "    output = text_generation_pipeline(\n",
    "        prompt,\n",
    "        max_length=30)\n",
    "    return \"\".join([o['generated_text'] for o in output])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12e6bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "what color is cow?\n",
       "\n",
       "Alice: The color of cow is red."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = Completion(prompt=\"what color is cow?\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469e12d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./RWKV-1b5-tokenizer\\\\tokenizer_config.json',\n",
       " './RWKV-1b5-tokenizer\\\\special_tokens_map.json',\n",
       " './RWKV-1b5-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장할 경로 지정\n",
    "model_path = \"./RWKV-1b5-model\"\n",
    "tokenizer_path = \"./RWKV-1b5-tokenizer\"\n",
    "\n",
    "# 모델과 토크나이저 저장\n",
    "language_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0caaa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b815c5f7d64f189a5ca51f96a613d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "language_model = AutoModelForCausalLM.from_pretrained(\"C:/Users/user/Downloads/llama-2-ko-1b5-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/user/Downloads/llama-2-ko-1b5-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585ee35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, how are you? 안녕하세요? 10. How do you do, Mr. Baker? 미스터 베이커씨, 처음 뵙겠습니다. 11. It's nice to meet you. 만나서 반갑습니다.\"}]\n"
     ]
    }
   ],
   "source": [
    "# CPU로 모델 이동\n",
    "language_model.to(\"cpu\")                                                     # 모델은 cpu에서 가동. 디폴트는 GPU\n",
    "\n",
    "# 텍스트 생성 파이프라인 생성\n",
    "text_generation_pipeline = TextGenerationPipeline(                           # 텍스트 파이프라인 설정\n",
    "    model=language_model,                                                    # 모델 설정\n",
    "    tokenizer=tokenizer,                                                     # 토크나이저 설정\n",
    "    device=-1                                                                # CPU 사용을 위해 -1 설정. GPU사용은 0이 설정값\n",
    ")\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "generated_text = text_generation_pipeline(\"Hello, how are you?\", max_length=50) # 텍스트 형성 \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1841f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def md(t):\n",
    "    display(Markdown(t))\n",
    "def Completion(prompt):\n",
    "    output = text_generation_pipeline(\n",
    "        prompt,\n",
    "        max_length=30)\n",
    "    return \"\".join([o['generated_text'] for o in output])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3de9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "라마는 어떤 동물이야? ㅋㅋ​​​​​​​​​​​​​​​​​"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = Completion(prompt=\"라마는 어떤 동물이야?\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0721c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
