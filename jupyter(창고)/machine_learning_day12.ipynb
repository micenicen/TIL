{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56d78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6057d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde65930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf를 설명해보자.\n",
    "# 자연어는 일상에서 사용하는 언어를 말한다. 사람들이 일상에서 쓰는 언어가 자연어이다. 강아지소리도 자연어는 아니다. \n",
    "# 보통 음성인식, 요약, 번역, 감성분석, 텍스트분석, 기사분류, 분류, 질의응답, 챗봇등에 사용된다.\n",
    "# 자연어처리에 관련된 기술들이 많이 등장하였다. \n",
    "# 환경구성 : 주피터노트북, 아나콘다를 설치했을 것이다. 머신러닝,딥러닝,데이터분석(사이킷런) 패키지가 설치되어 있다.\n",
    "# 추가로 설치해야하는 것은 텐서플로, 젠심, 파이토치를 설치해야하고 한국어는 konlpy를 써야한다. \n",
    "# 영어는 nltk가 있지만 주피터노트북에 기본설치가 되어있다. \n",
    "# 자연어 처리를 하는 과정을 생각해보자. \n",
    "# 1. 텍스트 전처리 절차\n",
    "# - 토큰화, 정제, 어간추출, 불용어 제거, 정수인코딩(단어를 숫자로 바꾸기), 패딩\n",
    "# 이러한 작업들을 해야하며 패키지를 이용해야 한다. \n",
    "# 2. 텍스트 수치화 표현\n",
    "# - BoW, DTM/TDM ,TF-IDF등이 사용된다. \n",
    "# 다음과 같이 텍스트를 수치로 표현하는 방법이 여러가지가 있다.\n",
    "# 3. 유사도\n",
    "# - 문서간의 유사도, 단어간의 유사도, 문장간의 유사도\n",
    "# 머신러닝/ 딥러닝 알고리즘 사용하여 딥러닝 모델을 생성함\n",
    "# 먼저 설치해야 하는 것들은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a99da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\user\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy # 한국어 데이터분석 패키지이다. \n",
    "# 기본적으로 자연어처리를 위한 여러 도구가 있는데 이중에 한국어패키지이다. \n",
    "# 문제는 이것이 자바로 만들어져 있다. 여기는 파이썬이다. 다이렉트로 연결이 잘 안된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d561c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt #먼저 불러보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e085930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e28ee940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
      "['항공기', '체계', '종합', '개발', '경험']\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(u'단독입찰보다 복수입찰의 경우')) # 형태소 단위로 나눈 것이다.\n",
    "print(okt.nouns(u'유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는')) # 명사 추출을 한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb151a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "# https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0 형태소분석기 이름들이다.\n",
    "# 여기서 Mecab, Okt을 자주 쓴다. Okt는 원래 네임은 Twitter korean Text이다. 저작권으로 인해서 바꾼 것이다. \n",
    "print(okt.pos(u'이것도 되나욬ㅋㅋ', norm=True, stem=True)) # 각각 나눠서 이 말의 단어의 속성도 볼 수 있다.\n",
    "# 이것은 형태소 분석기인데 리스트 안에 튜플로 만들어놓았다. \n",
    "# 한나눔, 코코마는 카이스트, 서울대에서 만들었지만 잘 쓰지 않는다. 코모란도 잘 쓰지 않는다. \n",
    "# 매켑은 교토대학에서 제작한 것으로 한국어 매켑은 성능이 괜찮다. \n",
    "# 유호영님께서 만든 Okt는 성능이 좋기로 유명하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d69b7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import * # 전부 가져와봤다. 물론 이걸 권장하지 않는다. 메모리로 전부 적재되어버리기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65d6bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "han = Hannanum()\n",
    "kkma = Kkma()\n",
    "# 먼저 3가지만 해보자. 예를들어 '아버지가방에들어가신다.'가 있다. 띄어쓰기가 안되어 있으면 판단하기 어려울 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb33b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'Noun'), ('가방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb'), ('.', 'Punctuation')]\n",
      "[('아버지가방에들어가', 'N'), ('이', 'J'), ('시ㄴ다', 'E'), ('.', 'S')]\n",
      "[('아버지', 'NNG'), ('가방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos('아버지가방에들어가신다.'))\n",
    "print(han.pos('아버지가방에들어가신다.'))\n",
    "print(kkma.pos('아버지가방에들어가신다.'))\n",
    "# 한나눔은 성능이 별로 안좋다는게 보인다. kkma는 의외로 성능이 좋은 편이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12ea838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('정부', 'Noun'), ('가', 'Josa'), ('발표', 'Noun'), ('하는', 'Verb'), ('물가상승률', 'Noun'), ('과', 'Josa'), ('소비자', 'Noun'), ('가', 'Josa'), ('느끼는', 'Verb'), ('물가상승률', 'Noun'), ('은', 'Josa'), ('다르다', 'Adjective'), ('.', 'Punctuation')]\n",
      "\n",
      "\n",
      "[('정부', 'N'), ('가', 'J'), ('발표', 'N'), ('하', 'X'), ('는', 'E'), ('물가상승률', 'N'), ('과', 'J'), ('소비자', 'N'), ('가', 'J'), ('느끼', 'P'), ('는', 'E'), ('물가상승률', 'N'), ('은', 'J'), ('다르', 'P'), ('다', 'E'), ('.', 'S')]\n",
      "\n",
      "\n",
      "[('정부', 'NNG'), ('가', 'JKS'), ('발표', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('물가', 'NNG'), ('상승률', 'NNG'), ('과', 'JC'), ('소비자', 'NNG'), ('가', 'JKS'), ('느끼', 'VV'), ('는', 'ETD'), ('물가', 'NNG'), ('상승률', 'NNG'), ('은', 'JX'), ('다르', 'VA'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"))\n",
    "print (\"\\n\")\n",
    "print(han.pos(\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"))\n",
    "print (\"\\n\")\n",
    "print(kkma.pos(\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"))\n",
    "# Punctuation는 콤마같이 기준점이 되는 것들이다. \n",
    "# 여기서 내가 원하는 것만 뽑아서 작업을 하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96778a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log \n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "]\n",
    "# 매우 짧은 문서가 4개가 있다. 영화마다 있는 overview랑 비슷하게 생각하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16d293de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['먹고', '좋아요', '싶은', '과일이', '길고', '노란', '사과', '바나나', '저는']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=list(set(w for doc in docs for w in doc.split()))# doc을 docs에서 추출해서 스플릿하여 w에 담아라\n",
    "vocab # 코퍼스(말뭉치) : 전체 문서에 등장하는 단어집합이다. \n",
    "# 총 9개의 단어로 구성되어있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5b3a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf의 원리 함수\n",
    "N = len(docs) # 전체 문서의 길이\n",
    "\n",
    "def tf(t, d): # 단어와 문서를 전달받는다. \n",
    "  return d.count(t) # t라는 단어의 빈도수를 본다. \n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for doc in docs: \n",
    "    df += t in doc # 문서의 내부의 값이 있으면 df에 1씩 더한다. \n",
    "  return log(N/(df+1)) # 문서 전체를 로그로 씌운 후 전체문서 길이에 df+1을 나눈다. \n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t) # tf와 idf를 곱하는 것이 tfidf이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94b1805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N): # 전체 문서의 길이만큼 반복하라. \n",
    "  result.append([])# 결과에 비어있는 리스트를 대입하라.\n",
    "  d = docs[i] # i번째 문서를 d로 정의하라\n",
    "  for j in range(len(vocab)): # vocab의 길이만큼 확인하라\n",
    "    t = vocab[j] # vocap의 단어를 t에 대입해라.\n",
    "    result[-1].append(tf(t, d)) # 과일의 라는 단어를 tf함수에 전달한다.\n",
    "                                # tf(원하는 단어, 문서 전체)순으로 정렬하여 문서에 몇번 나왔는지 카운트한다. \n",
    "                                # 얘들이 들어가있다. 문서별 단어의 빈도수이다. 이것으로 데이터프레임을 만들었다. \n",
    "tf_ = pd.DataFrame(result, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcfbeefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>먹고</th>\n",
       "      <th>좋아요</th>\n",
       "      <th>싶은</th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>사과</th>\n",
       "      <th>바나나</th>\n",
       "      <th>저는</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   먹고  좋아요  싶은  과일이  길고  노란  사과  바나나  저는\n",
       "0   1    0   1    0   0   0   1    0   0\n",
       "1   1    0   1    0   0   0   0    1   0\n",
       "2   0    0   0    0   1   1   0    2   0\n",
       "3   0    1   0    1   0   0   0    0   1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ # 이것을 tf행렬이라고 부른다. DTM이라고도 부른다. 문서단어행렬이라고도 한다.\n",
    "# 행에는 문서의 번호가 적혀있다. 열에는 단어집합이 적혀있다.\n",
    "# 데이터는 빈도수이다. 이것을 문서단어행렬이라고 한다. \n",
    "# vocab에 있는 것으로 열이름을 정의했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3482c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #이걸 써보자. 카운터 벡터라이저이다. TF를 구하는 목적으로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83620d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29d19e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.fit_transform(docs).toarray() # dtm을 만들때는 이렇게 만들면 된다.  사실상 이게 TF이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bf8591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'먹고': 3,\n",
       " '싶은': 6,\n",
       " '사과': 5,\n",
       " '바나나': 4,\n",
       " '길고': 1,\n",
       " '노란': 2,\n",
       " '저는': 7,\n",
       " '과일이': 0,\n",
       " '좋아요': 8}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_ # 열 인덱스 번호이다. 위의 어레이의 인덱스이다.인덱스값이다. 여기까지 TF를 구한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "614c5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도만 가지고 임베딩을 하는 것은 문제가 있다. 이것이 얼마만큼 중요한 단어인지 설명할 수 있어야 한다.\n",
    "# TF-IDF(Term Frequency - Inverse Document Frequency) 단어빈도*역문서빈도의 약자이다. \n",
    "# 즉 문서에서 각 \"단어의 중요도\"를 나타낸 행렬이다. \n",
    "# 구하는 목적은 각 문서에서 중요단어가 무엇인지 궁금하기에 사용한다. \n",
    "# 키워드로 만들고, 더 나아가 토픽모델링으로 확장해서 사용하기도 한다. \n",
    "# 여기서 단어의 중요하다는 의미는 단어가 자주 나올때 중요하다고 인식하는 것이다.\n",
    "# 문제는 단순히 많이 등장했다고 해서 중요도가 높지는 않을 것이다. \n",
    "# 모든 문장에는 관사가 존재한다. 관사가 중요하다고 말하지는 않을 것이다.\n",
    "# df는 단어의 빈도수이다.위에 먹고는 2번 나왔고 바나나는 3번 나왔다. df가 높다는 것은 많은 단어라는 뜻이다. \n",
    "# 즉 df가 작아져아지 중요도가 높다고 판단한 것이다. \n",
    "# 로그를 취하는 이유 : 로그를 취하지 않았을 때 문서의 갯수가 커질수록 idf값이 기하급수적으로 커진다. \n",
    "# 로그를 안쓰면 idf값이 마구마구 커진다. 1백만은 log 6 이다.  로그를 안쓰면 1백만을 그대로 써야한다. 그리고 log1 = 0 이다. \n",
    "\n",
    "# 요소연산(element predict) : 요소끼리 곱셈을 하는 것이다. \n",
    "# 행렬곱셈(matrix multifly) : 행렬의 전체곱셈, 위치하는 방식이다. 우리가 일반적으로 쓰는 곱셈이다.\n",
    "# 문서빈도(Term Frequency)는 위에 '먹고'라는 단어가 몇개에 문서에서 등장했냐는 것이다. 2개의 문서에서 등장했을 것이다.\n",
    "# 바나나도 2이다 문서에 바나나가 나왔는지만 보면 2이다. 문서 내 갯수는 상관없다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "763e5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Tfidf 벡터라이저이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a68f74b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "먹고   0.287682\n",
       "좋아요  0.693147\n",
       "싶은   0.287682\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "사과   0.693147\n",
       "바나나  0.287682\n",
       "저는   0.693147"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)): \n",
    "    t = vocab[j] # 각각의 단어를 t에 대입했다. \n",
    "    result.append(idf(t)) # idf값으로 계산하여 결과값에 대입한다.\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"]) #행의 이름을  각 단어들로 했고 열값을 IDF라고 정의했다. \n",
    "idf_ # idf행렬이 출력되었다. 먹고, 바나나는 흔한 단어라는 소리이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "661fd18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>먹고</th>\n",
       "      <th>좋아요</th>\n",
       "      <th>싶은</th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>사과</th>\n",
       "      <th>바나나</th>\n",
       "      <th>저는</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   먹고  좋아요  싶은  과일이  길고  노란  사과  바나나  저는\n",
       "0   1    0   1    0   0   0   1    0   0\n",
       "1   1    0   1    0   0   0   0    1   0\n",
       "2   0    0   0    0   1   1   0    2   0\n",
       "3   0    1   0    1   0   0   0    0   1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ # tf도 만들어졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a2c4662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>먹고</th>\n",
       "      <th>좋아요</th>\n",
       "      <th>싶은</th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>사과</th>\n",
       "      <th>바나나</th>\n",
       "      <th>저는</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         먹고       좋아요        싶은       과일이        길고        노란        사과  \\\n",
       "0  0.287682  0.000000  0.287682  0.000000  0.000000  0.000000  0.693147   \n",
       "1  0.287682  0.000000  0.287682  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.693147  0.693147  0.000000   \n",
       "3  0.000000  0.693147  0.000000  0.693147  0.000000  0.000000  0.000000   \n",
       "\n",
       "        바나나        저는  \n",
       "0  0.000000  0.000000  \n",
       "1  0.287682  0.000000  \n",
       "2  0.575364  0.000000  \n",
       "3  0.000000  0.693147  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 이 두 개를 곱하면 된다. \n",
    "# 이제 행렬곱으로 tf와 idf를 곱하면 되는 것이다. \n",
    "result = []\n",
    "for i in range(N): # n개의 문서를 돌았다.\n",
    "  result.append([]) # 먼저 문서에 리스트를 추가했다.\n",
    "  d = docs[i] # 문서를 d로 정으했다.\n",
    "  for j in range(len(vocab)): # 문자의 길이만큼 정의했다.\n",
    "    t = vocab[j] # 문자를 하나하나 출력해서 t에 대입했따.\n",
    "    result[-1].append(tfidf(t,d)) # 결과값에 tfidf로 계산해서 새로생긴 리스트에 대입한다. \n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aed7c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 데이터 전처리를 잘 시행해보자. 불용어 제거 등의 절차를 해야 한다. \n",
    "# 1. 각 문서(영화)에서 중요 단어 추출\n",
    "# 2. 코사인유사도 기반 가장 유사한 문서(영화) 검색\n",
    "# https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=tf+idf+cosine+similarity&oq=tfidf+cosin \n",
    "# 학술논문이다.2023년 논문들이다. \n",
    "# 어느 것에 대한 가중치를 부여해서 tf-idf를 사용할 수도 있다. \n",
    "# 오후에 해보도록 하자. \n",
    "# 영어전처리는nltpl을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01e51ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/80/6f/57d36f6507e432d7fc1956b2e9e8530c5c2d2bfcd8821bcbfae271cd6688/tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/ad/6e/1bfe367855dd87467564f7bf9fa14f3b17889988e79598bc37bf18f5ffb6/tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.7.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/c2/59/f89c04923d68595d359f4cd7adbbdf5e5d791257945f8873d88b2fd1f979/protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 0.4/1.5 MB 7.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.3 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/75/c5/fb3ed7495c73c0de58b08376a468a35bdb61b89ddfbdb96a37bceb54f959/grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 143.4/151.7 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl (284.2 MB)\n",
      "   ---------------------------------------- 0.0/284.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/284.2 MB 4.8 MB/s eta 0:01:00\n",
      "   ---------------------------------------- 0.4/284.2 MB 4.6 MB/s eta 0:01:02\n",
      "   ---------------------------------------- 0.6/284.2 MB 4.3 MB/s eta 0:01:06\n",
      "   ---------------------------------------- 0.7/284.2 MB 4.3 MB/s eta 0:01:07\n",
      "   ---------------------------------------- 1.0/284.2 MB 4.3 MB/s eta 0:01:06\n",
      "   ---------------------------------------- 1.2/284.2 MB 4.4 MB/s eta 0:01:05\n",
      "   ---------------------------------------- 1.4/284.2 MB 4.4 MB/s eta 0:01:04\n",
      "   ---------------------------------------- 1.6/284.2 MB 4.5 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 1.9/284.2 MB 4.6 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 2.1/284.2 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------------------------------------- 2.4/284.2 MB 4.8 MB/s eta 0:00:59\n",
      "   ---------------------------------------- 2.8/284.2 MB 5.0 MB/s eta 0:00:56\n",
      "   ---------------------------------------- 3.1/284.2 MB 5.2 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 3.4/284.2 MB 5.3 MB/s eta 0:00:53\n",
      "    --------------------------------------- 3.7/284.2 MB 5.4 MB/s eta 0:00:52\n",
      "    --------------------------------------- 4.1/284.2 MB 5.5 MB/s eta 0:00:51\n",
      "    --------------------------------------- 4.4/284.2 MB 5.6 MB/s eta 0:00:50\n",
      "    --------------------------------------- 4.8/284.2 MB 5.8 MB/s eta 0:00:49\n",
      "    --------------------------------------- 5.2/284.2 MB 5.9 MB/s eta 0:00:48\n",
      "    --------------------------------------- 5.6/284.2 MB 6.0 MB/s eta 0:00:47\n",
      "    --------------------------------------- 6.0/284.2 MB 6.2 MB/s eta 0:00:45\n",
      "    --------------------------------------- 6.5/284.2 MB 6.4 MB/s eta 0:00:44\n",
      "    --------------------------------------- 7.0/284.2 MB 6.6 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 7.4/284.2 MB 6.7 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 7.9/284.2 MB 6.8 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 8.4/284.2 MB 7.0 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 8.8/284.2 MB 7.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 9.3/284.2 MB 7.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 9.7/284.2 MB 7.2 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 10.1/284.2 MB 7.3 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 10.5/284.2 MB 7.4 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 11.0/284.2 MB 7.8 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 11.4/284.2 MB 8.1 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 11.9/284.2 MB 8.4 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 12.3/284.2 MB 8.6 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 12.7/284.2 MB 8.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 13.1/284.2 MB 8.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 13.5/284.2 MB 9.0 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 13.9/284.2 MB 9.0 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 14.2/284.2 MB 9.1 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 14.6/284.2 MB 9.1 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 15.0/284.2 MB 9.1 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 15.5/284.2 MB 9.2 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 15.8/284.2 MB 9.1 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 16.2/284.2 MB 9.1 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 16.5/284.2 MB 9.0 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 16.8/284.2 MB 8.8 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 17.1/284.2 MB 8.7 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 17.4/284.2 MB 8.5 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 17.7/284.2 MB 8.4 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 18.0/284.2 MB 8.3 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 18.3/284.2 MB 8.2 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 18.6/284.2 MB 8.1 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 18.9/284.2 MB 8.0 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 19.3/284.2 MB 7.9 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 19.6/284.2 MB 7.9 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 19.9/284.2 MB 7.7 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 20.2/284.2 MB 7.6 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 20.5/284.2 MB 7.6 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 20.8/284.2 MB 7.5 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 21.1/284.2 MB 7.4 MB/s eta 0:00:36\n",
      "   --- ------------------------------------ 21.4/284.2 MB 7.4 MB/s eta 0:00:36\n",
      "   --- ------------------------------------ 21.7/284.2 MB 7.4 MB/s eta 0:00:36\n",
      "   --- ------------------------------------ 22.0/284.2 MB 7.3 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 22.4/284.2 MB 7.2 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 22.8/284.2 MB 7.2 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 23.1/284.2 MB 7.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 23.4/284.2 MB 7.1 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 23.7/284.2 MB 7.0 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 23.9/284.2 MB 6.9 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 24.1/284.2 MB 6.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 24.4/284.2 MB 6.7 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 24.7/284.2 MB 6.7 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 24.9/284.2 MB 6.6 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 25.2/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 25.6/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 25.9/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 26.3/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 26.6/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 27.0/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 27.3/284.2 MB 6.6 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 27.7/284.2 MB 6.5 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 28.1/284.2 MB 6.7 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 28.6/284.2 MB 6.9 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 29.0/284.2 MB 6.9 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 29.4/284.2 MB 7.0 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 29.9/284.2 MB 7.0 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 30.3/284.2 MB 7.1 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 30.7/284.2 MB 7.4 MB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 31.2/284.2 MB 7.4 MB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 31.7/284.2 MB 7.5 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 32.1/284.2 MB 7.6 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 32.4/284.2 MB 7.6 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 32.8/284.2 MB 7.7 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 33.2/284.2 MB 7.6 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 33.6/284.2 MB 7.8 MB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 34.0/284.2 MB 8.0 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 34.5/284.2 MB 8.3 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 34.9/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 35.2/284.2 MB 8.5 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 35.5/284.2 MB 8.5 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 35.8/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 36.1/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 36.5/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 36.8/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 37.2/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 37.5/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 37.9/284.2 MB 8.5 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 38.3/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 38.7/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 39.1/284.2 MB 8.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 39.4/284.2 MB 8.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 39.9/284.2 MB 8.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 40.3/284.2 MB 8.3 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 40.6/284.2 MB 8.2 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 41.0/284.2 MB 8.2 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 41.4/284.2 MB 8.2 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 41.8/284.2 MB 8.1 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 42.3/284.2 MB 8.1 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 42.8/284.2 MB 8.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 43.3/284.2 MB 8.4 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 43.8/284.2 MB 8.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 44.3/284.2 MB 8.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 44.8/284.2 MB 8.5 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 45.3/284.2 MB 8.6 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 45.8/284.2 MB 8.8 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 46.4/284.2 MB 9.1 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 46.9/284.2 MB 9.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 47.4/284.2 MB 9.5 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 47.9/284.2 MB 9.8 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 48.5/284.2 MB 9.9 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 49.1/284.2 MB 10.2 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 49.7/284.2 MB 10.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 50.3/284.2 MB 10.7 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 50.9/284.2 MB 11.1 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 51.6/284.2 MB 11.5 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 52.3/284.2 MB 11.9 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 53.0/284.2 MB 12.1 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 53.7/284.2 MB 12.4 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 54.4/284.2 MB 12.8 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 55.1/284.2 MB 13.1 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 55.8/284.2 MB 13.4 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 56.4/284.2 MB 13.6 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 57.1/284.2 MB 13.9 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 57.9/284.2 MB 14.2 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 58.6/284.2 MB 14.6 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 59.3/284.2 MB 14.9 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 60.0/284.2 MB 14.9 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 60.7/284.2 MB 14.9 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 61.3/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 62.0/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 62.7/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 63.3/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 64.0/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 64.7/284.2 MB 14.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 65.2/284.2 MB 14.5 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 65.8/284.2 MB 14.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 66.3/284.2 MB 14.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 66.9/284.2 MB 14.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 67.5/284.2 MB 13.9 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 68.1/284.2 MB 13.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 68.6/284.2 MB 13.4 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 69.2/284.2 MB 13.4 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 69.7/284.2 MB 13.1 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 70.2/284.2 MB 12.9 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 70.7/284.2 MB 12.6 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 71.2/284.2 MB 12.6 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 71.7/284.2 MB 12.4 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 72.2/284.2 MB 12.1 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 72.8/284.2 MB 12.1 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 73.3/284.2 MB 11.9 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 73.7/284.2 MB 11.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 74.3/284.2 MB 11.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 74.8/284.2 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 75.4/284.2 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 76.0/284.2 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 76.5/284.2 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 77.0/284.2 MB 11.3 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 77.4/284.2 MB 11.5 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 77.9/284.2 MB 11.1 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 78.3/284.2 MB 10.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 78.8/284.2 MB 11.1 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 79.3/284.2 MB 10.7 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 79.8/284.2 MB 10.7 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 80.3/284.2 MB 10.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 80.8/284.2 MB 10.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 81.3/284.2 MB 10.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 81.8/284.2 MB 10.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 82.4/284.2 MB 10.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 82.9/284.2 MB 10.7 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 83.4/284.2 MB 10.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 83.9/284.2 MB 10.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 84.5/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 85.1/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 85.7/284.2 MB 10.9 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 86.2/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 86.7/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 87.2/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 87.8/284.2 MB 11.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 88.5/284.2 MB 11.5 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 89.2/284.2 MB 11.7 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 89.9/284.2 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 90.5/284.2 MB 11.9 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 91.0/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 91.6/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 92.2/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 92.8/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 93.4/284.2 MB 12.6 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 94.0/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 94.5/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 95.1/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 95.7/284.2 MB 12.3 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 96.2/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 96.7/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 97.3/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 97.8/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 98.3/284.2 MB 12.4 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 98.8/284.2 MB 12.1 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 99.3/284.2 MB 11.9 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 99.9/284.2 MB 11.7 MB/s eta 0:00:16\n",
      "   ------------- ------------------------- 100.5/284.2 MB 11.7 MB/s eta 0:00:16\n",
      "   ------------- ------------------------- 101.0/284.2 MB 11.9 MB/s eta 0:00:16\n",
      "   ------------- ------------------------- 101.5/284.2 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------- ------------------------ 102.1/284.2 MB 11.7 MB/s eta 0:00:16\n",
      "   -------------- ------------------------ 102.8/284.2 MB 11.9 MB/s eta 0:00:16\n",
      "   -------------- ------------------------ 103.5/284.2 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------ 104.2/284.2 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------ 105.0/284.2 MB 12.6 MB/s eta 0:00:15\n",
      "   -------------- ------------------------ 105.7/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 106.3/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 106.9/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 107.5/284.2 MB 12.9 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 108.0/284.2 MB 13.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 108.5/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 109.0/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 109.6/284.2 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 110.1/284.2 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 110.7/284.2 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 111.2/284.2 MB 12.8 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 111.7/284.2 MB 12.9 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 112.1/284.2 MB 12.6 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 112.6/284.2 MB 12.6 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 113.2/284.2 MB 12.4 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 113.7/284.2 MB 12.4 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 114.4/284.2 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------- ----------------------- 114.9/284.2 MB 11.9 MB/s eta 0:00:15\n",
      "   --------------- ----------------------- 115.6/284.2 MB 11.9 MB/s eta 0:00:15\n",
      "   --------------- ----------------------- 116.2/284.2 MB 11.9 MB/s eta 0:00:15\n",
      "   ---------------- ---------------------- 116.7/284.2 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------- ---------------------- 117.3/284.2 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------- ---------------------- 117.8/284.2 MB 11.7 MB/s eta 0:00:15\n",
      "   ---------------- ---------------------- 118.4/284.2 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 118.9/284.2 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 119.5/284.2 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 120.2/284.2 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 121.0/284.2 MB 12.1 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 121.7/284.2 MB 12.6 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 122.3/284.2 MB 12.8 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 122.9/284.2 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 123.4/284.2 MB 13.1 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 124.1/284.2 MB 13.1 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 124.9/284.2 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 125.7/284.2 MB 13.6 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 126.9/284.2 MB 14.6 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 127.8/284.2 MB 15.2 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 128.5/284.2 MB 15.2 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 129.2/284.2 MB 16.0 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 130.0/284.2 MB 16.0 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 130.9/284.2 MB 16.4 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 131.7/284.2 MB 16.4 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 132.4/284.2 MB 16.4 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 133.1/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 133.8/284.2 MB 17.3 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 134.6/284.2 MB 17.2 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 135.3/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 136.1/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 136.9/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 137.7/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 138.6/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 139.3/284.2 MB 16.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 139.9/284.2 MB 16.4 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 140.7/284.2 MB 16.4 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 141.4/284.2 MB 16.4 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 142.1/284.2 MB 16.4 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 142.7/284.2 MB 16.0 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 143.3/284.2 MB 16.0 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 143.8/284.2 MB 15.6 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 144.4/284.2 MB 15.2 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 144.9/284.2 MB 14.9 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 145.5/284.2 MB 14.9 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 146.1/284.2 MB 14.6 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 146.8/284.2 MB 14.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 147.4/284.2 MB 13.9 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 147.9/284.2 MB 13.6 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 148.5/284.2 MB 13.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 149.1/284.2 MB 13.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 149.7/284.2 MB 13.1 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 150.3/284.2 MB 13.1 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 151.0/284.2 MB 13.1 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 151.6/284.2 MB 12.8 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 152.3/284.2 MB 12.8 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 152.9/284.2 MB 12.8 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 153.6/284.2 MB 13.1 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 154.3/284.2 MB 13.4 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 154.9/284.2 MB 13.6 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 155.6/284.2 MB 13.6 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 156.3/284.2 MB 13.6 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 157.0/284.2 MB 13.9 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 157.8/284.2 MB 13.9 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 158.5/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 159.1/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 159.7/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 160.3/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 160.8/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 161.6/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 162.4/284.2 MB 14.6 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 163.3/284.2 MB 14.9 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 164.1/284.2 MB 15.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 164.9/284.2 MB 15.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 165.7/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 166.5/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 167.3/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 168.2/284.2 MB 16.0 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 168.9/284.2 MB 16.4 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 169.7/284.2 MB 16.4 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 170.8/284.2 MB 17.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 171.6/284.2 MB 17.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 172.8/284.2 MB 18.7 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 174.2/284.2 MB 19.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 175.7/284.2 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 177.1/284.2 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 178.4/284.2 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 179.7/284.2 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 180.9/284.2 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 181.9/284.2 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 182.8/284.2 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 183.7/284.2 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 184.5/284.2 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 185.3/284.2 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 186.2/284.2 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 187.0/284.2 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 187.9/284.2 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 188.9/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 189.7/284.2 MB 19.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 190.7/284.2 MB 19.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 191.8/284.2 MB 19.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 192.7/284.2 MB 19.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 193.7/284.2 MB 19.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 194.7/284.2 MB 19.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 195.6/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 196.5/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 197.5/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 198.3/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 199.2/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 200.2/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 201.1/284.2 MB 20.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 202.5/284.2 MB 21.1 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 204.0/284.2 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 205.9/284.2 MB 25.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 207.8/284.2 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 210.1/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 212.4/284.2 MB 43.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 214.7/284.2 MB 46.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 216.5/284.2 MB 46.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 218.3/284.2 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 220.7/284.2 MB 46.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 223.2/284.2 MB 46.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 225.8/284.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 228.7/284.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 231.6/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 234.6/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 237.6/284.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 240.7/284.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 243.4/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 246.1/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 249.4/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 252.6/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 254.6/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 257.5/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 260.9/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 263.3/284.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 267.0/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 269.6/284.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 271.5/284.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 273.4/284.2 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 275.3/284.2 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  277.1/284.2 MB 43.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  278.8/284.2 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  280.7/284.2 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  282.5/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 284.2/284.2 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "   ---------------------------------------- 0.0/938.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 938.7/938.7 kB 19.8 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.2/130.2 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  3.7/3.7 MB 117.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 46.8 MB/s eta 0:00:00\n",
      "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 54.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 27.4 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 55.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.1/24.4 MB 64.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.5/24.4 MB 67.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.7/24.4 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.9/24.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.4/24.4 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.4/24.4 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 46.9 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/430.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 430.5/430.5 kB 13.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.9/5.5 MB 62.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 70.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 44.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 0.0/440.7 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/440.7 kB 28.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 440.7/440.7 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.3/182.3 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 keras-2.14.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-intel-2.14.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": [
    "#nltk 패키지 이용가이드\n",
    "!pip install tensorflow # 사용하기 위해서는 텐서플로를 설치해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9072d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.1/67.1 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=821c32cf529edd6fdf1f0f1165f93e12e11eb4fe07f6d4fb54157ee1406d1a27\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=90e66a285e4a38decbdfb4c098d75d094dcf47971a93729eb0dfcdae4f78ea18\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim # 젠심을 설치하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1609ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk # nltk를 최종적으로 설치했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d137043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize # 영어로 된 글자를 토큰화시킨다. \n",
    "from nltk.tokenize import WordPunctTokenizer # 위랑 비슷하다. 다만 점을 분리하는 방식이 다르다. \n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence # 토큰화를 텐서플로가 해준다. 단어를 전부 소문자로 바꾼다. \n",
    "# 버젼도 맞춰야 하는 과정들이 필요할 수 있다. 꽤 번거롭다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6566d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language tokenizetion \n",
    "# 토큰화를 하기 위한 셋이다. \n",
    "# 토큰 : 단어(문장)을 말한다. 단어단위로 나눌 수 있고 문장단위로 나눌 수도 있다.\n",
    "# 심지어는 문단을 토큰으로 볼 수도 있다. 심하면 문자도 토큰이 될 수 있다.(단어, 문장, 문단, 문자)  \n",
    "# 즉 토큰은 자연어처리 작업을 수행하는 기본 단위를 말한다. \n",
    "# 토큰화 : 주어진 코퍼스를 토큰 단위로 나누는 작업을 말한다. \n",
    "# 자연어 -> 토큰화 -> 세부작업을 진행하는 식이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39dd58c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m단어 토큰화1 :\u001b[39m\u001b[38;5;124m'\u001b[39m,word_tokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be fooled by the dark sounding name, Mr. Jone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Orphanage is as cheery as cheery goes for a pastry shop.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c8419e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# 룩업애러는 이렇게 하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c0295e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "# 잘 작동하였다. 룩업애러에서 다운로드 하라고 하는 것은 다 다운받으면 된다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b1838f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "# 위랑 처리하는 방식이 다르다. 영어에도 토큰화 도구가 있지만 많이 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be9181aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화3 :',text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "# 케라스에서 제공하는 도구이다.\n",
    "# 모든 문자를 소문자로 바꾼다. 특이하게 콤마가 없어졌다. 마침표도 제거해버린다. 즉 구두점(fuctuation)을 전부 날린다. \n",
    "# don't가 그대로 보존된다. jone's도 보존되었다. \n",
    "# 이걸 단어토큰화라고 한다. 물론 문장토큰화도 존재한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc3ea0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize # 문장을 문단으로 토큰화하였다. 자료형 리스트로 담는다. \n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :',sent_tokenize(text))\n",
    "# 문장별로 토큰화를 하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e43751a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))\n",
    "# 재밌는건 단순하게 마침표로 문장을 인식하지 않는다. 마침표만으로 문장의 끝으로 안본다. 성능이 괜찮다. \n",
    "# 이와같은 것들이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75ddfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오후에는 전처리를 좀더 소개하고 어제 사용한 영화데이터로 전처리를 거칠 것이다.\n",
    "# 코사인 유사도를 구해서 오버뷰를 구해서 가장 유사한 영화를 찾아주는 시스템을 만들 것이다. \n",
    "# 직접 구축해볼 것이다. \n",
    "# 자연어는 범주가 매우 넓고 이번것은 극히 일부분이다. \n",
    "# 앞시간에 영어의 토크나이즈를 확인했다. 복습을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9631f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "# pos는 파트오브 스피치다. \n",
    "# 이런 식으로 형태소 분석이 이루어졌다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "880ba422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "# 자연어 처리를 할 때는 토큰화 작업을 먼저 한다. \n",
    "# 토큰화작업 이후에는 정제를 해야한다. 불필요한 단어를 제거하는 절차이다. 불용어라고 한다.\n",
    "# 불용어는 길이가 짧은 단어나 등장 빈도수가 낮은 단어이다. \n",
    "# 영화 오버뷰에는 단어가 너무 많을 것이다. 그래서 일일히 출력하면 다 못나온다. \n",
    "# 불용어 제거는 등장빈도수가 낮은 단어를 선택해야 하는데 이 수치를 얼마로 할 것인지가 애매하다. \n",
    "# 상황에 맞는 기준을 정해서 만들어야 한다. \n",
    "# 만약 5000편의 영화를 가지고 연관분석을 한다면 전체 영화에 대해서 열의 합계를 모은다. \n",
    "# 열단위로 합계수치가 5 미만이면 제거. 이런 식으로 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04258f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')# 단어나 문자나 숫자가 아닌것과 매치, 공백문자, 문자숫자, 1글자 이상, 2글자 이하에 부합하는 경우,공백문자.\n",
    "print(shortword.sub('', text)) # 숏워드패턴의 글자는 전부 지워라.\n",
    "# 정규표현식을 이용해서불필요한 단어를 제거하는 작업을 한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "996f1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거는 불용어 사전을 이용해서 제거할 수도 있다. \n",
    "# 단어에서도 중요한 부분이 어간이다. 부가 되는 것을 접사라고 한다.\n",
    "# 형태소는 어간(stem, 단어의 중요의미)과 접사(부가의미)로 구분된다. \n",
    "# 예를 들어서 영어에 dogs가 있다고 해보자.\n",
    "# dogs => dog + s 로 정의할 수 있다. \n",
    "# dog부터는 분리할 수 없다. \n",
    "# 필요하면 쓰고 필요하면 쓰지 않아도 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ba10a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhave\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatched\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m표제어 추출 전 :\u001b[39m\u001b[38;5;124m'\u001b[39m,words)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m표제어 추출 후 :\u001b[39m\u001b[38;5;124m'\u001b[39m,[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words])\n",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhave\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatched\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m표제어 추출 전 :\u001b[39m\u001b[38;5;124m'\u001b[39m,words)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m표제어 추출 후 :\u001b[39m\u001b[38;5;124m'\u001b[39m,[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05b86d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # 애러 메세지가 뜨면 이걸 다운로드 하라고 나올 것이다.\n",
    "nltk.download('wordnet')# 그리고 다시 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a97c7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()# 레머타이저란 단어들 중 are, is와 같은 am같은 be(표제어)를 말한다. \n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])\n",
    "# 리브가 라이프로 바꾼다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dba6a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "00c992cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('are','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8649d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2c758a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watching','v')\n",
    "# 단어정리를 할 때 매우 유용해보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adddba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간을 추출하는 작업을 해보자.(stemming)\n",
    "# 성능이 좋지만은 않다. 영어권의 학자들 중 쿼터, 레케스터가 만든 스팸어같은 것이다.\n",
    "# 약간씩 성능이 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f281b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # 포토스테머를 통해 스태밍을 해보자. \n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "956f2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical'] # 어간을 추출할 대상이다.\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])# 어간 추출 결과이다. \n",
    "# 이걸 만든 사람의 사이트이다. https://tartarus.org/martin/PorterStemmer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2503924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 후 : ['form', 'allow', 'elect']\n"
     ]
    }
   ],
   "source": [
    "print('어간 추출 후 :',[lancaster_stemmer.stem(word) for word in words])# 레케스터 스테머는 뭔가 나사가 빠져있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8869e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # 불용어 제거작업을 해보자.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc041831",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a3578b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #이것도 다운로드 받아야 한다.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b9bb2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english') # 불용어 사전이다. \n",
    "# 200여개의 단어가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5b7fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) # 불용어사전을 할당했다. \n",
    "\n",
    "word_tokens = word_tokenize(example) # 단어를 토큰화해서 담았따. \n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) # 단어가 만약 불용어사전에 없으면 담는다. \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n",
    "# 불용어를 제거하면서 is not an 이 빠졌다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1fc1c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "# 만약에 불용어를 추가하고 싶다면 stop_words에 append로 추가하자.\n",
    "# 한국어 불용어를 제거해보자. \n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\" # 불용어를 설정했다.\n",
    "\n",
    "stop_words = set(stop_words.split(' ')) # 불용어를 나눠서 세트에 할당했따. \n",
    "word_tokens = okt.morphs(example) \n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001818a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
