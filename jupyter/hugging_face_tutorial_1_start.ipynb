{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493df0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 튜토리얼\n",
    "# 허깅페이스는 모델 다운로드, 파인튜닝, API제동등을 담당하는 사이트이다. \n",
    "# 모델을 처음부터 훈련시키지 않아 시간과 리소스를 절햑할 수 있는 장점이 있다.\n",
    "# 트랜스포머는 텐서플로, 파이토치, 작스를 지원한다. 가장 많이 지원하는 형식은 파이토치이다. \n",
    "# 물론 다른 형식도 가능하면 파이토치로 훈련한 모델을 텐서플로 환경에서도 구동할 수 있다.\n",
    "# 해당 튜토리얼은 5단계로 구분될 것이다.\n",
    "# 1. 간략한 확인 / 설치\n",
    "# 2. 기본적인 부분 안내하기\n",
    "# 3. 사전학습된 모델을 파인튜닝하는 법 / 모델을 잓어하고 공유하는법\n",
    "# 4. 트랜스포머의 설계도, 개념 확인\n",
    "# 5. 모든 클래스와 함수 설명하기 ( 메인클래스에 대한 설명, 모델에 대한 설명, 내부 유틸리티 설명)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 종류\n",
    "# 모델은 대체로 4개의 모델로 분류된다. \n",
    "# 자연어 처리: 텍스트 분류, 개체명 인식, 질의응답, 언어 모델링, 요약, 번역, 객관식 질의응답, 텍스트 생성\n",
    "# 컴퓨터 비전: 이미지 분류, 객체 탐지, 객체 분할\n",
    "# 오디오: 자동음성인식, 오디오 분류\n",
    "# 멀티모달: 표 질의응답, 광학 문자 인식 (OCR), 스캔한 문서에서 정보 추출, 비디오 분류, 시각 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT\n",
    "# ALBERT는 \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\" 논문에서 소개된 모델로, \n",
    "# Google Research와 Chicago의 Toyota Technological Institute에서 개발되었다. 이 모델은 BERT의 라이트 버전으로, \n",
    "# 자기 지도 학습(self-supervised learning)을 사용하여 언어 표현을 학습한다.\n",
    "#\n",
    "# 특징\n",
    "# 경량화된 구조: ALBERT는 BERT의 파라미터 수를 줄이면서도 효과적인 언어 표현 학습을 가능하게 하는 구조적 개선을 도입하였다.\n",
    "# 이를 통해 더 작은 메모리와 더 빠른 학습 속도를 달성할 수 있다.\n",
    "# 교차 레이어 파라미터 공유(Cross-layer parameter sharing): ALBERT는 레이어 간의 파라미터를 공유하여 모델의 크기를 줄인다. \n",
    "# 이는 메모리 사용량을 줄이고, 오버피팅(overfitting)의 위험을 감소시키는데 유용하다.\n",
    "# 자기 지도 학습(Self-supervised learning): ALBERT는 문장의 일부를 가리고 (masked language model), \n",
    "# 이를 예측하는 방식으로 학습하며. 이를 통해 언어의 깊은 이해를 도모한다.\n",
    "# 다양한 언어 처리 작업에 적용 가능: ALBERT는 문장 분류, 질문 답변, 명명된 엔터티 인식 등 다양한 자연어 처리 작업에 \n",
    "# 사용될 수 있다.\n",
    "#\n",
    "# 사용 사례\n",
    "# 문장 분류: 감정 분석, 스팸 탐지 등 문장의 분류 작업에 사용된다.\n",
    "# 질문 답변 시스템: 사용자의 질문에 대한 답변을 생성하거나 관련 정보를 찾는 데 사용된다.\n",
    "# 텍스트 요약: 긴 문서를 요약하는 작업에 효과적으로 사용될 수 있다.\n",
    "# \n",
    "# 요약\n",
    "# ALBERT는 BERT와 유사한 아키텍처를 갖고 있지만, 파라미터 수가 적고, 학습 속도가 빠르며, 메모리 효율성이 높다는 장점을 갖고 있다.\n",
    "# 이러한 특징으로 인해, 자원이 제한적인 환경에서도 효과적으로 사용될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART\n",
    "# BART는 Facebook에서 개발한 모델로, \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, \n",
    "# Translation, and Comprehension\"이라는 논문을 통해 발표되었다. 이 모델은 자연어 생성(Natural Language Generation), 번역, \n",
    "# 이해에 중점을 두고 있는 시퀀스-투-시퀀스(sequence-to-sequence) 구조의 사전 훈련(pre-training) 모델이다.\n",
    "#  \n",
    "# 특징\n",
    "# 시퀀스-투-시퀀스 구조: BART는 인코더(encoder)와 디코더(decoder)를 포함하는 전통적인 시퀀스-투-시퀀스 구조를 사용한다. \n",
    "# 이 구조는 다양한 언어 처리 작업에 효과적입니다.\n",
    "# 노이즈 제거를 위한 학습: BART는 원문에서 노이즈(예: 문장의 일부 삭제, 순서 재배치 등)를 도입한 다음, \n",
    "# 이를 원래의 형태로 복원하는 방식으로 학습하며. 이를 통해 모델은 더 강력한 문맥 이해 능력과 텍스트 복원 능력을 개발한다.\n",
    "# 다양한 NLP 작업 적용 가능: BART는 텍스트 요약, 질문 응답, 기계 번역 등 다양한 자연어 처리 작업에 적용 가능하다.\n",
    "# 높은 성능: BART는 여러 자연어 처리 벤치마크에서 뛰어난 성능을 보여주었다.\n",
    "#\n",
    "# 사용 사례\n",
    "# 텍스트 요약: 긴 문서의 중요한 내용을 요약하는 데 사용된다.\n",
    "# 기계 번역: 하나의 언어에서 다른 언어로 문장을 번역하는 데 활용된다.\n",
    "# 질문 응답: 주어진 문맥에서 질문에 대한 답변을 생성한다.\n",
    "# \n",
    "# 요약\n",
    "# BART는 그 구조와 학습 방법으로 인해 다양한 언어 처리 작업에서 우수한 성능을 발휘한다.\n",
    "# 특히 텍스트 생성과 관련된 작업에서 강력한 성능을 보이며, 다양한 언어와 도메인에 걸쳐 활용될 수 있는 범용적인 모델이다. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6114df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a57d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032311d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4d85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f8b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ec69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6e87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2e4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed2326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410434b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e703cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd162323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7e8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e85027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0331eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b76a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ea468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b715c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03f8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac146b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929068d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8e17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcf7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타모델\n",
    "# BARThez : 프랑스어 전문 시퀀스-투-시퀀스(sequence-to-sequence) 모델\n",
    "# BARTpho : 베트남어 전문 시퀀스-투-시퀀스(sequence-to-sequence) 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 둘러보기\n",
    "# pipeline을 사용하여 추론하고, 사전학습된 모델과 전처리기를 AutoClass로 로드하고, \n",
    "# PyTorch 또는 TensorFlow로 모델을 빠르게 학습시키는 방법을 소개한다.\n",
    "# 시작하기 전에 필요한 라이브러리가 모두 설치되어 있는지 확인해야 한다. \n",
    "!pip install transformers datasets           \n",
    "# transformers datasets를 설치하는것부터 시작한다. \n",
    "# 만약 파이토치를 설치한다면\n",
    "pip install torch\n",
    "# 텐서플로를 설치한다면 \n",
    "pip install tensorflow\n",
    "# 선호하는 머신러닝 프레임워크를 설치하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b6763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fcce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031e166e37224cb78558f6176c8d520a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1146cb2696b44a098b1c3d310332dd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1db1e396b0e47c7af57758027b4ab66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92300c83e524cb19dfec522823cd77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8371170163154602}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17c289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8371170163154602},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 또는 원하는 라벨을 설정할 수 있다. \n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-ana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0d875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
