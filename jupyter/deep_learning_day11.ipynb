{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a16479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 다루기\n",
    "# https://wikidocs.net/109251\n",
    "# Transfomer가 등장한 이후 대부분의 RNN계열의 딥러닝은 트랜스포머로 대체되어져 갔다.\n",
    "# 여기서 트랜스포머 계열의 다양한 사전훈련 모델들이 등장하게 되었다. bert, gpt,t5등이 등장했다. \n",
    "# Bert, Albert, roberta, electra등이 있다. \n",
    "# https://jalammar.github.io/illustrated-bert/\n",
    "# 여기에서 잘 설명하고 있다. 한국어로 번역이 가능하니 한번 확인해보자. \n",
    "# 이번시간에는 bert를 학습하는 법을 배워볼 것이다. \n",
    "# 사전학습된 모델을 받을 수 있다.\n",
    "# 또한 모델을 파인튜닝을 통해서 모델을 원하는 방식으로 바꿀 수 있다. \n",
    "# 위키피디어와 북코퍼스로 버트 모델을 만들었다. 사전학습된 모델을 데이터셋을 추가학습을 해서 다양한 용도로 변형 사용가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56828f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트, 엘모 모두 노동요 인형의 이름에서 따왔다. \n",
    "# NLP는 이미 모델이 만들어져 있다. 관련 커뮤니티에서 어렵지 않게 모델과 파이프라인을 받을 수 있다. \n",
    "# bert는 이후 바드라는 인공지능 모델로 확장되었으며 gpt는 chat gpt로 확장된다. \n",
    "# bert 등장 이후 자연어 처리에 상당한 영향을 끼치게 되었다. \n",
    "# 33억개의 단어로 학습이 되어있다. 위키피디어와 북 코퍼스를 (최대 효율로)4일간 학습시킨 것이다. \n",
    "# 33억개가 학습되어져 잇는 pre trained를 다운받아서 자연어처리에 사용될 수 있게 된 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7503edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트모델 파인튜닝\n",
    "# 예를 들어 스팸메일, 메일을 분류하거나 문장들이 입력되었을 때 유사도 판별기도 만들 수 있다. \n",
    "# 서로 이어지는 문장인지, 관련있는 문장인지, 관련없는 문장인지 판별해주는 것도 만들 수 있다.\n",
    "# 버트모델을 파인튜닝하게 됨으로서 원하는 목적의 타겟시스템으로 버트모델을 사용할 수 있다. \n",
    "# 버트는 초등학교~고등학교까지의 기본과정을 거친 모델로 비유할 수 있다.\n",
    "# 기본적인 소양과 지식을 갖게 된 상태인 것이다. \n",
    "# 대학을 가게 되면 관심분야에 대해 특정 분야에 해당하는 전문적 지식을 쌓게 된다. \n",
    "# 전문적 지식은 기본적인 베이스 위에 올라올 수 있는 응용분야, 전문분야이다. 이걸 파인튜닝 하는 것이다. \n",
    "# 파인튜닝을 통하여 다양한 분류기, 생성기를 만들어낼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf5bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교수님도 기존의 버트 모델을 이용하여 판례검색기를 만든 적이 있다.\n",
    "# 이처럼 한국어 데이터를 추가학습 시켜서 bert에 전문교육을 대입하는 것이다. \n",
    "# 이러한 파인튜닝의 발전 가능성은 무궁무진하다. \n",
    "# 물론 전문영역임에는 틀림없다. 많은 사람들이 파인튜닝에 관해 관심을 가지기 시작한 이유다. \n",
    "# 앞으로의 시대는 파인튜닝의 시대이다. 개인이나 작은 회사는 모델을 만들 능력이나 비용이 없다. \n",
    "# 하지만 미리 만들어진 모델을 다운받아서 우리의 목적에 맞는 형태로 분류기를 제작할 수 있다. \n",
    "# 이 모델이 좋은 모델이 되려면 가장 중요한 것은 (여러가지 겠지만) 텍스트 데이터가 많아야 한다. \n",
    "# LLM(거대언어모델). 즉 대규모 텍스트로 만드는 모델의 중요성이 중요하다. \n",
    "# 기업 맞춤형 파인튜닝의 시대인 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbda9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수십억 웨이트 이상의 인공신경망으로 구축되어있다. 레이블링이 되지 않은 상당한 양의 텍스트로 훈련되어진다. \n",
    "# 취업시장에서 LLM관련하여 전쟁이 일어나고 있다. \n",
    "# 자연어처리일로 3~5년가량 공부하면서 내공을 많이 쌓아야 한다. \n",
    "# 논문을 많이 보는 것이 좋다. 면접에서 최근에 본 논문을 물어보는 경우가 많다. 연구색채가 강한 분야기 때문이다. \n",
    "# 레퍼런스 뒤에 보면 관련 연구가 많이 달린다. 어떤 연구를 통해서 지금의 결론이 나왔는지 확인하는 습관을 가지자.\n",
    "# 기업에서 요구하는 사람들은 논문을 많이 쓴 사람들이며 앞으로도 많이 쓸 사람들이다. \n",
    "# 프로젝트를 두 번 더 하게 될 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e80718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert가 어떻게 이용될 수 있는가?\n",
    "# 하나의 텍스트를 분류하는 것이 버트 모델의 메인 역할이다. \n",
    "# 연산을 한 결과가 소프트맥스를 거쳐서 결과를 도출하는 것이다. \n",
    "# 기존의 버트는 모델에 불과하다. 기존의 버트를 classifire로 만들기 위해서 훈련시키되 \n",
    "# 기존의 bert의 변화를 최소로 하는 것이 핵심이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76598f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금 현재에도 사전 훈련된 단어 임베딩(word 2 vec, fastText,glove)이 사전 훈련된 언어모델에 대체되고 있다. \n",
    "# 모델을 훈련시키는 방식은 두 가지이다. \n",
    "# 임베딩 레이어를 초기화하여 처음부터 학습하는 방법이있다. \n",
    "# 또는 사전학습된 임베딩 벡터들을 가져와서 모델을 학습시키는 방법이 있다. \n",
    "# 문제는 다의어와 동음이의어를 구분하지 못하는 문제점이 발생한다. \n",
    "# 이러한 다양한 문제들을 극복하기 위해 사전훈련된 언어모델을 이용해서 극복할 수 있게 되었다. \n",
    "# 엘모와 버트가 그 해결책이 되었다. \n",
    "# 워드 투 벡터는 단어단위로 쪼개서 사용했을 것이다.\n",
    "# 이번것은 다르다. 문장을 먼저 받는다. 공란을 먼저 하나 둔다. 그 문장안에 들어가야 할 단어를 추측하는 방식으로 훈련한다.\n",
    "# 문장에 따라 다양한 방법으로 훈련할 수 있다. \n",
    "# 사전학습 모델을 이전 단어들로 다음 단어를 예측하도록 계속 학습해왔다. 문장을 가지고 다음 단어를 예측하게 만든 언어모델이다. \n",
    "# 비유하자면 고등학교까지의 학습과정을 수료한 것이다. 파인튜닝은 대학교육과정이다.  \n",
    "# 레이블은 별도로 지정하지 않았다. 왜냐하면 나온 단어가 레이블이 되기 때문이다. \n",
    "# 미리 언어모델을 만들고 언어모델을 가지고 와서 적절한 데이터를 가져와서 추가학습을 하여 모델을 제작하는 것이 대세가 되었다.\n",
    "# 방대한 텍스트로 lstm 언어 모델을 학습해둔다. 텍스트 전처리를 상당히 잘해야 하며 데이터가 쓰레기면 결과가 쓰레기가 된다.\n",
    "# 의도적으로 편향적 모델이 나올 수도 있다. AI는 중립적이지만 제작자가 편향적이면 AI도 편향적이게 변한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7cc9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘모\n",
    "# 2017년에 새로운 논문이 발표되었다. \n",
    "# 엘모는 순방향 언어모델과 역방향 언어모델을 따로 학습시킨 이후 사전 학습된 언어 모델로부터 임베딩 값을 얻는 다는 아이디어였다.\n",
    "# 문맥에 따라 벡터 값이 달라지기 때문에 기존 워드 임베딩의 문제를 해결할 수 있었다.\n",
    "# 트랜스포머로 사전학습된 언어모델을 학습하는 시도가 등장했다.\n",
    "#\n",
    "# 기존 모델은 언어의 문맥이 단방향으로 움직였다. 하지만 언어의 문맥은 양방향이다. \n",
    "# 하지만 일반적으로는 이런 절차가 힘들었다. 그래서 엘모는 순방향, 역방향을 따로 훈련시켜서 준비하는 방법을 적용하였다.\n",
    "# 단어의 맥락이 중요하다고 판단한 것이다. 단어의 의미 뿐 아니라 맥락의 의미도 전부 파악하는 것이 핵심이다.  \n",
    "# contextualized word-ebmedding가 탄생하는 순간이다. \n",
    "# 사과는 하나의 벡터로 존재한다. 하지만 문맥에 대한 정보를 포함하면 사과는 단어가 같아도 문맥에 따라 다른벡터가 된다. \n",
    "# 엘모는 모든 문장이 문맥에 대한 정보가 담기는 벡터가 될 수 있도록 했다. \n",
    "# 엘모의 시작은 곧 버트의 성장과 연결되어진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508eaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt1\n",
    "# 트랜스포머 디코더를 12개 층을 쌓아서 방대한 텍스트 데이터를 학습시킨 것이다. \n",
    "# 사전훈련된 언어모델을 만들고 특정 태스크에 추가학습을 시켜 해당 태스크에서 높은 성능을 얻는 것이 중요해졌다. \n",
    "# 프롬프트에 질문 답변을 넣어서 학습시키는 방법으로 학습을 한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763b559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스크드 언어모델\n",
    "# 언어 중 15%부분의 단어를 랜덤으로 마스킹(가리기, 블랭크)를 한 다음 구멍에 들어간 단어들을 예측하게 하는 방식으로 학습했다.\n",
    "# 버트는 그런 방식으로 학습했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4987068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트\n",
    "# 버트는 2018년에 구글에서 공개된 사전훈련된 모델이다. NLP태그에서 최고성능을 보여줬다.\n",
    "# 버트를 사용하기 위해서 pip install transformers를 먼저 사용해서 설치해야 한다. \n",
    "# 33억개의 단어를 통해 학습이 되어졌다. \n",
    "# 레이블이 없는 방대한 데이터를 사전훈련된 모델을 가지고 레이블이 있는 다른 작업에서 추가훈련과 함께 \n",
    "# 하이퍼파라미터를 재조정했다.(파인 튜닝)\n",
    "# 위키피디어에는 많은 문장이 존재한다. 여기에 중간에 blank를 뚫어놓고 계속 훈련한 것이다. \n",
    "# 여기에 분류작업을 하기 위한 신경망 구조를 얹는다. \n",
    "# 사전학습에서 얻은 지식이 이 과정에서 활용된다. 기본적인 상식을 가지고 있는 상태인 것이다. \n",
    "# 버트 베이스는 12개의 레이어가 존재한다. 버트 라지는 24개의 레이어가 존재한다. \n",
    "# 그래서 베이스는 레이어 12, 768 히든, 셀프 어텐션헤드 12개이다. 110m개의 파라미터가 존재한다.  \n",
    "# 라지는 레이어 24, 1024히든 , 16 어텐션헤드로 구성된다. 340m개의 파라미터가 존재한다.\n",
    "# 셀프 어텐션 헤드는 다양한 시각으로 바라보는 시점을 말한다. 모델은 성능적인 측면에서 더 올라갔다. \n",
    "# 현재는 gpt4프로가 출시되었다. \n",
    "# bert는 문맥을 반영한 임베딩을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78e40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert는 문맥을 반영한 임베딩을 사용한다. \n",
    "# CLS : classification 토큰이라는 특수 토큰이 존재한다. \n",
    "# 그리고 베이스 기준으로 12개의 인코더를 타고 올라간다. \n",
    "# 인코더 내에서 셀프 어텐션, 피드포워드가 여러 번 반복되어 이어진다.\n",
    "# 인코딩하는 단계는 동일하다. 다만 출력부가 다르다. \n",
    "# bertbase는 768차원이다.  각 단어에 대해 768차원이 할당된다는 것이다. \n",
    "# 만약 단어 입력, 출력에서 햄메일, 스팸메일을 구분한다면 출력차원의 CLS토큰만 본다. \n",
    "# 이 CLS가 분류기의 핵심이다. \n",
    "# 만약 다중분류기로 변형한다면 출력을 바꾸는 것도 가능하다. \n",
    "# [cls] i love you 라는 4개의 벡터를 입력받아서 [cls] i love you 4개의 벡터를 출력한다\n",
    "# 지나기 전에는 단순한 정렬이었지만 통과한 이후 모델은 모든 문맥정보를 가지고 있다. \n",
    "# 버트는 단어를 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용한다. \n",
    "# 만약에 사전에 없는 단어(OOV)가 나왔다고 해보자. 그러면 단어를 쪼개서 버트의 단어집합에 있는지 없는지를 먼저 찾아본다. \n",
    "# embeddings가 OOV라고 하면 em,##bed,##ding,#s로 분류한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26419fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN과 여러모로 비슷한다. 마지막 토큰을 활용한다. \n",
    "# 문장에 대한 전체적인 내용이 들어가 있다. \n",
    "# 마지막 부분은 구조를 변경해서 다양한 예측결과를 도출할 수 있다. \n",
    "# 작은 데이터셋에서 처음부터 학습하는것보다 미리 훈련된 데이터를 이용하는 것이 효율적이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드 토크나이저\n",
    "# 신조어나 유행어같은 경우는 OOV로 분류되는 경우가 일반적이다. 문제는 이러한 신조어나 유행어는 문제 해결에 지장을 준다.\n",
    "# 그래서 하나의 단어를 여러 서브워드로 분리하여 단어를 인코딩 / 임베딩하겠다는 의도를 가진 전처리 작업이다. \n",
    "# 서브워드 코트나이저는 센턴스 피스와 허깅페이스에서 사용할 수 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04520ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바이트페어 인코딩 (OOV문제 해결하는 방법)\n",
    "# bpe는 데이터 압축 알고리즘이다. 연속적으로 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식이다.\n",
    "# aa를 Z로 합치는 방식이다. 두 글자가 쌍으로 있는 것을 특정 바이트 쌍으로 합치는 것이다.\n",
    "# aaabdaaabac -> ZabdZabac->ZYdZYac->XdXac\n",
    "#                Z=aa        Y=ab     X=ZY\n",
    "#                            Z=aa     Y=ab\n",
    "#                                     Z=aa\n",
    "# 기존 방식\n",
    "# 자연어 처리에서도 같다. 등장한 단어 빈도수로 딕셔너리를 기록한다.\n",
    "# 즉 아예 새로운 단어나 오타는 OOV로 취급될 수 밖에 없었다. apple과 aple은 아예다른 말이다. 사람은 일부 알아들어도 컴퓨터는 아니다.\n",
    "# bpe는 이 방식을 글자단위로 분리했다. 중복을 제외한 문자집합으로 구분한다. \n",
    "# 여기서 2글자로 된 글자를 쌍으로 해서 묶어버린다. 만약 단어갯수가 같은데 더 내용이 길다면 그 내용으로도 묶인다.  \n",
    "# 이미 내부로 구현이 전부 되어져 있다. \n",
    "# 단어빈도수가 높은 것을 체크해서 추가하는 것이다. \n",
    "# 만약 그 단어집합에서 조합하여 OOV가 나타나면 그 것은 더 이상 OOV가 아니게 된다. \n",
    "# 이를 기반으로 wordpiece tokenizer가 나왔다. 버트 모델을 만들 당시에 사용되었다. \n",
    "# 유니그램 랭귀지 모델 토크나이저도 같은 시기에 나오게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센텐스피스. \n",
    "# 일종의 패키지이다. \n",
    "# 워드 투 벡터는 한창 예전에 쓰던 것인데 아직 바뀌지 않는 경우가 많다. 그래서 OOV로 고생하는 경우도 많다.\n",
    "# 센텐스피스는 BPE 알고리즘이 포함되어 있다. \n",
    "# 내부단어 분리를 위한 유용한 패키지로 구글의 센텐스피스가 있다. \n",
    "# 데이터를 단어 단위로 토큰화를 진행한다. 한국어는 형태소 분석기를 써야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dcad009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sentencepiec (from versions: none)\n",
      "ERROR: No matching distribution found for sentencepiec\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiec         # 센턴스피스 패키지를 다운받아보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf74458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm     # 센턴스피스 패키지이다. \n",
    "import pandas as pd             # 판다스를 호출하였다. \n",
    "import urllib.request           # 리퀘스트로 외부 링크를 호출한다. \n",
    "import csv                      # 이걸 쓰는 이유는 텍스트를 읽어올 목적으로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51670079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x1f9339e4dd0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
    "# 리뷰 데이터를 다운로드 받아보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80e9da3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('IMDb_Reviews.csv') # 받은 csv파일을 받아보자. \n",
    "train_df['review']                         # 리뷰를 한번 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707432c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdb_review.txt', 'w', encoding='utf8') as \n",
    "    f.write('\\n'.join(train_df['review']))             # f:파일 핸들러 f를 통해 'imdb_review.txt' 파일에 데이터를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7335c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\n",
    "# 단어사전을 만드는 작업이다. 기존 단어의 서브워드들로 구성되어져 있을 것이다. 센턴스피스를 이용하여 단어단위로 나눈다. \n",
    "# vocabsize는 단어를 5천개로 제한한 것이다. 모델타입은 bpe알고리즘을 적용했다. , 문장의 최대길이는 9999로 설정되었다.\n",
    "# 아마 모델이 이미 생성되었을 것이다. imdb vocab에는 단어를 전부 정리한 것들이 나올 것이다. \n",
    "# read_csv로 읽어야 한다. tab기호로 구분해서 읽고 헤더를 없이 읽으야 할 듯 하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3944bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁a</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>8</td>\n",
       "      <td>-4992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4</td>\n",
       "      <td>-4993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>7</td>\n",
       "      <td>-4994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>-4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>6</td>\n",
       "      <td>-4996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1\n",
       "0     <unk>     0\n",
       "1       <s>     0\n",
       "2      </s>     0\n",
       "3        ▁t     0\n",
       "4        ▁a    -1\n",
       "...     ...   ...\n",
       "4995      8 -4992\n",
       "4996      4 -4993\n",
       "4997      7 -4994\n",
       "4998      & -4995\n",
       "4999      6 -4996\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('imdb.vocab',sep='\\t',header = None,quoting=csv.QUOTE_NONE)# 꺾쇠기호'<>'로 인해서 불러와지지 않았다. \n",
    "# 그래서 쿼팅을 통해서 불러오면 된다. 쿼트캐릭터를 말한다. 'hi,hello'같은 것이 나오면 구분자가 ,로 나눠지는 참사가 벌어질 수 있다. \n",
    "# 이러한 상황을 처리할 경우에 쓰는 것이 쿼트캐릭터이다. 묶는 작업을 어떻게 할 것이냐? 다.\n",
    "# 0번에는 기호가 들어가고 1번은 번호가 들어가있다. 바이트페어로 인코딩이 되어져있다. \n",
    "# \n",
    "# csv 모듈은 이래서 쓰인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02289fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>▁gone</td>\n",
       "      <td>-2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>roy</td>\n",
       "      <td>-2742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>▁mel</td>\n",
       "      <td>-2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>▁breath</td>\n",
       "      <td>-3741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>so</td>\n",
       "      <td>-315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>▁boyfriend</td>\n",
       "      <td>-3410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>▁stars</td>\n",
       "      <td>-1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>▁fine</td>\n",
       "      <td>-1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>ras</td>\n",
       "      <td>-4318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>erest</td>\n",
       "      <td>-568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0     1\n",
       "2484       ▁gone -2481\n",
       "2745         roy -2742\n",
       "2926        ▁mel -2923\n",
       "3744     ▁breath -3741\n",
       "318           so  -315\n",
       "3413  ▁boyfriend -3410\n",
       "1442      ▁stars -1439\n",
       "1554       ▁fine -1551\n",
       "4321         ras -4318\n",
       "571        erest  -568"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_list.sample(10)\n",
    "# 이제 단어의 시퀀스를 정수로 바꿔보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "733cdb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()            # 센텐스피스 프로세서를 불러오자. \n",
    "vocab_file = \"imdb.model\"                    # 모델파일을 불러오자. \n",
    "sp.load(vocab_file)                          # 모델을 불러오기 해서 단어를 정수로 변환하는 작업이다. \n",
    "                                             # 위에 있는 단어들은 전부 읽어졌다. 단어들 각각에 대해 정수로 변환한다. \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6be8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "  \"I didn't at all think of it this way.\",\n",
    "  \"I have waited a long time for someone to film\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd68aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I',\n",
       " '▁didn',\n",
       " \"'\",\n",
       " 't',\n",
       " '▁at',\n",
       " '▁all',\n",
       " '▁think',\n",
       " '▁of',\n",
       " '▁it',\n",
       " '▁this',\n",
       " '▁way',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode_as_pieces(lines[0]) # 각각의 단어들이 쪼개진 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f91b5948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 624, 4950, 4926, 139, 170, 378, 30, 58, 73, 413, 4945]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode_as_ids(lines[0]) # 각각의 단어들이 번호화된 것을 볼 수 있다. \n",
    "# 문장을 정수코드로 변환해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46ed1d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('▁I') #이런 식으로 구현된 숫자를 확인하는 것도 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c2b032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x1f93eea7790>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "# 네이버 영화 리뷰 데이터이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cde07bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df = pd.read_table('ratings.txt')\n",
    "naver_df[:5]\n",
    "# 20만개의 리뷰가 존재하는 네이버 평점이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6e2ad42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cf81327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수 : 199992\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력\n",
    "# 꽤 많다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e40718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf8') as f:       # 네이버 리뷰 텍스트를 쓰기용으로 열어서 f에 할당한다. \n",
    "    f.write('\\n'.join(naver_df['document']))                     # 네이버 df의 document에 저장되었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd19ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fee380a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>영화</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁영화</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁이</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁아</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ᄏᄏ</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  <unk>  0\n",
       "1    <s>  0\n",
       "2   </s>  0\n",
       "3     ..  0\n",
       "4     영화 -1\n",
       "5    ▁영화 -2\n",
       "6     ▁이 -3\n",
       "7     ▁아 -4\n",
       "8    ... -5\n",
       "9     ᄏᄏ -6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab_list[:10]\n",
    "# 영화가 끝에 있으면 언더바가 붙고 중간에 있으면 언더바가 있는 모양이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34a34019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "vocab_file = \"naver.model\"\n",
    "sp.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffbab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "  \"뭐 이딴 것도 영화냐.\",\n",
    "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51dba34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.'], ['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces(lines))  # 분리가 잘 이루어진 것을 볼 수 있다. OOV가 아니라 제대로 인코딩 되어있다. \n",
    "# 만약 '영화입니다'가 없으면 '영화'와 '입니다'로 분리될 것이다. \n",
    "# 이러한 발표는 OOV문제를 지금과 같이  해결할 수 있게 되었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0741ae91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'진짜 최고의 영화입니다 ᄏᄏ'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])\n",
    "# 서브워드를 투입하여 원래문장을 만들 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ed215ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'진짜 원 산~~'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds([54, 200, 821, 85])\n",
    "# 아이디를 주고서도 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dea35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
      "[54, 204, 825, 121]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\n",
    "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))\n",
    "# 아웃타입을 주는 방식을 결정할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "568dfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어에서도 연구가 많이 이루어진다. 초성, 중성, 종성을 나누는 시도가 많았다. \n",
    "# 위에 쓰이는 센턴스피스 (BPE)는 버트의 기반이 되었다. \n",
    "# 토큰화 수행순서는 다음과 같다.\n",
    "# 1. 토큰이 존재한다. - > 토큰을 분리하지 않는다.\n",
    "# 2. 토큰이 존재하지 않는다. - > 해당 토큰을 서브 워드로 분리한다. 첫번째 서브워드를 제외한 나머지에 ##을 붙인다. \n",
    "# 샵 두개는 단어 중간부터 등장하는 서브워드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "529a0a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 231508/231508 [00:01<00:00, 146273.47B/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Bert-base의 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "933d8de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size # vocabsize는 3만개나 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd67a8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[unused0]', 1),\n",
       "             ('[unused1]', 2),\n",
       "             ('[unused2]', 3),\n",
       "             ('[unused3]', 4),\n",
       "             ('[unused4]', 5),\n",
       "             ('[unused5]', 6),\n",
       "             ('[unused6]', 7),\n",
       "             ('[unused7]', 8),\n",
       "             ('[unused8]', 9),\n",
       "             ('[unused9]', 10),\n",
       "             ('[unused10]', 11),\n",
       "             ('[unused11]', 12),\n",
       "             ('[unused12]', 13),\n",
       "             ('[unused13]', 14),\n",
       "             ('[unused14]', 15),\n",
       "             ('[unused15]', 16),\n",
       "             ('[unused16]', 17),\n",
       "             ('[unused17]', 18),\n",
       "             ('[unused18]', 19),\n",
       "             ('[unused19]', 20),\n",
       "             ('[unused20]', 21),\n",
       "             ('[unused21]', 22),\n",
       "             ('[unused22]', 23),\n",
       "             ('[unused23]', 24),\n",
       "             ('[unused24]', 25),\n",
       "             ('[unused25]', 26),\n",
       "             ('[unused26]', 27),\n",
       "             ('[unused27]', 28),\n",
       "             ('[unused28]', 29),\n",
       "             ('[unused29]', 30),\n",
       "             ('[unused30]', 31),\n",
       "             ('[unused31]', 32),\n",
       "             ('[unused32]', 33),\n",
       "             ('[unused33]', 34),\n",
       "             ('[unused34]', 35),\n",
       "             ('[unused35]', 36),\n",
       "             ('[unused36]', 37),\n",
       "             ('[unused37]', 38),\n",
       "             ('[unused38]', 39),\n",
       "             ('[unused39]', 40),\n",
       "             ('[unused40]', 41),\n",
       "             ('[unused41]', 42),\n",
       "             ('[unused42]', 43),\n",
       "             ('[unused43]', 44),\n",
       "             ('[unused44]', 45),\n",
       "             ('[unused45]', 46),\n",
       "             ('[unused46]', 47),\n",
       "             ('[unused47]', 48),\n",
       "             ('[unused48]', 49),\n",
       "             ('[unused49]', 50),\n",
       "             ('[unused50]', 51),\n",
       "             ('[unused51]', 52),\n",
       "             ('[unused52]', 53),\n",
       "             ('[unused53]', 54),\n",
       "             ('[unused54]', 55),\n",
       "             ('[unused55]', 56),\n",
       "             ('[unused56]', 57),\n",
       "             ('[unused57]', 58),\n",
       "             ('[unused58]', 59),\n",
       "             ('[unused59]', 60),\n",
       "             ('[unused60]', 61),\n",
       "             ('[unused61]', 62),\n",
       "             ('[unused62]', 63),\n",
       "             ('[unused63]', 64),\n",
       "             ('[unused64]', 65),\n",
       "             ('[unused65]', 66),\n",
       "             ('[unused66]', 67),\n",
       "             ('[unused67]', 68),\n",
       "             ('[unused68]', 69),\n",
       "             ('[unused69]', 70),\n",
       "             ('[unused70]', 71),\n",
       "             ('[unused71]', 72),\n",
       "             ('[unused72]', 73),\n",
       "             ('[unused73]', 74),\n",
       "             ('[unused74]', 75),\n",
       "             ('[unused75]', 76),\n",
       "             ('[unused76]', 77),\n",
       "             ('[unused77]', 78),\n",
       "             ('[unused78]', 79),\n",
       "             ('[unused79]', 80),\n",
       "             ('[unused80]', 81),\n",
       "             ('[unused81]', 82),\n",
       "             ('[unused82]', 83),\n",
       "             ('[unused83]', 84),\n",
       "             ('[unused84]', 85),\n",
       "             ('[unused85]', 86),\n",
       "             ('[unused86]', 87),\n",
       "             ('[unused87]', 88),\n",
       "             ('[unused88]', 89),\n",
       "             ('[unused89]', 90),\n",
       "             ('[unused90]', 91),\n",
       "             ('[unused91]', 92),\n",
       "             ('[unused92]', 93),\n",
       "             ('[unused93]', 94),\n",
       "             ('[unused94]', 95),\n",
       "             ('[unused95]', 96),\n",
       "             ('[unused96]', 97),\n",
       "             ('[unused97]', 98),\n",
       "             ('[unused98]', 99),\n",
       "             ('[UNK]', 100),\n",
       "             ('[CLS]', 101),\n",
       "             ('[SEP]', 102),\n",
       "             ('[MASK]', 103),\n",
       "             ('[unused99]', 104),\n",
       "             ('[unused100]', 105),\n",
       "             ('[unused101]', 106),\n",
       "             ('[unused102]', 107),\n",
       "             ('[unused103]', 108),\n",
       "             ('[unused104]', 109),\n",
       "             ('[unused105]', 110),\n",
       "             ('[unused106]', 111),\n",
       "             ('[unused107]', 112),\n",
       "             ('[unused108]', 113),\n",
       "             ('[unused109]', 114),\n",
       "             ('[unused110]', 115),\n",
       "             ('[unused111]', 116),\n",
       "             ('[unused112]', 117),\n",
       "             ('[unused113]', 118),\n",
       "             ('[unused114]', 119),\n",
       "             ('[unused115]', 120),\n",
       "             ('[unused116]', 121),\n",
       "             ('[unused117]', 122),\n",
       "             ('[unused118]', 123),\n",
       "             ('[unused119]', 124),\n",
       "             ('[unused120]', 125),\n",
       "             ('[unused121]', 126),\n",
       "             ('[unused122]', 127),\n",
       "             ('[unused123]', 128),\n",
       "             ('[unused124]', 129),\n",
       "             ('[unused125]', 130),\n",
       "             ('[unused126]', 131),\n",
       "             ('[unused127]', 132),\n",
       "             ('[unused128]', 133),\n",
       "             ('[unused129]', 134),\n",
       "             ('[unused130]', 135),\n",
       "             ('[unused131]', 136),\n",
       "             ('[unused132]', 137),\n",
       "             ('[unused133]', 138),\n",
       "             ('[unused134]', 139),\n",
       "             ('[unused135]', 140),\n",
       "             ('[unused136]', 141),\n",
       "             ('[unused137]', 142),\n",
       "             ('[unused138]', 143),\n",
       "             ('[unused139]', 144),\n",
       "             ('[unused140]', 145),\n",
       "             ('[unused141]', 146),\n",
       "             ('[unused142]', 147),\n",
       "             ('[unused143]', 148),\n",
       "             ('[unused144]', 149),\n",
       "             ('[unused145]', 150),\n",
       "             ('[unused146]', 151),\n",
       "             ('[unused147]', 152),\n",
       "             ('[unused148]', 153),\n",
       "             ('[unused149]', 154),\n",
       "             ('[unused150]', 155),\n",
       "             ('[unused151]', 156),\n",
       "             ('[unused152]', 157),\n",
       "             ('[unused153]', 158),\n",
       "             ('[unused154]', 159),\n",
       "             ('[unused155]', 160),\n",
       "             ('[unused156]', 161),\n",
       "             ('[unused157]', 162),\n",
       "             ('[unused158]', 163),\n",
       "             ('[unused159]', 164),\n",
       "             ('[unused160]', 165),\n",
       "             ('[unused161]', 166),\n",
       "             ('[unused162]', 167),\n",
       "             ('[unused163]', 168),\n",
       "             ('[unused164]', 169),\n",
       "             ('[unused165]', 170),\n",
       "             ('[unused166]', 171),\n",
       "             ('[unused167]', 172),\n",
       "             ('[unused168]', 173),\n",
       "             ('[unused169]', 174),\n",
       "             ('[unused170]', 175),\n",
       "             ('[unused171]', 176),\n",
       "             ('[unused172]', 177),\n",
       "             ('[unused173]', 178),\n",
       "             ('[unused174]', 179),\n",
       "             ('[unused175]', 180),\n",
       "             ('[unused176]', 181),\n",
       "             ('[unused177]', 182),\n",
       "             ('[unused178]', 183),\n",
       "             ('[unused179]', 184),\n",
       "             ('[unused180]', 185),\n",
       "             ('[unused181]', 186),\n",
       "             ('[unused182]', 187),\n",
       "             ('[unused183]', 188),\n",
       "             ('[unused184]', 189),\n",
       "             ('[unused185]', 190),\n",
       "             ('[unused186]', 191),\n",
       "             ('[unused187]', 192),\n",
       "             ('[unused188]', 193),\n",
       "             ('[unused189]', 194),\n",
       "             ('[unused190]', 195),\n",
       "             ('[unused191]', 196),\n",
       "             ('[unused192]', 197),\n",
       "             ('[unused193]', 198),\n",
       "             ('[unused194]', 199),\n",
       "             ('[unused195]', 200),\n",
       "             ('[unused196]', 201),\n",
       "             ('[unused197]', 202),\n",
       "             ('[unused198]', 203),\n",
       "             ('[unused199]', 204),\n",
       "             ('[unused200]', 205),\n",
       "             ('[unused201]', 206),\n",
       "             ('[unused202]', 207),\n",
       "             ('[unused203]', 208),\n",
       "             ('[unused204]', 209),\n",
       "             ('[unused205]', 210),\n",
       "             ('[unused206]', 211),\n",
       "             ('[unused207]', 212),\n",
       "             ('[unused208]', 213),\n",
       "             ('[unused209]', 214),\n",
       "             ('[unused210]', 215),\n",
       "             ('[unused211]', 216),\n",
       "             ('[unused212]', 217),\n",
       "             ('[unused213]', 218),\n",
       "             ('[unused214]', 219),\n",
       "             ('[unused215]', 220),\n",
       "             ('[unused216]', 221),\n",
       "             ('[unused217]', 222),\n",
       "             ('[unused218]', 223),\n",
       "             ('[unused219]', 224),\n",
       "             ('[unused220]', 225),\n",
       "             ('[unused221]', 226),\n",
       "             ('[unused222]', 227),\n",
       "             ('[unused223]', 228),\n",
       "             ('[unused224]', 229),\n",
       "             ('[unused225]', 230),\n",
       "             ('[unused226]', 231),\n",
       "             ('[unused227]', 232),\n",
       "             ('[unused228]', 233),\n",
       "             ('[unused229]', 234),\n",
       "             ('[unused230]', 235),\n",
       "             ('[unused231]', 236),\n",
       "             ('[unused232]', 237),\n",
       "             ('[unused233]', 238),\n",
       "             ('[unused234]', 239),\n",
       "             ('[unused235]', 240),\n",
       "             ('[unused236]', 241),\n",
       "             ('[unused237]', 242),\n",
       "             ('[unused238]', 243),\n",
       "             ('[unused239]', 244),\n",
       "             ('[unused240]', 245),\n",
       "             ('[unused241]', 246),\n",
       "             ('[unused242]', 247),\n",
       "             ('[unused243]', 248),\n",
       "             ('[unused244]', 249),\n",
       "             ('[unused245]', 250),\n",
       "             ('[unused246]', 251),\n",
       "             ('[unused247]', 252),\n",
       "             ('[unused248]', 253),\n",
       "             ('[unused249]', 254),\n",
       "             ('[unused250]', 255),\n",
       "             ('[unused251]', 256),\n",
       "             ('[unused252]', 257),\n",
       "             ('[unused253]', 258),\n",
       "             ('[unused254]', 259),\n",
       "             ('[unused255]', 260),\n",
       "             ('[unused256]', 261),\n",
       "             ('[unused257]', 262),\n",
       "             ('[unused258]', 263),\n",
       "             ('[unused259]', 264),\n",
       "             ('[unused260]', 265),\n",
       "             ('[unused261]', 266),\n",
       "             ('[unused262]', 267),\n",
       "             ('[unused263]', 268),\n",
       "             ('[unused264]', 269),\n",
       "             ('[unused265]', 270),\n",
       "             ('[unused266]', 271),\n",
       "             ('[unused267]', 272),\n",
       "             ('[unused268]', 273),\n",
       "             ('[unused269]', 274),\n",
       "             ('[unused270]', 275),\n",
       "             ('[unused271]', 276),\n",
       "             ('[unused272]', 277),\n",
       "             ('[unused273]', 278),\n",
       "             ('[unused274]', 279),\n",
       "             ('[unused275]', 280),\n",
       "             ('[unused276]', 281),\n",
       "             ('[unused277]', 282),\n",
       "             ('[unused278]', 283),\n",
       "             ('[unused279]', 284),\n",
       "             ('[unused280]', 285),\n",
       "             ('[unused281]', 286),\n",
       "             ('[unused282]', 287),\n",
       "             ('[unused283]', 288),\n",
       "             ('[unused284]', 289),\n",
       "             ('[unused285]', 290),\n",
       "             ('[unused286]', 291),\n",
       "             ('[unused287]', 292),\n",
       "             ('[unused288]', 293),\n",
       "             ('[unused289]', 294),\n",
       "             ('[unused290]', 295),\n",
       "             ('[unused291]', 296),\n",
       "             ('[unused292]', 297),\n",
       "             ('[unused293]', 298),\n",
       "             ('[unused294]', 299),\n",
       "             ('[unused295]', 300),\n",
       "             ('[unused296]', 301),\n",
       "             ('[unused297]', 302),\n",
       "             ('[unused298]', 303),\n",
       "             ('[unused299]', 304),\n",
       "             ('[unused300]', 305),\n",
       "             ('[unused301]', 306),\n",
       "             ('[unused302]', 307),\n",
       "             ('[unused303]', 308),\n",
       "             ('[unused304]', 309),\n",
       "             ('[unused305]', 310),\n",
       "             ('[unused306]', 311),\n",
       "             ('[unused307]', 312),\n",
       "             ('[unused308]', 313),\n",
       "             ('[unused309]', 314),\n",
       "             ('[unused310]', 315),\n",
       "             ('[unused311]', 316),\n",
       "             ('[unused312]', 317),\n",
       "             ('[unused313]', 318),\n",
       "             ('[unused314]', 319),\n",
       "             ('[unused315]', 320),\n",
       "             ('[unused316]', 321),\n",
       "             ('[unused317]', 322),\n",
       "             ('[unused318]', 323),\n",
       "             ('[unused319]', 324),\n",
       "             ('[unused320]', 325),\n",
       "             ('[unused321]', 326),\n",
       "             ('[unused322]', 327),\n",
       "             ('[unused323]', 328),\n",
       "             ('[unused324]', 329),\n",
       "             ('[unused325]', 330),\n",
       "             ('[unused326]', 331),\n",
       "             ('[unused327]', 332),\n",
       "             ('[unused328]', 333),\n",
       "             ('[unused329]', 334),\n",
       "             ('[unused330]', 335),\n",
       "             ('[unused331]', 336),\n",
       "             ('[unused332]', 337),\n",
       "             ('[unused333]', 338),\n",
       "             ('[unused334]', 339),\n",
       "             ('[unused335]', 340),\n",
       "             ('[unused336]', 341),\n",
       "             ('[unused337]', 342),\n",
       "             ('[unused338]', 343),\n",
       "             ('[unused339]', 344),\n",
       "             ('[unused340]', 345),\n",
       "             ('[unused341]', 346),\n",
       "             ('[unused342]', 347),\n",
       "             ('[unused343]', 348),\n",
       "             ('[unused344]', 349),\n",
       "             ('[unused345]', 350),\n",
       "             ('[unused346]', 351),\n",
       "             ('[unused347]', 352),\n",
       "             ('[unused348]', 353),\n",
       "             ('[unused349]', 354),\n",
       "             ('[unused350]', 355),\n",
       "             ('[unused351]', 356),\n",
       "             ('[unused352]', 357),\n",
       "             ('[unused353]', 358),\n",
       "             ('[unused354]', 359),\n",
       "             ('[unused355]', 360),\n",
       "             ('[unused356]', 361),\n",
       "             ('[unused357]', 362),\n",
       "             ('[unused358]', 363),\n",
       "             ('[unused359]', 364),\n",
       "             ('[unused360]', 365),\n",
       "             ('[unused361]', 366),\n",
       "             ('[unused362]', 367),\n",
       "             ('[unused363]', 368),\n",
       "             ('[unused364]', 369),\n",
       "             ('[unused365]', 370),\n",
       "             ('[unused366]', 371),\n",
       "             ('[unused367]', 372),\n",
       "             ('[unused368]', 373),\n",
       "             ('[unused369]', 374),\n",
       "             ('[unused370]', 375),\n",
       "             ('[unused371]', 376),\n",
       "             ('[unused372]', 377),\n",
       "             ('[unused373]', 378),\n",
       "             ('[unused374]', 379),\n",
       "             ('[unused375]', 380),\n",
       "             ('[unused376]', 381),\n",
       "             ('[unused377]', 382),\n",
       "             ('[unused378]', 383),\n",
       "             ('[unused379]', 384),\n",
       "             ('[unused380]', 385),\n",
       "             ('[unused381]', 386),\n",
       "             ('[unused382]', 387),\n",
       "             ('[unused383]', 388),\n",
       "             ('[unused384]', 389),\n",
       "             ('[unused385]', 390),\n",
       "             ('[unused386]', 391),\n",
       "             ('[unused387]', 392),\n",
       "             ('[unused388]', 393),\n",
       "             ('[unused389]', 394),\n",
       "             ('[unused390]', 395),\n",
       "             ('[unused391]', 396),\n",
       "             ('[unused392]', 397),\n",
       "             ('[unused393]', 398),\n",
       "             ('[unused394]', 399),\n",
       "             ('[unused395]', 400),\n",
       "             ('[unused396]', 401),\n",
       "             ('[unused397]', 402),\n",
       "             ('[unused398]', 403),\n",
       "             ('[unused399]', 404),\n",
       "             ('[unused400]', 405),\n",
       "             ('[unused401]', 406),\n",
       "             ('[unused402]', 407),\n",
       "             ('[unused403]', 408),\n",
       "             ('[unused404]', 409),\n",
       "             ('[unused405]', 410),\n",
       "             ('[unused406]', 411),\n",
       "             ('[unused407]', 412),\n",
       "             ('[unused408]', 413),\n",
       "             ('[unused409]', 414),\n",
       "             ('[unused410]', 415),\n",
       "             ('[unused411]', 416),\n",
       "             ('[unused412]', 417),\n",
       "             ('[unused413]', 418),\n",
       "             ('[unused414]', 419),\n",
       "             ('[unused415]', 420),\n",
       "             ('[unused416]', 421),\n",
       "             ('[unused417]', 422),\n",
       "             ('[unused418]', 423),\n",
       "             ('[unused419]', 424),\n",
       "             ('[unused420]', 425),\n",
       "             ('[unused421]', 426),\n",
       "             ('[unused422]', 427),\n",
       "             ('[unused423]', 428),\n",
       "             ('[unused424]', 429),\n",
       "             ('[unused425]', 430),\n",
       "             ('[unused426]', 431),\n",
       "             ('[unused427]', 432),\n",
       "             ('[unused428]', 433),\n",
       "             ('[unused429]', 434),\n",
       "             ('[unused430]', 435),\n",
       "             ('[unused431]', 436),\n",
       "             ('[unused432]', 437),\n",
       "             ('[unused433]', 438),\n",
       "             ('[unused434]', 439),\n",
       "             ('[unused435]', 440),\n",
       "             ('[unused436]', 441),\n",
       "             ('[unused437]', 442),\n",
       "             ('[unused438]', 443),\n",
       "             ('[unused439]', 444),\n",
       "             ('[unused440]', 445),\n",
       "             ('[unused441]', 446),\n",
       "             ('[unused442]', 447),\n",
       "             ('[unused443]', 448),\n",
       "             ('[unused444]', 449),\n",
       "             ('[unused445]', 450),\n",
       "             ('[unused446]', 451),\n",
       "             ('[unused447]', 452),\n",
       "             ('[unused448]', 453),\n",
       "             ('[unused449]', 454),\n",
       "             ('[unused450]', 455),\n",
       "             ('[unused451]', 456),\n",
       "             ('[unused452]', 457),\n",
       "             ('[unused453]', 458),\n",
       "             ('[unused454]', 459),\n",
       "             ('[unused455]', 460),\n",
       "             ('[unused456]', 461),\n",
       "             ('[unused457]', 462),\n",
       "             ('[unused458]', 463),\n",
       "             ('[unused459]', 464),\n",
       "             ('[unused460]', 465),\n",
       "             ('[unused461]', 466),\n",
       "             ('[unused462]', 467),\n",
       "             ('[unused463]', 468),\n",
       "             ('[unused464]', 469),\n",
       "             ('[unused465]', 470),\n",
       "             ('[unused466]', 471),\n",
       "             ('[unused467]', 472),\n",
       "             ('[unused468]', 473),\n",
       "             ('[unused469]', 474),\n",
       "             ('[unused470]', 475),\n",
       "             ('[unused471]', 476),\n",
       "             ('[unused472]', 477),\n",
       "             ('[unused473]', 478),\n",
       "             ('[unused474]', 479),\n",
       "             ('[unused475]', 480),\n",
       "             ('[unused476]', 481),\n",
       "             ('[unused477]', 482),\n",
       "             ('[unused478]', 483),\n",
       "             ('[unused479]', 484),\n",
       "             ('[unused480]', 485),\n",
       "             ('[unused481]', 486),\n",
       "             ('[unused482]', 487),\n",
       "             ('[unused483]', 488),\n",
       "             ('[unused484]', 489),\n",
       "             ('[unused485]', 490),\n",
       "             ('[unused486]', 491),\n",
       "             ('[unused487]', 492),\n",
       "             ('[unused488]', 493),\n",
       "             ('[unused489]', 494),\n",
       "             ('[unused490]', 495),\n",
       "             ('[unused491]', 496),\n",
       "             ('[unused492]', 497),\n",
       "             ('[unused493]', 498),\n",
       "             ('[unused494]', 499),\n",
       "             ('[unused495]', 500),\n",
       "             ('[unused496]', 501),\n",
       "             ('[unused497]', 502),\n",
       "             ('[unused498]', 503),\n",
       "             ('[unused499]', 504),\n",
       "             ('[unused500]', 505),\n",
       "             ('[unused501]', 506),\n",
       "             ('[unused502]', 507),\n",
       "             ('[unused503]', 508),\n",
       "             ('[unused504]', 509),\n",
       "             ('[unused505]', 510),\n",
       "             ('[unused506]', 511),\n",
       "             ('[unused507]', 512),\n",
       "             ('[unused508]', 513),\n",
       "             ('[unused509]', 514),\n",
       "             ('[unused510]', 515),\n",
       "             ('[unused511]', 516),\n",
       "             ('[unused512]', 517),\n",
       "             ('[unused513]', 518),\n",
       "             ('[unused514]', 519),\n",
       "             ('[unused515]', 520),\n",
       "             ('[unused516]', 521),\n",
       "             ('[unused517]', 522),\n",
       "             ('[unused518]', 523),\n",
       "             ('[unused519]', 524),\n",
       "             ('[unused520]', 525),\n",
       "             ('[unused521]', 526),\n",
       "             ('[unused522]', 527),\n",
       "             ('[unused523]', 528),\n",
       "             ('[unused524]', 529),\n",
       "             ('[unused525]', 530),\n",
       "             ('[unused526]', 531),\n",
       "             ('[unused527]', 532),\n",
       "             ('[unused528]', 533),\n",
       "             ('[unused529]', 534),\n",
       "             ('[unused530]', 535),\n",
       "             ('[unused531]', 536),\n",
       "             ('[unused532]', 537),\n",
       "             ('[unused533]', 538),\n",
       "             ('[unused534]', 539),\n",
       "             ('[unused535]', 540),\n",
       "             ('[unused536]', 541),\n",
       "             ('[unused537]', 542),\n",
       "             ('[unused538]', 543),\n",
       "             ('[unused539]', 544),\n",
       "             ('[unused540]', 545),\n",
       "             ('[unused541]', 546),\n",
       "             ('[unused542]', 547),\n",
       "             ('[unused543]', 548),\n",
       "             ('[unused544]', 549),\n",
       "             ('[unused545]', 550),\n",
       "             ('[unused546]', 551),\n",
       "             ('[unused547]', 552),\n",
       "             ('[unused548]', 553),\n",
       "             ('[unused549]', 554),\n",
       "             ('[unused550]', 555),\n",
       "             ('[unused551]', 556),\n",
       "             ('[unused552]', 557),\n",
       "             ('[unused553]', 558),\n",
       "             ('[unused554]', 559),\n",
       "             ('[unused555]', 560),\n",
       "             ('[unused556]', 561),\n",
       "             ('[unused557]', 562),\n",
       "             ('[unused558]', 563),\n",
       "             ('[unused559]', 564),\n",
       "             ('[unused560]', 565),\n",
       "             ('[unused561]', 566),\n",
       "             ('[unused562]', 567),\n",
       "             ('[unused563]', 568),\n",
       "             ('[unused564]', 569),\n",
       "             ('[unused565]', 570),\n",
       "             ('[unused566]', 571),\n",
       "             ('[unused567]', 572),\n",
       "             ('[unused568]', 573),\n",
       "             ('[unused569]', 574),\n",
       "             ('[unused570]', 575),\n",
       "             ('[unused571]', 576),\n",
       "             ('[unused572]', 577),\n",
       "             ('[unused573]', 578),\n",
       "             ('[unused574]', 579),\n",
       "             ('[unused575]', 580),\n",
       "             ('[unused576]', 581),\n",
       "             ('[unused577]', 582),\n",
       "             ('[unused578]', 583),\n",
       "             ('[unused579]', 584),\n",
       "             ('[unused580]', 585),\n",
       "             ('[unused581]', 586),\n",
       "             ('[unused582]', 587),\n",
       "             ('[unused583]', 588),\n",
       "             ('[unused584]', 589),\n",
       "             ('[unused585]', 590),\n",
       "             ('[unused586]', 591),\n",
       "             ('[unused587]', 592),\n",
       "             ('[unused588]', 593),\n",
       "             ('[unused589]', 594),\n",
       "             ('[unused590]', 595),\n",
       "             ('[unused591]', 596),\n",
       "             ('[unused592]', 597),\n",
       "             ('[unused593]', 598),\n",
       "             ('[unused594]', 599),\n",
       "             ('[unused595]', 600),\n",
       "             ('[unused596]', 601),\n",
       "             ('[unused597]', 602),\n",
       "             ('[unused598]', 603),\n",
       "             ('[unused599]', 604),\n",
       "             ('[unused600]', 605),\n",
       "             ('[unused601]', 606),\n",
       "             ('[unused602]', 607),\n",
       "             ('[unused603]', 608),\n",
       "             ('[unused604]', 609),\n",
       "             ('[unused605]', 610),\n",
       "             ('[unused606]', 611),\n",
       "             ('[unused607]', 612),\n",
       "             ('[unused608]', 613),\n",
       "             ('[unused609]', 614),\n",
       "             ('[unused610]', 615),\n",
       "             ('[unused611]', 616),\n",
       "             ('[unused612]', 617),\n",
       "             ('[unused613]', 618),\n",
       "             ('[unused614]', 619),\n",
       "             ('[unused615]', 620),\n",
       "             ('[unused616]', 621),\n",
       "             ('[unused617]', 622),\n",
       "             ('[unused618]', 623),\n",
       "             ('[unused619]', 624),\n",
       "             ('[unused620]', 625),\n",
       "             ('[unused621]', 626),\n",
       "             ('[unused622]', 627),\n",
       "             ('[unused623]', 628),\n",
       "             ('[unused624]', 629),\n",
       "             ('[unused625]', 630),\n",
       "             ('[unused626]', 631),\n",
       "             ('[unused627]', 632),\n",
       "             ('[unused628]', 633),\n",
       "             ('[unused629]', 634),\n",
       "             ('[unused630]', 635),\n",
       "             ('[unused631]', 636),\n",
       "             ('[unused632]', 637),\n",
       "             ('[unused633]', 638),\n",
       "             ('[unused634]', 639),\n",
       "             ('[unused635]', 640),\n",
       "             ('[unused636]', 641),\n",
       "             ('[unused637]', 642),\n",
       "             ('[unused638]', 643),\n",
       "             ('[unused639]', 644),\n",
       "             ('[unused640]', 645),\n",
       "             ('[unused641]', 646),\n",
       "             ('[unused642]', 647),\n",
       "             ('[unused643]', 648),\n",
       "             ('[unused644]', 649),\n",
       "             ('[unused645]', 650),\n",
       "             ('[unused646]', 651),\n",
       "             ('[unused647]', 652),\n",
       "             ('[unused648]', 653),\n",
       "             ('[unused649]', 654),\n",
       "             ('[unused650]', 655),\n",
       "             ('[unused651]', 656),\n",
       "             ('[unused652]', 657),\n",
       "             ('[unused653]', 658),\n",
       "             ('[unused654]', 659),\n",
       "             ('[unused655]', 660),\n",
       "             ('[unused656]', 661),\n",
       "             ('[unused657]', 662),\n",
       "             ('[unused658]', 663),\n",
       "             ('[unused659]', 664),\n",
       "             ('[unused660]', 665),\n",
       "             ('[unused661]', 666),\n",
       "             ('[unused662]', 667),\n",
       "             ('[unused663]', 668),\n",
       "             ('[unused664]', 669),\n",
       "             ('[unused665]', 670),\n",
       "             ('[unused666]', 671),\n",
       "             ('[unused667]', 672),\n",
       "             ('[unused668]', 673),\n",
       "             ('[unused669]', 674),\n",
       "             ('[unused670]', 675),\n",
       "             ('[unused671]', 676),\n",
       "             ('[unused672]', 677),\n",
       "             ('[unused673]', 678),\n",
       "             ('[unused674]', 679),\n",
       "             ('[unused675]', 680),\n",
       "             ('[unused676]', 681),\n",
       "             ('[unused677]', 682),\n",
       "             ('[unused678]', 683),\n",
       "             ('[unused679]', 684),\n",
       "             ('[unused680]', 685),\n",
       "             ('[unused681]', 686),\n",
       "             ('[unused682]', 687),\n",
       "             ('[unused683]', 688),\n",
       "             ('[unused684]', 689),\n",
       "             ('[unused685]', 690),\n",
       "             ('[unused686]', 691),\n",
       "             ('[unused687]', 692),\n",
       "             ('[unused688]', 693),\n",
       "             ('[unused689]', 694),\n",
       "             ('[unused690]', 695),\n",
       "             ('[unused691]', 696),\n",
       "             ('[unused692]', 697),\n",
       "             ('[unused693]', 698),\n",
       "             ('[unused694]', 699),\n",
       "             ('[unused695]', 700),\n",
       "             ('[unused696]', 701),\n",
       "             ('[unused697]', 702),\n",
       "             ('[unused698]', 703),\n",
       "             ('[unused699]', 704),\n",
       "             ('[unused700]', 705),\n",
       "             ('[unused701]', 706),\n",
       "             ('[unused702]', 707),\n",
       "             ('[unused703]', 708),\n",
       "             ('[unused704]', 709),\n",
       "             ('[unused705]', 710),\n",
       "             ('[unused706]', 711),\n",
       "             ('[unused707]', 712),\n",
       "             ('[unused708]', 713),\n",
       "             ('[unused709]', 714),\n",
       "             ('[unused710]', 715),\n",
       "             ('[unused711]', 716),\n",
       "             ('[unused712]', 717),\n",
       "             ('[unused713]', 718),\n",
       "             ('[unused714]', 719),\n",
       "             ('[unused715]', 720),\n",
       "             ('[unused716]', 721),\n",
       "             ('[unused717]', 722),\n",
       "             ('[unused718]', 723),\n",
       "             ('[unused719]', 724),\n",
       "             ('[unused720]', 725),\n",
       "             ('[unused721]', 726),\n",
       "             ('[unused722]', 727),\n",
       "             ('[unused723]', 728),\n",
       "             ('[unused724]', 729),\n",
       "             ('[unused725]', 730),\n",
       "             ('[unused726]', 731),\n",
       "             ('[unused727]', 732),\n",
       "             ('[unused728]', 733),\n",
       "             ('[unused729]', 734),\n",
       "             ('[unused730]', 735),\n",
       "             ('[unused731]', 736),\n",
       "             ('[unused732]', 737),\n",
       "             ('[unused733]', 738),\n",
       "             ('[unused734]', 739),\n",
       "             ('[unused735]', 740),\n",
       "             ('[unused736]', 741),\n",
       "             ('[unused737]', 742),\n",
       "             ('[unused738]', 743),\n",
       "             ('[unused739]', 744),\n",
       "             ('[unused740]', 745),\n",
       "             ('[unused741]', 746),\n",
       "             ('[unused742]', 747),\n",
       "             ('[unused743]', 748),\n",
       "             ('[unused744]', 749),\n",
       "             ('[unused745]', 750),\n",
       "             ('[unused746]', 751),\n",
       "             ('[unused747]', 752),\n",
       "             ('[unused748]', 753),\n",
       "             ('[unused749]', 754),\n",
       "             ('[unused750]', 755),\n",
       "             ('[unused751]', 756),\n",
       "             ('[unused752]', 757),\n",
       "             ('[unused753]', 758),\n",
       "             ('[unused754]', 759),\n",
       "             ('[unused755]', 760),\n",
       "             ('[unused756]', 761),\n",
       "             ('[unused757]', 762),\n",
       "             ('[unused758]', 763),\n",
       "             ('[unused759]', 764),\n",
       "             ('[unused760]', 765),\n",
       "             ('[unused761]', 766),\n",
       "             ('[unused762]', 767),\n",
       "             ('[unused763]', 768),\n",
       "             ('[unused764]', 769),\n",
       "             ('[unused765]', 770),\n",
       "             ('[unused766]', 771),\n",
       "             ('[unused767]', 772),\n",
       "             ('[unused768]', 773),\n",
       "             ('[unused769]', 774),\n",
       "             ('[unused770]', 775),\n",
       "             ('[unused771]', 776),\n",
       "             ('[unused772]', 777),\n",
       "             ('[unused773]', 778),\n",
       "             ('[unused774]', 779),\n",
       "             ('[unused775]', 780),\n",
       "             ('[unused776]', 781),\n",
       "             ('[unused777]', 782),\n",
       "             ('[unused778]', 783),\n",
       "             ('[unused779]', 784),\n",
       "             ('[unused780]', 785),\n",
       "             ('[unused781]', 786),\n",
       "             ('[unused782]', 787),\n",
       "             ('[unused783]', 788),\n",
       "             ('[unused784]', 789),\n",
       "             ('[unused785]', 790),\n",
       "             ('[unused786]', 791),\n",
       "             ('[unused787]', 792),\n",
       "             ('[unused788]', 793),\n",
       "             ('[unused789]', 794),\n",
       "             ('[unused790]', 795),\n",
       "             ('[unused791]', 796),\n",
       "             ('[unused792]', 797),\n",
       "             ('[unused793]', 798),\n",
       "             ('[unused794]', 799),\n",
       "             ('[unused795]', 800),\n",
       "             ('[unused796]', 801),\n",
       "             ('[unused797]', 802),\n",
       "             ('[unused798]', 803),\n",
       "             ('[unused799]', 804),\n",
       "             ('[unused800]', 805),\n",
       "             ('[unused801]', 806),\n",
       "             ('[unused802]', 807),\n",
       "             ('[unused803]', 808),\n",
       "             ('[unused804]', 809),\n",
       "             ('[unused805]', 810),\n",
       "             ('[unused806]', 811),\n",
       "             ('[unused807]', 812),\n",
       "             ('[unused808]', 813),\n",
       "             ('[unused809]', 814),\n",
       "             ('[unused810]', 815),\n",
       "             ('[unused811]', 816),\n",
       "             ('[unused812]', 817),\n",
       "             ('[unused813]', 818),\n",
       "             ('[unused814]', 819),\n",
       "             ('[unused815]', 820),\n",
       "             ('[unused816]', 821),\n",
       "             ('[unused817]', 822),\n",
       "             ('[unused818]', 823),\n",
       "             ('[unused819]', 824),\n",
       "             ('[unused820]', 825),\n",
       "             ('[unused821]', 826),\n",
       "             ('[unused822]', 827),\n",
       "             ('[unused823]', 828),\n",
       "             ('[unused824]', 829),\n",
       "             ('[unused825]', 830),\n",
       "             ('[unused826]', 831),\n",
       "             ('[unused827]', 832),\n",
       "             ('[unused828]', 833),\n",
       "             ('[unused829]', 834),\n",
       "             ('[unused830]', 835),\n",
       "             ('[unused831]', 836),\n",
       "             ('[unused832]', 837),\n",
       "             ('[unused833]', 838),\n",
       "             ('[unused834]', 839),\n",
       "             ('[unused835]', 840),\n",
       "             ('[unused836]', 841),\n",
       "             ('[unused837]', 842),\n",
       "             ('[unused838]', 843),\n",
       "             ('[unused839]', 844),\n",
       "             ('[unused840]', 845),\n",
       "             ('[unused841]', 846),\n",
       "             ('[unused842]', 847),\n",
       "             ('[unused843]', 848),\n",
       "             ('[unused844]', 849),\n",
       "             ('[unused845]', 850),\n",
       "             ('[unused846]', 851),\n",
       "             ('[unused847]', 852),\n",
       "             ('[unused848]', 853),\n",
       "             ('[unused849]', 854),\n",
       "             ('[unused850]', 855),\n",
       "             ('[unused851]', 856),\n",
       "             ('[unused852]', 857),\n",
       "             ('[unused853]', 858),\n",
       "             ('[unused854]', 859),\n",
       "             ('[unused855]', 860),\n",
       "             ('[unused856]', 861),\n",
       "             ('[unused857]', 862),\n",
       "             ('[unused858]', 863),\n",
       "             ('[unused859]', 864),\n",
       "             ('[unused860]', 865),\n",
       "             ('[unused861]', 866),\n",
       "             ('[unused862]', 867),\n",
       "             ('[unused863]', 868),\n",
       "             ('[unused864]', 869),\n",
       "             ('[unused865]', 870),\n",
       "             ('[unused866]', 871),\n",
       "             ('[unused867]', 872),\n",
       "             ('[unused868]', 873),\n",
       "             ('[unused869]', 874),\n",
       "             ('[unused870]', 875),\n",
       "             ('[unused871]', 876),\n",
       "             ('[unused872]', 877),\n",
       "             ('[unused873]', 878),\n",
       "             ('[unused874]', 879),\n",
       "             ('[unused875]', 880),\n",
       "             ('[unused876]', 881),\n",
       "             ('[unused877]', 882),\n",
       "             ('[unused878]', 883),\n",
       "             ('[unused879]', 884),\n",
       "             ('[unused880]', 885),\n",
       "             ('[unused881]', 886),\n",
       "             ('[unused882]', 887),\n",
       "             ('[unused883]', 888),\n",
       "             ('[unused884]', 889),\n",
       "             ('[unused885]', 890),\n",
       "             ('[unused886]', 891),\n",
       "             ('[unused887]', 892),\n",
       "             ('[unused888]', 893),\n",
       "             ('[unused889]', 894),\n",
       "             ('[unused890]', 895),\n",
       "             ('[unused891]', 896),\n",
       "             ('[unused892]', 897),\n",
       "             ('[unused893]', 898),\n",
       "             ('[unused894]', 899),\n",
       "             ('[unused895]', 900),\n",
       "             ('[unused896]', 901),\n",
       "             ('[unused897]', 902),\n",
       "             ('[unused898]', 903),\n",
       "             ('[unused899]', 904),\n",
       "             ('[unused900]', 905),\n",
       "             ('[unused901]', 906),\n",
       "             ('[unused902]', 907),\n",
       "             ('[unused903]', 908),\n",
       "             ('[unused904]', 909),\n",
       "             ('[unused905]', 910),\n",
       "             ('[unused906]', 911),\n",
       "             ('[unused907]', 912),\n",
       "             ('[unused908]', 913),\n",
       "             ('[unused909]', 914),\n",
       "             ('[unused910]', 915),\n",
       "             ('[unused911]', 916),\n",
       "             ('[unused912]', 917),\n",
       "             ('[unused913]', 918),\n",
       "             ('[unused914]', 919),\n",
       "             ('[unused915]', 920),\n",
       "             ('[unused916]', 921),\n",
       "             ('[unused917]', 922),\n",
       "             ('[unused918]', 923),\n",
       "             ('[unused919]', 924),\n",
       "             ('[unused920]', 925),\n",
       "             ('[unused921]', 926),\n",
       "             ('[unused922]', 927),\n",
       "             ('[unused923]', 928),\n",
       "             ('[unused924]', 929),\n",
       "             ('[unused925]', 930),\n",
       "             ('[unused926]', 931),\n",
       "             ('[unused927]', 932),\n",
       "             ('[unused928]', 933),\n",
       "             ('[unused929]', 934),\n",
       "             ('[unused930]', 935),\n",
       "             ('[unused931]', 936),\n",
       "             ('[unused932]', 937),\n",
       "             ('[unused933]', 938),\n",
       "             ('[unused934]', 939),\n",
       "             ('[unused935]', 940),\n",
       "             ('[unused936]', 941),\n",
       "             ('[unused937]', 942),\n",
       "             ('[unused938]', 943),\n",
       "             ('[unused939]', 944),\n",
       "             ('[unused940]', 945),\n",
       "             ('[unused941]', 946),\n",
       "             ('[unused942]', 947),\n",
       "             ('[unused943]', 948),\n",
       "             ('[unused944]', 949),\n",
       "             ('[unused945]', 950),\n",
       "             ('[unused946]', 951),\n",
       "             ('[unused947]', 952),\n",
       "             ('[unused948]', 953),\n",
       "             ('[unused949]', 954),\n",
       "             ('[unused950]', 955),\n",
       "             ('[unused951]', 956),\n",
       "             ('[unused952]', 957),\n",
       "             ('[unused953]', 958),\n",
       "             ('[unused954]', 959),\n",
       "             ('[unused955]', 960),\n",
       "             ('[unused956]', 961),\n",
       "             ('[unused957]', 962),\n",
       "             ('[unused958]', 963),\n",
       "             ('[unused959]', 964),\n",
       "             ('[unused960]', 965),\n",
       "             ('[unused961]', 966),\n",
       "             ('[unused962]', 967),\n",
       "             ('[unused963]', 968),\n",
       "             ('[unused964]', 969),\n",
       "             ('[unused965]', 970),\n",
       "             ('[unused966]', 971),\n",
       "             ('[unused967]', 972),\n",
       "             ('[unused968]', 973),\n",
       "             ('[unused969]', 974),\n",
       "             ('[unused970]', 975),\n",
       "             ('[unused971]', 976),\n",
       "             ('[unused972]', 977),\n",
       "             ('[unused973]', 978),\n",
       "             ('[unused974]', 979),\n",
       "             ('[unused975]', 980),\n",
       "             ('[unused976]', 981),\n",
       "             ('[unused977]', 982),\n",
       "             ('[unused978]', 983),\n",
       "             ('[unused979]', 984),\n",
       "             ('[unused980]', 985),\n",
       "             ('[unused981]', 986),\n",
       "             ('[unused982]', 987),\n",
       "             ('[unused983]', 988),\n",
       "             ('[unused984]', 989),\n",
       "             ('[unused985]', 990),\n",
       "             ('[unused986]', 991),\n",
       "             ('[unused987]', 992),\n",
       "             ('[unused988]', 993),\n",
       "             ('[unused989]', 994),\n",
       "             ('[unused990]', 995),\n",
       "             ('[unused991]', 996),\n",
       "             ('[unused992]', 997),\n",
       "             ('[unused993]', 998),\n",
       "             ('!', 999),\n",
       "             ...])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab # 단어들이 보인다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6baf79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2079"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['do'] # 탭을 하면 많은 기능이 보일 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a1d7ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2293"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ec1a57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embeddings'"
     ]
    }
   ],
   "source": [
    "tokenizer.vocab['embeddings'] # OOV이다. 기존에는 존재하지 않는 단어인 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb5dfd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7861\n",
      "8270\n",
      "4667\n",
      "2015\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab['em'])\n",
    "print(tokenizer.vocab['##bed'])\n",
    "print(tokenizer.vocab['##ding'])\n",
    "print(tokenizer.vocab['##s'])\n",
    "# 모든 재료가 존재한다. 서브워드가 존재한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9437be33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'is',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'i',\n",
       " 'want',\n",
       " 'em',\n",
       " '##bed',\n",
       " '##ding',\n",
       " '##s',\n",
       " 'for',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Here is the sentence I want embeddings for.')\n",
    "# 토크나이저는 토큰단위로 나눠준다. 기본적으로는 단어단위로 나눈다. \n",
    "# 임베딩이라는 단어는 OOV이다. 토큰화는 되었다. 온전하지는 않지만 토큰화가 이루어졌다.\n",
    "# 버트의 단어전체집합을 저장해서 확인해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "229d6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'w',encoding='utf8') as f:\n",
    "    for token in tokenizer.vocab.keys():\n",
    "        f.write(token + '\\n')                            # 단어장이 만들어졌다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa884cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>##．</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>##／</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>##：</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30520</th>\n",
       "      <td>##？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>##～</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30522 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0          [PAD]\n",
       "1      [unused0]\n",
       "2      [unused1]\n",
       "3      [unused2]\n",
       "4      [unused3]\n",
       "...          ...\n",
       "30517        ##．\n",
       "30518        ##／\n",
       "30519        ##：\n",
       "30520        ##？\n",
       "30521        ##～\n",
       "\n",
       "[30522 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한자는 대체 왜 있는걸까?\n",
    "df = pd.read_fwf('vocabulary.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac915113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d457fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩이라는 방법이 있다. 사인, 코사인을 이용한다. \n",
    "# 마스크드 언어모델, 다음문장 예측모델등이 있다.\n",
    "# 버트, gpt, 엘모는 다른 모양을 가진다.\n",
    "# 마스크드 언어모델을 통해 양방향성을 얻었기 때문이다. \n",
    "# 문장만 있으면 되는 것이다. \n",
    "# 110개정도 되는 국가언어에 대해 학습되어져 있다. \n",
    "# 한국어 버트도 존재한다. 성능은 기대를 크게 하지는 말자. 아마 한국어 위키피디어를 참고한 듯 하다. \n",
    "# 일반적으로 버트를 파인튜닝해서 사용한다. 대기업들에서 한국어화를 하였다. \n",
    "# 한국 언어특성에 맞게 파인튜닝 하는 것이다.\n",
    "# 허깅페이스에서 다운받아서 사용 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e846e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마스크드 언어 모델 \n",
    "# 사전훈련을 위해서 인공신경망의 입력으로 들어가는 입력테스트의 15%의 단어를 랜덤으로 마스킹한다. \n",
    "# 인공신경망에게 가려진 단어를 예측하도록 훈련한다.\n",
    "# 위키피디어의 레이블은 존재하지 않는다.그 와중에 가려진 단어를 추측해서 맞춰야 한다. 단어의 위치도 고려되어야 한다.\n",
    "# 여기서 3규칙을 적용한다.\n",
    "# 80%의 단어를 마스크로 변경 12%가량이다. \n",
    "# 10%의 단어들은 단어가 바뀐다. 1.5%가량이다.  \n",
    "# 10%의 단어들은 동일하게 둔다. 1.5%가량이다. \n",
    "# 이렇게 학습을 하여 더 좋은 결과를 가져왔다는 것이다. \n",
    "# 셀프 어텐션을 통해서 스스로 학습해나간다. \n",
    "# 매번 어텐션을 할 때 마다 그 위치에 해당하는 단어가 있기 위해서 주변의 단어가 무엇인지를 살펴보는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33702184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP(다음 문장 예측)\n",
    "# 챗봇을 훈련시킬때 많이 사용된다. \n",
    "# 두 개의 문장을 준 다음 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련한다. \n",
    "# 대체로 훈련을 하는 당시에는 50대 50으로 이어지는 문장과 이어지지 않는 문장을 구성해서 레이블을 준다. \n",
    "# 이어지는 문장인지 아닌지 레이블을 줘서 학습을 하는 것이다. \n",
    "# NSP는 문맥을 파악할 때 유용하게 쓰인다. \n",
    "# BERT로 입력을 넣을 때 [SEP]이라는 특별 토큰으로 문장을 구분한다. \n",
    "# 문장이 끝난 다음에 sep토큰을 부여함으로서 분류문제를 낼 때 사용한다. \n",
    "# 다음문장에 대해서는 따로 학습을 하는 것이 아닌 loss를 합하여 학습이 동시에 이루어진다. \n",
    "# QA(문답형식)나 NLI(문장 유사도)와 같은 두 문장간의 관계를 이해하는 것이 중요한 태스크도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3e96c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세그먼트 임베딩\n",
    "# 버트에서 추가적으로 이루어진다. QA와 같이 두 개의 문장 입력이 필요한 태스크를 풀기도 한다. \n",
    "# 이때 문장구분을 위해 새그먼트 임베딩이라는 또 다른 임베딩층을 사용한다. \n",
    "# NLI방식으로 버트모델을 만들 경우 기본적으로 문장을 2개를 투입한다. 앞의 문장을 전부 0으로 주고 그 다음 문장을 전부 1로 준다.\n",
    "# 결론적으로 총 3개의 계층이 사용된다.\n",
    "# 1. 워드피스 임베딩 : 실질적인 입력이 되는 워드 임베딩, 단어집합의 크기로 30522개의 크기를 가진다. 그래도 OOV문제는 있긴 하다.\n",
    "# 상당수 해결하긴 했다. \n",
    "# 2. 포지셔널 임베딩 : 단어의 위치정보를 학습하기 위한 임베딩으로 임베딩 벡터의 종류는 문장의 최대 길이인 512개이다.\n",
    "# 3. 세그먼트 임베딩 : 두 개의 문장을 구분하기 위해 사용하는 임베딩으로 임베딩 벡터의 종류는 2개이다. 0과 1만나온다.\n",
    "# 이 3개를 종합하여 버트로 투입된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a2f2074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 파인튜닝\n",
    "# pip install transformers 먼저 이것을 다운로드 하자. \n",
    "# https://huggingface.co/ 이 사이트는 모델을 다운받는 사이트이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cc2702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer                          # 버트 토크나이저이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c94a968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec6f0b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x1f96bcaa050>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer  # 토크나이저가 받아졌다. 단어단위로 쪼개는 용도로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d301b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.tokenize('Here is the sentence I want embeddings for.')\n",
    "print(result)                            # 임베딩스가 ##으로 연결된 것을 볼 수 있다. OOV처리는 웬만큼 된 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "091df769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2182\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab['here'])            # 단어의 위치를 파악한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "562ed998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 모델로 분류기를 만들려고 한다. 또는 특정 위치에 들어가는 언어가 무엇인지 파악하고 싶다. \n",
    "# 토크나이저는 인코딩할 때 사용하는 것이고 모델은 분류작업, 다양한 형태로 가중치 정보를 집어넣는 모델이다.\n",
    "# 이번에는 테스트를 진행해보자. \n",
    "from transformers import TFBertForMaskedLM # 마스크드 언어모델로 만들어진 버트 모델이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased') # 마스크드 언어모델을 위한 구조로 버트가 읽어졌다. \n",
    "# 두 가지 모드로 읽을 수 있다. 하나는 masked고 다른 것은 NLI 모델이다. \n",
    "# 버트를 마스크드 언어모델 구조로 읽어들인다. \n",
    "# 모델명을 입력하게 되면 모델이 학습될 당시에 사용된 토크나이저가 읽어진다.  \n",
    "# 코랩이 압도적으로 빠르다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29469be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffe08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# 설명서가 유료라서 좀 아쉽긴 하다. \n",
    "# 사전학습된 버트 모델을 이용해서. 버트를 다운로드 받는다. \n",
    "# 트랜스포머스라고 하는 패키지가 설치되어있어야 한다. \n",
    "# 모델과 토크나이저는 한 쌍이다. 언어모델과 토크나이저가 한 세트다. \n",
    "# 입력한 단어에 대해 인코딩을 해야하기 때문이다. 숫자로 변환한 다음에는 분류작업을 해야한다. \n",
    "# 토크나이저도 모델마다 다르다. 즉 단어사전의 인덱스가 다르다. 그래야 제대로 해석된 결과가 나온다. \n",
    "# a라고 하는 토크나이저가 있다고 해보자. 사과라는 단어가 40번으로 인코딩 되어있다. \n",
    "# b라고 하는 토크나이저는 사과가 50번으로 인코딩되어있다. 토크나이저마다 다르다. \n",
    "# 토크나이저는 엄격하게 구분해야 한다. 모델과 토크나이저는 한 세트다. \n",
    "# 문장을 하나 생성해서 BERT에 입력을 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c668239",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf') # 마스크 자리에 과연 어떤 단어가 들어가야 할까?\n",
    "# 주피터 노트북에서는 한계가 있는 듯 하다. 코랩에서는 작동하는 코드이다. \n",
    "# tf는 텐서형태로 리턴하라는 것이다. \n",
    "# 애러가 뜬다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "98d16b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus('Soccer is a really fun [MASK].', add_special_tokens=True, return_tensors='tf')\n",
    "# 이렇게 돌리니까 사용이 가능해진다. \n",
    "# input_ids는 일종의 키이다. 인풋 IDS에 대한 값인데 1,9로 모양이 나온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8134fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f65416",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['token_type_ids']  # 문장을 구분하는 용도이다. 다만 지금은 문장이 1개라서 000000000으로 표시될 것이다. \n",
    "# 문장이 2개일 경우에는 경우에는 000000111111로 추가될 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask']\n",
    "# 주피터 노트북 상에서는 inputs['special_tokens_mask']로 입력이 될 것이다. 버젼 문제로 인해서 쉽지 않은 듯 하다. \n",
    "# 스페셜 토큰이라는 개념이 존재한다. \n",
    "# [PAD] - 0         # 패드에 해당한다. \n",
    "# [UNK] - 100       #  \n",
    "# [CLS] - 101       # 시작 표시이다. \n",
    "# [SEP] - 102       # 마침표이다. \n",
    "# [MASK] - 103      # 가려진 글자이다. \n",
    "# 어텐션 마스크는 실제 단어와 패딩토큰을 구분할 수 있다. 만약 패딩이 있다면 쭉 존재할 것이다. 다만 여기는 패딩을 할 필요가 없다.\n",
    "# 모델은 이미 학습되어 있기 때문에 물어보면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline  # 파이프라인을 불러왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9de6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)   # 마스크를 채우는 모델을 제작했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5542042",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip('Soccer is a really fun [MASK].')  # 이런 식으로 물어볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 버트 모델도 사용해보자. \n",
    "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
    "# 한국어 버트 모델이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# 한국어 버트 모델의 토크나이즈이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69629d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')\n",
    "#같은 절차로 실행해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer) # 토크나이저를 교체해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip('축구는 정말 재미있는 [MASK]다.')\n",
    "# 잘 나오긴 하지만 교육이 이상하게 되었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForNextSentencePrediction\n",
    "# 다음문장 예측을 기반으로 사전학습된 모델이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa913593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef53d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33479969",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\"\n",
    "# 문맥상으로 이어지는 문장이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178dadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(prompt, next_sentence, return_tensors='tf') # 정수를 텐서로 리턴하라는 것이다. 만약 pt로 쓰면 파이토치로 리턴한다.\n",
    "# 파이토치같은 경우는 추후 해봐야한다. \n",
    "# 숫자로 채워졌다. 다만 문자가 끝나고 102가 나오면 다음문장이 이어지는 형태이다. \n",
    "# 문장간의 구분도 가능하다. \n",
    "# 인코딩 변수중에 담겨있는 내용중에 별도로 실행한게 있다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding['input_ids'][0]) # 디코딩하는것도 가능하다. \n",
    "# 이제 인풋의 시간이다. encoding['input_ids']과 encoding['token_type_ids']를 연결하면 연결된 문장인지 확률로 표시해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3680ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(encoding['input_ids'],token_type_ids = encoding['token_type_ids'])[0] # 이 결과를 logits라고 부른다. 소프트맥스에 넣는 용도이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft=tf.keras.layers.Softmax()\n",
    "res = soft(logits)\n",
    "res\n",
    "# 확률로 출력될 것이다. 첫번째칸이 크면 이어지는 문장이라는 뜻이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.argmax(res,-1).numpy()[0] # 출력결과는 0이다. 1을 나오게 의도하면 1이 나올 것이다.\n",
    "# 코랩에서 TPU를 하는 법을 내일 배울 것이다. \n",
    "# 파인튜닝을 하는 법을 배울 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
