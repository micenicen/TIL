{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6c776699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf                              # 텐서플로 호출\n",
    "from tensorflow.keras.preprocessing import image     # 케라스 이미지 호출\n",
    "import numpy as np                                   # 넘파이 호출                               \n",
    "import matplotlib.pyplot as plt                      # 파이플랏\n",
    "from tensorflow.image import pad_to_bounding_box     # 이미지 묶기\n",
    "from tensorflow.image import central_crop            # 가운데 추출\n",
    "from tensorflow.image import resize                  # 이미지 크기조절\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6c639357",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays = []  # 이미지 배열을 저장할 리스트      \n",
    "for i in range(200):\n",
    "    # i를 3자리 숫자로 포맷팅하여 파일 이름을 생성\n",
    "    file_name = f'tf_image/train_{i:03d}.jpg'\n",
    "    img = image.load_img(file_name)\n",
    "    image_arrays.append(image.img_to_array(img).astype('float32') / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "785c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = []  # 이미지 배열을 저장할 리스트      \n",
    "for i in range(60):\n",
    "    # i를 3자리 숫자로 포맷팅하여 파일 이름을 생성\n",
    "    file_name = f'tf_image/test_{i:03d}.jpg'\n",
    "    img = image.load_img(file_name)\n",
    "    test_arrays.append(image.img_to_array(img).astype('float32') / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1fae533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_zunsik = []\n",
    "for i in range(len(image_arrays)):\n",
    "    flip_lr_tensor = tf.image.flip_left_right(image_arrays[i])\n",
    "    flip_ud_tensor = tf.image.flip_up_down(image_arrays[i]) \n",
    "    flip_lr_ud_tensor = tf.image.flip_up_down(flip_lr_tensor) \n",
    "    flip_or_image = tf.keras.preprocessing.image.array_to_img(image_arrays[i])\n",
    "    flip_lr_image = tf.keras.preprocessing.image.array_to_img(flip_lr_tensor)\n",
    "    flip_ud_image = tf.keras.preprocessing.image.array_to_img(flip_ud_tensor)\n",
    "    flip_lr_ud_image = tf.keras.preprocessing.image.array_to_img(flip_lr_ud_tensor)\n",
    "    listy1 = [flip_or_image,flip_lr_image,flip_ud_image,flip_lr_ud_image]\n",
    "    listy1 = [flip_or_image,flip_lr_image,flip_ud_image,flip_lr_ud_image]\n",
    "    for j in listy1:\n",
    "        for k in range(2):\n",
    "            random_bright_tensor = tf.image.random_brightness(j, max_delta=0.9) \n",
    "            # random_bright_tensor = tf.clip_by_value(random_bright_tensor, 0, 1)\n",
    "            # random_bright_image = tf.keras.preprocessing.image.array_to_img(random_bright_tensor)\n",
    "            image_zunsik.append(random_bright_tensor)\n",
    "len(image_zunsik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cc20bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(image_zunsik).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "82eae84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans_list = []\n",
    "for m in range(20):\n",
    "    for n in range(80):\n",
    "        train_ans_list.append(m)\n",
    "ans_set = np.array(train_ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bb700a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_set, ans_set, test_size=0.2)\n",
    "y_train = to_categorical(y_train, 20)\n",
    "y_test = to_categorical(y_test, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "828cabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ans = []\n",
    "for m in range(20):\n",
    "    for n in range(3):\n",
    "        real_ans.append(m)\n",
    "real_ans = to_categorical(real_ans, 20)\n",
    "real_ans = np.array(real_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bad11a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = np.array(test_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b64211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2725ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f06f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "679293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D # 랜덤 히든계층, 플래튼, 콘브, 맥스풀 이렇게 된다. \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping \n",
    "from tensorflow.keras.datasets import mnist        \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "225204dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(5,5), input_shape=(150, 500, 3), activation='relu')) # 필터갯수, 필터크기, 투입사진, 작동방식, 스트라이드 설정\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))      # 필터갯수, 필터크기(생략되었다), 작동방식\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))              # 풀링하기.2,2사이즈로 만들기                        # 25%를 드롭아웃한 상태에서 진행\n",
    "model.add(Flatten())                                  # 플래튼이 일어난 다음에 히든계층으로 간다. \n",
    "model.add(Dense(32,  activation='relu'))             # 128차원짜리 층으로 나눔\n",
    "model.add(Dropout(0.3))                               # 50% 드롭\n",
    "model.add(Dense(20, activation='softmax'))            # 소프트맥스로 결과값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5445815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "84724bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './data/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a7a11d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath=\"./data/model/MNIST_MLP.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2ea11820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x0000014D26D68540> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "48/48 [==============================] - 149s 3s/step - loss: 3.3909 - accuracy: 0.0760 - val_loss: 2.9156 - val_accuracy: 0.1063\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 148s 3s/step - loss: 2.5561 - accuracy: 0.2125 - val_loss: 2.2657 - val_accuracy: 0.4219\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 1.9186 - accuracy: 0.4635 - val_loss: 2.2324 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 1.7718 - accuracy: 0.4885 - val_loss: 2.0871 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 1.3975 - accuracy: 0.6177 - val_loss: 1.7137 - val_accuracy: 0.6250\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 1.2226 - accuracy: 0.6677 - val_loss: 1.7195 - val_accuracy: 0.6187\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 1.0753 - accuracy: 0.7104 - val_loss: 1.4548 - val_accuracy: 0.7437\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 149s 3s/step - loss: 1.0015 - accuracy: 0.7542 - val_loss: 1.7199 - val_accuracy: 0.6000\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 147s 3s/step - loss: 0.9076 - accuracy: 0.7781 - val_loss: 1.7068 - val_accuracy: 0.6187\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 147s 3s/step - loss: 0.6424 - accuracy: 0.8344 - val_loss: 1.7131 - val_accuracy: 0.5969\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 147s 3s/step - loss: 1.0340 - accuracy: 0.7094 - val_loss: 1.7111 - val_accuracy: 0.6719\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 147s 3s/step - loss: 0.5759 - accuracy: 0.8531 - val_loss: 1.5766 - val_accuracy: 0.7156\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 172s 4s/step - loss: 0.5041 - accuracy: 0.8750 - val_loss: 1.7816 - val_accuracy: 0.7281\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.5283 - accuracy: 0.8635 - val_loss: 1.3672 - val_accuracy: 0.7656\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.4527 - accuracy: 0.8875 - val_loss: 1.5374 - val_accuracy: 0.7031\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 152s 3s/step - loss: 0.6823 - accuracy: 0.8271 - val_loss: 1.7464 - val_accuracy: 0.6062\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 150s 3s/step - loss: 0.5274 - accuracy: 0.8583 - val_loss: 1.9240 - val_accuracy: 0.7625\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 151s 3s/step - loss: 0.5617 - accuracy: 0.8635 - val_loss: 1.8042 - val_accuracy: 0.5750\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 149s 3s/step - loss: 0.7437 - accuracy: 0.8021 - val_loss: 1.7314 - val_accuracy: 0.7688\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.3542 - accuracy: 0.9198 - val_loss: 1.6890 - val_accuracy: 0.7594\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.4036 - accuracy: 0.8979 - val_loss: 1.7857 - val_accuracy: 0.6406\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.6275 - accuracy: 0.8406 - val_loss: 1.6611 - val_accuracy: 0.7469\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.4427 - accuracy: 0.8760 - val_loss: 1.6989 - val_accuracy: 0.7656\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 148s 3s/step - loss: 0.4289 - accuracy: 0.9167 - val_loss: 1.6907 - val_accuracy: 0.7844\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train , validation_split=0.25, epochs=30, batch_size=20, verbose=1, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "38a82e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 852ms/step - loss: 9.6345 - accuracy: 0.3833\n",
      "\n",
      " Test Accuracy: 0.3833\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(real_test, real_ans)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af43c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
