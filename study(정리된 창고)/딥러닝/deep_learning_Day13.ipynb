{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 팀이 짜여졌다. 프로젝트 관련하여 확인해보자\n",
    "# 파이널 프로젝트에는 주도적으로 조를 편성할 수 있도록 할 것이다.\n",
    "# 주제는 다양한 분야이다. 어떤 식으로 보여줄 것인가는 다르다. HTML, 자바스크립트, 플라스크로 보여주는 기반을 만들 것이다.\n",
    "# 프로젝트는 이번달 말에 진행될 예정이다.주제를 미리 생각해 둘 필요는 있다.\n",
    "# 태블로, 시각화를 하루 반 정도 할 예정이다. 취업때문에 태블로를 편성했다. 딥러닝 자연어처리, 이미지 파인튜닝도 해야한다.\n",
    "# 그냥 바쁘다. 웹 프로그래밍도 공부해야한다. 코드를 아예 받고 시작할 것이다. \n",
    "# 자바스크립트는 어려울 수 있지만 HTML은 그나마 쉬울 것이다. \n",
    "# 대시보드\n",
    "# 대시보드는 계기판을 말한다. 여러 정보들을 한꺼번에 볼 수 있는 그거 맞다.\n",
    "# IT 쪽에서는 웹 페이지 안에 전체적인 다양한 정보들이 축약되어 모여있는 페이지를 말한다. \n",
    "# 시각적으로 분석결과를 보여주거나 앞으로 어떻게 예측되어지는지 보여주거나 실시간 정보를 보여주는 것 등 서비스를 할 수 있는\n",
    "# 페이지로 이해하면 될 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731e0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태깅. 객체명 인식을 해보자.\n",
    "# 토픽 모델링이라고 이야기하는데 주어진 문서에서 중요한 의미를 추출하는 것을 말한다.\n",
    "# 텍스트를 요약하는 방법들이 있다. \n",
    "# 케라스를 이용한 태깅작업은 전처리 과정과 연관이 깊다. 모델을 만드는 것 이전에 단어에 대한 인식을 시켜주는 것을 말한다.\n",
    "# 언어중 이름인지 장소인지 집단인지 관계추출을 위한 자연어처리 영역에서도 중요하게 다룬다. \n",
    "# 사람이나 장소, 기관, 날짜 등 이 있을텐데 15일이라는 숫자가 있으면 컴퓨터는 날짜인지 알지 못한다. \n",
    "# 만약 유정이는 2018년 골드만삭스에 입사했다. 라고 말하면 유정(ps)이는 2018년(dt)에 골드만삭스(org)에 입사했다. 라고 표시하는 것을 원한다.\n",
    "# 이 분야는 지도학습이 필요하다. \n",
    "# 과거에 발표된 약물의 종류에 대해 식별할 수 있도록 자질정보를 나타낸 것이 있다. \n",
    "# 먼저 글자로 된 이름들을 AI가 인식을 할 수 있도록 만들어야 한다. \n",
    "# 바이오태깅 기법이라고도 한다. 개체명 인식은 챗봇 등에서 필요한 주요 전처리 작업이면서 까다로운 작업이다. \n",
    "# 도메인 목적에 특화되도록 개체명 인식을 하기 위해 기존에 공개된 것을 사용하는게 아니라 우리가 제작할 수 있어야 한다. \n",
    "# 이미 공개된 개채명 인식기를 넘어 새로운 개채명 인식기를 만들 수 있어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32505fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO표현(begin inside outside) \n",
    "# 시작, 내부, 외부로 나타낸다.\n",
    "# 해리포터 보러가자 라고 하면 BIIIOOOO로 표현된다. 개체명은 B와 I로 표현된다. 개채명이 아닌 경우 O로 표기된다. \n",
    "# 해리포터(movie)보러 메가박스(theater)가자 라는 식으로 나눠진다. \n",
    "# New York는 위치를 나타낼 때 new(B-loc) york(iloc)로 표현한다. \n",
    "# 객체가 나타나면 그 객체에 대해 함께 태깅해야 한다. \n",
    "# https://github.com/machinereading/KoreanNERCorpus\n",
    "# 한국어 객체명 인식기를 만들어내는 목적으로 위의 사이트를 이용 가능하다. \n",
    "# https://huggingface.co/datasets/bigbio/chemdner\n",
    "# 의약품 관련 객체명을 인식하는 말뭉치(코퍼스)데이터가 정렬된 데이터셋이다. \n",
    "# 개체명 인식 데이터에서 COMLL2003은 개체명을 인식하기 위한 전통적인 영어 데이터셋이다.\n",
    "# 문장과 문장사이의 구분은 공백으로 하도록 구성되어져있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10730832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토픽 모델링\n",
    "# 토픽모델링은 중요한 단어를 추출하거나 중요한 문장을 추출하던지 문서를 요약하는 작업을 말한다. \n",
    "# 토픽모델링이라는 연구분야가 따로 존재한다. 사실 간단한 개념은 아니다. \n",
    "# 텍스트가 가지고있는 본래의 의미를 최대한 유지하면서 텍스트의 내용을 간략하게 줄이는 것을 말한다.\n",
    "# 여러줄의 중요한 부분을 뽑아서 요약문을 만드는 작업을 할 수 있다(잠재 의미 분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f70ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서요약 과정\n",
    "# 문서요약은 3 과정으로 정의가능하다.\n",
    "# I : 인터프리테이션 - 텍스트로 된 문자를 해석하고 컴퓨터가 이해할 수 있도록 표현하는 것을 말한다. \n",
    "# T : 트랜스포메이션 - 요약문으로 표현할 수 있도록 본래문장을 변형하는 것을 말한다.\n",
    "# G : 제너레이션 - 최종적으로 요약문을 생성하는 것이다.\n",
    "# 세 가지 과정을 순차적으로 진행하면서 요약하는 것이다.\n",
    "# 문서를 요약할 때 입력 / 출력에 해당하는 문서, 목적에 해당하는 문서등으로 다양하게 분류 가능할 것이다.\n",
    "# 문서는 한 개일 수도 있고 여러 개일수도 있다. 일반문서일 수 있고 전문문서일 수 있다. 특정 도메인 특화여부도 다르다.\n",
    "# 분류에 따라 방법이 달라져야 한다. 도메인에 따라 차이가 크다.  \n",
    "# 문서가 어느정도의 규모인지 고려를 해 볼 필요가 있다. 트위터와 뉴스는 길이가 다르다. 장르에 따라서도 다르다.\n",
    "# 문서요약을 할 때에 중요한 것은 문서를 요약한 결과를 낼 때 단순히 주어진 문서 안에 있는 단어들을 조합해서 문서를 요약할 것인지 \n",
    "# 문서 전체를 존재하지 않는 단어이지만 함축적인 의미를 담고있는 문장을 생성할 것인지 고려해야 한다. \n",
    "# 요약문에 대한 문장의 길이를 어떻게 할 것인지도 조정해야할 것이다. \n",
    "# 또는 편파성을 고려해야한다. 편파적인 요약문이 만들어지지 않도록 해야한다. AI는 의도적으로 편파적으로 만들 수 있다. \n",
    "# AI는 물어보는거 대답을 전부 잘해준다. 하지만 특정 이익집단에 의해 원하는 대답만 하게 만들 수도 있다. \n",
    "# 입력문서는 다양한 형태로 적용된다. \n",
    "# 일반적으로 사용자는 자신이 알고싶은 것을 요약해서 알려주는 것을 원할 것이다.사실상 정보를 알고 싶을 것이다. \n",
    "# 하지만 지금은 정보가 너무나도 많다. 차라리 요약을 해 주길 원한다. \n",
    "# 예전에는 지식을 책을 통해 접했지만 지금은 영상(그것도 축약형)으로 본다. \n",
    "# 전체 문서의 내용을 담고있으면서도 핵심 키가 되는 부분을 압축해서 전체적인 의미가 함의된 문장으로 표현할 수 있어야 한다. \n",
    "# 사용자가 원하는 방식으로 만들 수 있고 입력문서를 기반으로 요약할 수도 있을 것이다. \n",
    "# 사용 용도의 측면에서도 볼 수 있을 것이다. 중요한 정보만 원하느냐. 자세한 정보를 원하느냐의 차이도 존재한다. \n",
    "# 확장성 측면에서 생각할 수도 있다. 요약문이 원본 문서에 배경을 두는지 다른 문서와 비교할 때 최신의 문서를 제공할 수 있는지도\n",
    "# 중요하다. 문서는 동적이어야 한다. 정적이면 안된다. 과거의 뉴스기사를 요약한 것이다.\n",
    "# 중요한 단어들로부터 요약문서를 만드는 것이 추출방식이다. 추상은 전체적인 문서를 요약해서 문장으로 설명하는 것이다. \n",
    "# LDA기법은 행렬분해기법과 유사하다. \n",
    "# 기계학습 기반으로 토픽모델을 추출하는데 그것을 keyBERT라고 한다. 주제어를 잘 검출할 수 있도록 설계되었다. \n",
    "# 물론 한국어도 많이 나와있다. CTM같은 것들이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37deba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# LSA는 빈번하게 등장하는 단어들을 중요도를 계산해서 모델링하는 방식이다. \n",
    "# DTM과 TF-IDF를 기반으로 하기에 단어의 의미를 고려하지 못하는 LDA방식을 개선한 것이다. \n",
    "# 단어가 너무 빈번하면 중요하지 않다고 판단한 TF-IDF가 있었다. 물론 관사같이 연결용 단어는 중요하진 않다.\n",
    "# 확실히 논문이 너무 많이 쏟아져나온다... 꽤 많다.\n",
    "# LSA는 잠재의미분석(latent semantic analysis을 위한 새로운 대안이다. \n",
    "# 쉽고 빠르게 구현 가능하며 단어의 잠재적 의미를 이끌어 낼 수 있어서 유용하다.\n",
    "# 이미 계산된 특성상 새로운 데이터를 계산하려고 하면 처음부터 다시 계산해야 한다. 업데이트도 쉽지 않다. \n",
    "# 신경망 기반의 단어표현방식이 각광받는 현재에는 잘 쓰이지는 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fc84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "# SVD는 특이값 분해이다. 선형대수의 마지막 분야에 해당한다. \n",
    "# 취업이후 선형대수학, 통계학을 공부해보자. \n",
    "# A가 M*n일때 3개의 행렬의 곱으로 분해하는 것을 말한다. \n",
    "# 자신과 자신의 전치 행렬을 곱한 것이 직교행렬이다. 국민대 제출할 때 했었다. \n",
    "# 대각선을 제외한 곳의 원소가 모두 0인 행렬은 대각행렬이라고 한다. \n",
    "# svd로 나온 대각행렬의 대각 원소의 값을 행렬 A의 특이값이라고 한다\n",
    "# 원래 행렬에서 행과 열을 바꾸는 것을 전치행렬이라고 한다. 예전에 넘파이때 배웠다. \n",
    "# 단위행렬은 대각선으로 1인 행렬이다. \n",
    "# 역행렬은 어느 행렬과 곱하여 단위행렬이 나오는 행렬을 말한다. \n",
    "# 직교행렬은 A 랑 A.T를 곱할 때 단위행렬을 만족하고 다시 바꾸면 역행렬이 되는 것을 직교행렬이라고 한다.\n",
    "# 대각행렬은 대각형으로 내용이 존재하는 경우이다. \n",
    "# SVD는 풀 SVD(full SVD)와 절단된 SVD(turncated SVD)가 존재한다. \n",
    "# 일부 벡터를 삭제한 것인 절단 SVD는 LSA에 사용한다. \n",
    "# 일부 정보를 손실을 감안하고 절단한 것이다. 그래서 절단된 SVD로 기존행렬을 복구할 수는 없다. \n",
    "# t를 작게 잡아야 노이즈제거가 잘 이루어진다. 계산비용이 낮아지면서 상대적으로 중요하지 않은 정보를 정리할 수 있다.\n",
    "# PCA알고리즘과 비슷하다. \n",
    "# TF-IDF는 단어의미 고려가 전혀 안되어있다. 절단된 SVD를 사용해서 차원축소를 시키고 단어의 잠재적 의미를 끌어오는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffd0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잠재 디레클레 할당(LDA)\n",
    "# 문서가 있을 때 주제를 찾아내는 역할을 하는 프로세스를 말한다. 공공기관은 민원이 엄청나게 몰려온다. \n",
    "# 이러한 민원을 처리하는 것을 여러사람이 처리하는 시스템을 구축하는 것과 같다. \n",
    "# 논문을 통해서 지금도 많이 활용되고 있다. 지금도 잘 쓰인다는 이야기이다. \n",
    "# 모든 문서들은 토픽이 있고 토픽이 혼합되어있다고 가정한다. \n",
    "# 문서의 집합으로 부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘이다.\n",
    "# 단어의 순서는 상관없다. 문서에 사용할 단어의 개수를 정하면 토픽의 혼합을 확률분포에 기반하여 결정한다. \n",
    "# https://lettier.com/projects/lda-topic-modeling/\n",
    "# 여기서 확인 가능하다. \n",
    "# 사실 LDA가 두 개다. 머신러닝 LDA(Linear Discriminant Analysis 선형 판별 분석) 알고리즘.\n",
    "# 그리고 딥러닝 LDA(Latent Dirichlet Allocation 잠재 디리클레 할당)가 있다. \n",
    "# \n",
    "# 문서가 3개가 존재한다.\n",
    "# 1. 저는 사과랑 바나나를 먹어요\n",
    "# 2. 우리는 귀여운 강아지가 좋아요\n",
    "# 3. 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요\n",
    "# \n",
    "# 1번의 토픽이 100%고 2번의 토픽이 100%라고 하자. 3번의 토빅은 2번 60%, 1번 40%로 토픽이 분포한다. \n",
    "# 수행과정은 다음과 같다.\n",
    "# 1. 토픽의 개수 알려주기\n",
    "# 2. 모든 단어를 토픽에 할당하기\n",
    "# 3. 토픽을 재할당하는 과정을 거치기\n",
    "# 위의 토픽은 과일과 강아지로 토픽이 분포가 될 것이다. \n",
    "# 뭔가 apriori와 느낌이 비슷하다. 차이점이 있다면 소비자가 원하는 물건을 추천하는 아페리오리와 토픽을 찾아서 종합하는 느낌의\n",
    "# LDA는 느낌이 많이 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d809509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 솔직히 뭔 말인지 알아듣기 어렵다. \n",
    "# 다만 고객의 심리를 파악하는데에 디레클레 분포만큼 좋은 것도 없다. 고객의 인사이트를 파악하는데 사용하는 것이 디레클레이다. \n",
    "# 토픽모델링은 숨겨진 주제를 찾는 것이 핵심이기에 같은 주제끼리 묶어주는 역할도 한다. \n",
    "# 이건 비지도학습에 해당한다. 문서만 있지 그 문서가 무엇인지를 설명하지 않는 것이다. \n",
    "# \n",
    "# 제목은 없지만 기사/글이 10개가 있다고 해보자. 10개의 기사를 3가지의 카테고리로 나누어라! 라고 말한다면?\n",
    "# 3가지 카테고리에 어떤 단어가 들어갈지를 먼저 생각할 것이다. 정치, 사회, 경제라고 나눈다고 가정하자.\n",
    "# 각각의 단어의 분포를 파악해서 그 단어를 바탕으로 카테고리를 나눌 것이다.\n",
    "# 컴퓨터는 문서가 있고 문서 안에 단어가 있다면 디리클레분포를 가정한다. \n",
    "# 토픽 내부에 문서와 단어를 하나씩 넣어보고 잠재적 의미를 찾는다. \n",
    "# 솔직히 프로덕트라는 말은 흔한 개념은 아니니까 생략하자.\n",
    "# 1. 문고리 거래 하실분\n",
    "# 2. 가방 나눔합니다. 문고리 드림\n",
    "# 3. 비내면 거래로 합니다. 택배로 할께요.\n",
    "# 이런 상황이면 1번은 문고리, 거래가 토픽이고 2번은 가방, 나눔, 문고리, 드림으로 핵심단어가 나온다. 3번은 비대면, 거래, 택배다.\n",
    "# 여기서 마스크드 훈련을 했던 것처럼 토픽을 가려서 훈련한다. 골때린다. \n",
    "# 마스크드 훈련과 다른점은 가정에서부터 시작한다는 것이다. \n",
    "# 분포를 알고 있다면 가장 유사한 분포를 찾는 것이다. 즉 분포의 모양을 보는 것이 마스크드 모델과는 차이가 존재한다. \n",
    "# 단어의 순서가 상관없는 것이 마스크드 언어모델과 차이점이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd6c7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af40e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기(shape) : (4, 9)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "print('DTM의 크기(shape) :', np.shape(A))\n",
    "# 4,9짜리 행렬이다. 문서가 4개가 있고 9개의 단어가 존재한다고 가정하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e13a1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[ 0.24  0.75  0.    0.62]\n",
      " [ 0.51  0.44 -0.   -0.74]\n",
      " [ 0.83 -0.49 -0.    0.27]\n",
      " [ 0.   -0.    1.   -0.  ]]\n",
      "행렬 U의 크기(shape) : (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# 원래의 행렬을 여러개로 나누고 특징값을 추출하는 것이다. 데이터에 잠재되어있는 특성값을 알아내는 것이다.\n",
    "U, s, VT = np.linalg.svd(A, full_matrices = True) # 행렬을 분해한 것이다.\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :',np.shape(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48835a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59550a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.31,  0.31,  0.28,  0.8 ,  0.09,  0.28,  0.  ,  0.  ],\n",
       "       [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ],\n",
       "       [ 0.58, -0.  ,  0.  ,  0.  , -0.  ,  0.  , -0.  ,  0.58,  0.58],\n",
       "       [-0.  ,  0.35,  0.35, -0.16, -0.25,  0.8 , -0.16,  0.  ,  0.  ],\n",
       "       [-0.  , -0.78, -0.01, -0.2 ,  0.4 ,  0.4 , -0.2 ,  0.  ,  0.  ],\n",
       "       [-0.29,  0.31, -0.78, -0.24,  0.23,  0.23,  0.01,  0.14,  0.14],\n",
       "       [-0.29, -0.1 ,  0.26, -0.59, -0.08, -0.08,  0.66,  0.14,  0.14],\n",
       "       [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19,  0.75, -0.25],\n",
       "       [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19, -0.25,  0.75]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.round(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95f011cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64,  1.54,  0.  ,  0.47],\n",
       "       [ 1.36,  0.91, -0.  , -0.57],\n",
       "       [ 2.23, -1.  , -0.  ,  0.21],\n",
       "       [ 0.  , -0.  ,  1.73, -0.  ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이렇게 4*4,1*4,9*9 의 행렬들이 만들어졌다.\n",
    "# 이제 이것들을 원래대로 합치는 것이 일일 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b67941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.212 -0.285 -0.574 -0.44 ]\n",
      " [-0.33   1.184  1.615  0.367]\n",
      " [-0.014  0.63   1.71  -1.327]\n",
      " [ 0.402 -0.191  1.404 -1.969]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# 4X4 Random 행렬 a 생성 \n",
    "np.random.seed(121)\n",
    "a = np.random.randn(4,4)\n",
    "print(np.round(a, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "835ecf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4) (4,) (4, 4)\n",
      "U matrix:\n",
      " [[-0.079 -0.318  0.867  0.376]\n",
      " [ 0.383  0.787  0.12   0.469]\n",
      " [ 0.656  0.022  0.357 -0.664]\n",
      " [ 0.645 -0.529 -0.328  0.444]]\n",
      "Sigma Value:\n",
      " [3.423 2.023 0.463 0.079]\n",
      "V transpose matrix:\n",
      " [[ 0.041  0.224  0.786 -0.574]\n",
      " [-0.2    0.562  0.37   0.712]\n",
      " [-0.778  0.395 -0.333 -0.357]\n",
      " [-0.593 -0.692  0.366  0.189]]\n"
     ]
    }
   ],
   "source": [
    "U, Sigma, Vt = svd(a)\n",
    "print(U.shape, Sigma.shape, Vt.shape)\n",
    "print('U matrix:\\n',np.round(U, 3))\n",
    "print('Sigma Value:\\n',np.round(Sigma, 3))    # 원래는 대괄호 요소값이었지만 나머지가 0이라서 대각요소값들 4개가 존재하는 것이다. \n",
    "                                              # a행렬의 특이값이다. \n",
    "print('V transpose matrix:\\n',np.round(Vt, 3))\n",
    "# 문제는 시그마벨류는 1차원 구조이다. 이걸 2차원으로 바꿔야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a040205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4229581 , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 2.02287339, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.46263157, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.07935069]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(Sigma)                                 # 대각행렬을 만드는 식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7601f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27089043, -0.64373352,  0.40088514,  0.02985847],\n",
       "       [ 1.31080548,  1.59109688,  0.05552549,  0.03724266],\n",
       "       [ 2.24685261,  0.04537671,  0.16501535, -0.05271899],\n",
       "       [ 2.20832581, -1.06956796, -0.15167705,  0.03519512]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(U, np.diag(Sigma)) # 여기에 VT를 곱해야 할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e93f3193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21203317, -0.28492917, -0.57389821, -0.44031017],\n",
       "       [-0.33011056,  1.18369457,  1.61537293,  0.36706247],\n",
       "       [-0.01411931,  0.6296418 ,  1.70964074, -1.32698736],\n",
       "       [ 0.40187312, -0.19142667,  1.40382596, -1.96876855]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(U, np.diag(Sigma)),Vt)  # 얼핏 비슷하게 나왔다. 잠재되어있는 자료를 이런 식으로 찾아낼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "611e89fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 행렬:\n",
      " [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941]\n",
      " [0.5557906  0.74552394 0.24849976 0.9686594  0.95268418 0.48984885]\n",
      " [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852]\n",
      " [0.4056155  0.56730065 0.24575605 0.22573721 0.03827786 0.58098021]\n",
      " [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176]\n",
      " [0.51161442 0.46920965 0.6439515  0.82081228 0.14548493 0.01806415]]\n",
      "\n",
      "분해 행렬 차원: (6, 6) (6,) (6, 6)\n",
      "\n",
      "Sigma값 행렬: [3.2535007  0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ]\n"
     ]
    }
   ],
   "source": [
    "# 원본 행렬을 출력하고, SVD를 적용할 경우 U, Sigma, Vt 의 차원 확인 \n",
    "np.random.seed(121)\n",
    "matrix = np.random.random((6, 6))\n",
    "print('원본 행렬:\\n',matrix)\n",
    "U, Sigma, Vt = svd(matrix, full_matrices=False)\n",
    "print('\\n분해 행렬 차원:',U.shape, Sigma.shape, Vt.shape)\n",
    "print('\\nSigma값 행렬:', Sigma)  # 특이값 행렬(잠재되어있는 특성) 차원축소를 한 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6428deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Truncated SVD 분해 행렬 차원: (6, 6) (6,) (6, 6)\n",
      "\n",
      "Truncated SVD Sigma값 행렬: [3.2535007  0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ]\n",
      "\n",
      "Truncated SVD로 분해 후 복원 행렬:\n",
      " [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941]\n",
      " [0.5557906  0.74552394 0.24849976 0.9686594  0.95268418 0.48984885]\n",
      " [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852]\n",
      " [0.4056155  0.56730065 0.24575605 0.22573721 0.03827786 0.58098021]\n",
      " [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176]\n",
      " [0.51161442 0.46920965 0.6439515  0.82081228 0.14548493 0.01806415]]\n"
     ]
    }
   ],
   "source": [
    "num_components = 4\n",
    "U_tr, Sigma_tr, Vt_tr = svd(matrix, num_components)\n",
    "print('\\nTruncated SVD 분해 행렬 차원:',U_tr.shape, Sigma_tr.shape, Vt_tr.shape)\n",
    "print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr)\n",
    "matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr)  # output of TruncatedSVD\n",
    "\n",
    "print('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99093e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 행렬:\n",
      " [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941]\n",
      " [0.5557906  0.74552394 0.24849976 0.9686594  0.95268418 0.48984885]\n",
      " [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852]\n",
      " [0.4056155  0.56730065 0.24575605 0.22573721 0.03827786 0.58098021]\n",
      " [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176]\n",
      " [0.51161442 0.46920965 0.6439515  0.82081228 0.14548493 0.01806415]]\n",
      "\n",
      "분해 행렬 차원: (6, 6) (6,) (6, 6)\n",
      "\n",
      "Sigma값 행렬: [3.2535007  0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ]\n",
      "\n",
      "Truncated SVD 분해 행렬 차원: (6, 4) (4,) (4, 6)\n",
      "\n",
      "Truncated SVD Sigma값 행렬: [0.55463089 0.83865238 0.88116505 3.2535007 ]\n",
      "\n",
      "Truncated SVD로 분해 후 복원 행렬:\n",
      " [[0.19222941 0.21792946 0.15951023 0.14084013 0.81641405 0.42533093]\n",
      " [0.44874275 0.72204422 0.34594106 0.99148577 0.96866325 0.4754868 ]\n",
      " [0.12656662 0.88860729 0.30625735 0.59517439 0.28036734 0.93961948]\n",
      " [0.23989012 0.51026588 0.39697353 0.27308905 0.05971563 0.57156395]\n",
      " [0.83806144 0.78847467 0.93868685 0.72673231 0.6740867  0.73812389]\n",
      " [0.59726589 0.47953891 0.56613544 0.80746028 0.13135039 0.03479656]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.linalg import svd\n",
    "\n",
    "np.random.seed(121)\n",
    "matrix = np.random.random((6, 6))\n",
    "print('원본 행렬:\\n',matrix)\n",
    "U, Sigma, Vt = svd(matrix, full_matrices=False)\n",
    "print('\\n분해 행렬 차원:',U.shape, Sigma.shape, Vt.shape)\n",
    "print('\\nSigma값 행렬:', Sigma)\n",
    "\n",
    "\n",
    "num_components = 4\n",
    "U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components)\n",
    "print('\\nTruncated SVD 분해 행렬 차원:',U_tr.shape, Sigma_tr.shape, Vt_tr.shape)    # 만약 내가 추출하고 싶은 벡터가 4개라면 다음과 같아진다. \n",
    "print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr)                    \n",
    "matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr)  # output of TruncatedSVD\n",
    "\n",
    "print('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr) # 값이 비슷하나 차이가 존재한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bea3177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이게 실제로는 어떻게 쓰일지 알아보자. \n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b57d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82c97089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv') # error_bad_lines=False는 오류가 있으면 건너뛰라는 소리이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db6da0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082163</th>\n",
       "      <td>20170630</td>\n",
       "      <td>when is it ok to compliment a womans smile a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082164</th>\n",
       "      <td>20170630</td>\n",
       "      <td>white house defends trumps tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082165</th>\n",
       "      <td>20170630</td>\n",
       "      <td>winter closes in on tasmania as snow ice falls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082166</th>\n",
       "      <td>20170630</td>\n",
       "      <td>womens world cup australia wins despite atapat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082167</th>\n",
       "      <td>20170630</td>\n",
       "      <td>youtube stunt death foreshadowed by tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1082168 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1082163      20170630  when is it ok to compliment a womans smile a g...\n",
       "1082164      20170630                   white house defends trumps tweet\n",
       "1082165      20170630     winter closes in on tasmania as snow ice falls\n",
       "1082166      20170630  womens world cup australia wins despite atapat...\n",
       "1082167      20170630          youtube stunt death foreshadowed by tweet\n",
       "\n",
       "[1082168 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af2b1df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤드라인 텍스트는 제목에 해당하는 부분이다보니 많이 중요하다. \n",
    "text = data[['headline_text']]\n",
    "text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "689c267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12616\\443543600.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1) # 단어단위로 토큰화를 한 것이다. \n",
    "                                                                                                  # 확실히 이런 것은 코랩에서 하는 것이 낫다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0206fc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, against, community, broadcastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, be, aware, of, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, g, calls, for, infrastructure, protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082163</th>\n",
       "      <td>[when, is, it, ok, to, compliment, a, womans, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082164</th>\n",
       "      <td>[white, house, defends, trumps, tweet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082165</th>\n",
       "      <td>[winter, closes, in, on, tasmania, as, snow, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082166</th>\n",
       "      <td>[womens, world, cup, australia, wins, despite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082167</th>\n",
       "      <td>[youtube, stunt, death, foreshadowed, by, tweet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1082168 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline_text\n",
       "0        [aba, decides, against, community, broadcastin...\n",
       "1        [act, fire, witnesses, must, be, aware, of, de...\n",
       "2        [a, g, calls, for, infrastructure, protection,...\n",
       "3        [air, nz, staff, in, aust, strike, for, pay, r...\n",
       "4        [air, nz, strike, to, affect, australian, trav...\n",
       "...                                                    ...\n",
       "1082163  [when, is, it, ok, to, compliment, a, womans, ...\n",
       "1082164             [white, house, defends, trumps, tweet]\n",
       "1082165  [winter, closes, in, on, tasmania, as, snow, i...\n",
       "1082166  [womens, world, cup, australia, wins, despite,...\n",
       "1082167   [youtube, stunt, death, foreshadowed, by, tweet]\n",
       "\n",
       "[1082168 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5222446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12616\\1467219579.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])  # 단어가 스탑이라는 워드에 없으면 추가하라는 뜻이다.\n"
     ]
    }
   ],
   "source": [
    "# 불용어를 제거해야할 듯 하다. \n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])  # 단어가 스탑이라는 워드에 없으면 추가하라는 뜻이다. \n",
    "print(text.head(5)) \n",
    "# 3인칭 단수를 1인칭으로 바꾸거나 크기가 작은 단어들을 제거하는 작업 등을 해야할 듯 하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dcb14d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0       [aba, decide, community, broadcast, licence]\n",
      "1      [act, fire, witness, must, aware, defamation]\n",
      "2      [g, call, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12616\\1264568339.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer   # 레머타이저는 3인칭 단수에 해당하는 단어를 1인칭으로 변환하기 위해서 사용한 것이다.\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20dc04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 길이 3이하 단어 제거\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d720b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12616\\876560129.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = detokenized_doc\n"
     ]
    }
   ],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "# 다시 text['headline_text']에 재저장\n",
    "text['headline_text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5c62332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                decide community broadcast licence\n",
       "1                fire witness must aware defamation\n",
       "2             call infrastructure protection summit\n",
       "3                            staff aust strike rise\n",
       "4               strike affect australian travellers\n",
       "                             ...                   \n",
       "1082163               compliment womans smile guide\n",
       "1082164              white house defend trump tweet\n",
       "1082165             winter close tasmania snow fall\n",
       "1082166    womens world australia despite atapattus\n",
       "1082167        youtube stunt death foreshadow tweet\n",
       "Name: headline_text, Length: 1082168, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['headline_text'] # 대괄호 없애고 원래 상태로 되돌린 상태다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "811e5173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기 : (1082168, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 상위 1,000개의 단어를 보존 \n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000) # 벡터라이저의 스탑워드, 글자수를 지정했다.\n",
    "X = vectorizer.fit_transform(text['headline_text'])                    # 벡터로 변환하였다. \n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf5418c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decide': 255,\n",
       " 'community': 191,\n",
       " 'licence': 521,\n",
       " 'witness': 981,\n",
       " 'infrastructure': 457,\n",
       " 'protection': 678,\n",
       " 'summit': 858,\n",
       " 'staff': 833,\n",
       " 'aust': 59,\n",
       " 'strike': 847,\n",
       " 'rise': 745,\n",
       " 'affect': 16,\n",
       " 'australian': 61,\n",
       " 'jump': 486,\n",
       " 'record': 707,\n",
       " 'break': 112,\n",
       " 'aussie': 57,\n",
       " 'waste': 962,\n",
       " 'match': 551,\n",
       " 'address': 13,\n",
       " 'security': 782,\n",
       " 'council': 213,\n",
       " 'iraq': 469,\n",
       " 'australia': 60,\n",
       " 'million': 563,\n",
       " 'celebrate': 150,\n",
       " 'plan': 629,\n",
       " 'ahead': 22,\n",
       " 'boost': 105,\n",
       " 'water': 964,\n",
       " 'supply': 860,\n",
       " 'unite': 936,\n",
       " 'state': 838,\n",
       " 'dismiss': 274,\n",
       " 'report': 724,\n",
       " 'troop': 925,\n",
       " 'british': 116,\n",
       " 'arrive': 48,\n",
       " 'lead': 508,\n",
       " 'double': 279,\n",
       " 'bushfire': 125,\n",
       " 'victims': 946,\n",
       " 'urge': 943,\n",
       " 'businesses': 127,\n",
       " 'prepare': 656,\n",
       " 'attack': 54,\n",
       " 'final': 342,\n",
       " 'defeat': 258,\n",
       " 'fuel': 371,\n",
       " 'leave': 515,\n",
       " 'miss': 567,\n",
       " 'fund': 372,\n",
       " 'bank': 74,\n",
       " 'home': 429,\n",
       " 'help': 416,\n",
       " 'youth': 996,\n",
       " 'chief': 160,\n",
       " 'fail': 321,\n",
       " 'secure': 781,\n",
       " 'councillor': 214,\n",
       " 'protect': 677,\n",
       " 'heritage': 417,\n",
       " 'welcome': 969,\n",
       " 'ambulance': 30,\n",
       " 'levy': 518,\n",
       " 'decision': 256,\n",
       " 'tell': 888,\n",
       " 'leadership': 511,\n",
       " 'threat': 898,\n",
       " 'expect': 311,\n",
       " 'death': 251,\n",
       " 'toll': 906,\n",
       " 'continue': 205,\n",
       " 'hold': 427,\n",
       " 'iraqi': 470,\n",
       " 'learn': 514,\n",
       " 'march': 546,\n",
       " 'anger': 33,\n",
       " 'govt': 388,\n",
       " 'soldier': 819,\n",
       " 'dispute': 275,\n",
       " 'process': 667,\n",
       " 'plant': 631,\n",
       " 'phone': 625,\n",
       " 'england': 306,\n",
       " 'change': 156,\n",
       " 'recover': 708,\n",
       " 'clean': 172,\n",
       " 'cost': 211,\n",
       " 'seek': 783,\n",
       " 'build': 120,\n",
       " 'feed': 333,\n",
       " 'national': 576,\n",
       " 'firefighters': 347,\n",
       " 'spill': 827,\n",
       " 'injure': 458,\n",
       " 'head': 410,\n",
       " 'highway': 422,\n",
       " 'crash': 222,\n",
       " 'profit': 669,\n",
       " 'domestic': 278,\n",
       " 'violence': 951,\n",
       " 'risk': 746,\n",
       " 'announce': 36,\n",
       " 'bridge': 113,\n",
       " 'work': 987,\n",
       " 'upgrade': 940,\n",
       " 'court': 219,\n",
       " 'accuse': 11,\n",
       " 'policy': 641,\n",
       " 'girl': 383,\n",
       " 'gold': 385,\n",
       " 'coast': 182,\n",
       " 'hear': 412,\n",
       " 'project': 671,\n",
       " 'club': 178,\n",
       " 'feel': 334,\n",
       " 'smoke': 815,\n",
       " 'impact': 445,\n",
       " 'blame': 92,\n",
       " 'green': 394,\n",
       " 'offer': 592,\n",
       " 'police': 640,\n",
       " 'station': 839,\n",
       " 'group': 395,\n",
       " 'meet': 555,\n",
       " 'north': 584,\n",
       " 'west': 971,\n",
       " 'rock': 752,\n",
       " 'gain': 374,\n",
       " 'access': 9,\n",
       " 'credit': 224,\n",
       " 'card': 142,\n",
       " 'issue': 476,\n",
       " 'come': 184,\n",
       " 'health': 411,\n",
       " 'minister': 566,\n",
       " 'heavy': 415,\n",
       " 'survey': 866,\n",
       " 'near': 579,\n",
       " 'pull': 683,\n",
       " 'open': 600,\n",
       " 'inquest': 460,\n",
       " 'underway': 932,\n",
       " 'investigation': 466,\n",
       " 'creek': 225,\n",
       " 'plead': 636,\n",
       " 'white': 975,\n",
       " 'house': 437,\n",
       " 'arrest': 47,\n",
       " 'bomb': 101,\n",
       " 'vote': 954,\n",
       " 'river': 747,\n",
       " 'management': 545,\n",
       " 'israeli': 475,\n",
       " 'force': 357,\n",
       " 'push': 684,\n",
       " 'gaza': 378,\n",
       " 'consider': 203,\n",
       " 'murder': 573,\n",
       " 'case': 144,\n",
       " 'allege': 28,\n",
       " 'scare': 769,\n",
       " 'surprise': 865,\n",
       " 'confidence': 200,\n",
       " 'hand': 405,\n",
       " 'demand': 263,\n",
       " 'service': 792,\n",
       " 'central': 151,\n",
       " 'attempt': 55,\n",
       " 'charge': 157,\n",
       " 'aboriginal': 5,\n",
       " 'raid': 693,\n",
       " 'jail': 477,\n",
       " 'fraud': 365,\n",
       " 'light': 524,\n",
       " 'plane': 630,\n",
       " 'lobby': 532,\n",
       " 'lose': 538,\n",
       " 'seat': 778,\n",
       " 'drug': 290,\n",
       " 'crop': 233,\n",
       " 'western': 972,\n",
       " 'mayor': 552,\n",
       " 'warn': 960,\n",
       " 'protesters': 680,\n",
       " 'focus': 354,\n",
       " 'hill': 423,\n",
       " 'woes': 982,\n",
       " 'lift': 523,\n",
       " 'growth': 398,\n",
       " 'young': 995,\n",
       " 'drink': 283,\n",
       " 'alcohol': 24,\n",
       " 'restrictions': 734,\n",
       " 'predict': 654,\n",
       " 'northern': 585,\n",
       " 'women': 984,\n",
       " 'live': 531,\n",
       " 'raise': 696,\n",
       " 'hospital': 433,\n",
       " 'concern': 196,\n",
       " 'parliament': 614,\n",
       " 'reject': 717,\n",
       " 'claim': 169,\n",
       " 'clear': 173,\n",
       " 'defend': 260,\n",
       " 'zealand': 997,\n",
       " 'zimbabwe': 998,\n",
       " 'race': 692,\n",
       " 'campaign': 132,\n",
       " 'pledge': 637,\n",
       " 'drought': 288,\n",
       " 'relief': 719,\n",
       " 'nurse': 589,\n",
       " 'number': 588,\n",
       " 'asylum': 53,\n",
       " 'japanese': 479,\n",
       " 'student': 850,\n",
       " 'time': 903,\n",
       " 'stay': 840,\n",
       " 'politics': 643,\n",
       " 'opposition': 602,\n",
       " 'begin': 82,\n",
       " 'students': 851,\n",
       " 'world': 990,\n",
       " 'cross': 234,\n",
       " 'country': 217,\n",
       " 'doubt': 280,\n",
       " 'rule': 759,\n",
       " 'pair': 610,\n",
       " 'face': 319,\n",
       " 'avoid': 65,\n",
       " 'lions': 529,\n",
       " 'peace': 619,\n",
       " 'agreement': 21,\n",
       " 'bring': 114,\n",
       " 'second': 779,\n",
       " 'farmers': 327,\n",
       " 'crack': 220,\n",
       " 'driver': 285,\n",
       " 'safety': 765,\n",
       " 'federal': 331,\n",
       " 'crime': 228,\n",
       " 'probe': 664,\n",
       " 'launch': 505,\n",
       " 'program': 670,\n",
       " 'monitor': 570,\n",
       " 'forest': 360,\n",
       " 'harvest': 408,\n",
       " 'public': 682,\n",
       " 'check': 159,\n",
       " 'qantas': 685,\n",
       " 'international': 462,\n",
       " 'crew': 226,\n",
       " 'unions': 934,\n",
       " 'sack': 763,\n",
       " 'question': 690,\n",
       " 'grow': 396,\n",
       " 'control': 207,\n",
       " 'trial': 924,\n",
       " 'olympic': 597,\n",
       " 'rain': 695,\n",
       " 'ease': 295,\n",
       " 'highlight': 421,\n",
       " 'stock': 843,\n",
       " 'order': 603,\n",
       " 'anti': 38,\n",
       " 'authorities': 64,\n",
       " 'surgery': 864,\n",
       " 'closure': 177,\n",
       " 'angry': 34,\n",
       " 'review': 742,\n",
       " 'premier': 655,\n",
       " 'action': 12,\n",
       " 'murray': 574,\n",
       " 'stand': 835,\n",
       " 'search': 776,\n",
       " 'shire': 800,\n",
       " 'kill': 491,\n",
       " 'slow': 811,\n",
       " 'recovery': 709,\n",
       " 'economy': 298,\n",
       " 'line': 527,\n",
       " 'delay': 261,\n",
       " 'label': 497,\n",
       " 'shark': 797,\n",
       " 'sign': 807,\n",
       " 'stop': 844,\n",
       " 'sugar': 855,\n",
       " 'industry': 456,\n",
       " 'reveal': 741,\n",
       " 'surge': 863,\n",
       " 'sales': 767,\n",
       " 'look': 536,\n",
       " 'future': 373,\n",
       " 'place': 628,\n",
       " 'talk': 877,\n",
       " 'asian': 51,\n",
       " 'nuclear': 587,\n",
       " 'downer': 281,\n",
       " 'tasmanian': 880,\n",
       " 'scientists': 772,\n",
       " 'east': 296,\n",
       " 'deny': 264,\n",
       " 'quit': 691,\n",
       " 'teen': 885,\n",
       " 'test': 894,\n",
       " 'thousands': 897,\n",
       " 'remember': 721,\n",
       " 'darwin': 243,\n",
       " 'support': 861,\n",
       " 'protest': 679,\n",
       " 'tree': 923,\n",
       " 'disease': 273,\n",
       " 'study': 852,\n",
       " 'target': 878,\n",
       " 'local': 533,\n",
       " 'councils': 215,\n",
       " 'poll': 644,\n",
       " 'victorian': 948,\n",
       " 'honour': 430,\n",
       " 'award': 67,\n",
       " 'retire': 738,\n",
       " 'season': 777,\n",
       " 'coach': 179,\n",
       " 'players': 634,\n",
       " 'friday': 369,\n",
       " 'master': 550,\n",
       " 'paul': 618,\n",
       " 'williams': 978,\n",
       " 'warriors': 961,\n",
       " 'wine': 980,\n",
       " 'dead': 248,\n",
       " 'rebel': 704,\n",
       " 'philippines': 624,\n",
       " 'army': 46,\n",
       " 'sale': 766,\n",
       " 'higher': 420,\n",
       " 'education': 300,\n",
       " 'appoint': 42,\n",
       " 'land': 501,\n",
       " 'declare': 257,\n",
       " 'result': 735,\n",
       " 'cancel': 134,\n",
       " 'release': 718,\n",
       " 'flag': 350,\n",
       " 'port': 649,\n",
       " 'baby': 69,\n",
       " 'burn': 123,\n",
       " 'brisbane': 115,\n",
       " 'weather': 965,\n",
       " 'cause': 149,\n",
       " 'nationals': 577,\n",
       " 'high': 419,\n",
       " 'doctor': 276,\n",
       " 'wait': 955,\n",
       " 'inquiry': 461,\n",
       " 'wind': 979,\n",
       " 'bush': 124,\n",
       " 'hope': 431,\n",
       " 'late': 503,\n",
       " 'clash': 171,\n",
       " 'smash': 813,\n",
       " 'mark': 548,\n",
       " 'shock': 801,\n",
       " 'christmas': 166,\n",
       " 'detention': 268,\n",
       " 'centre': 152,\n",
       " 'defence': 259,\n",
       " 'spend': 826,\n",
       " 'indigenous': 452,\n",
       " 'legal': 516,\n",
       " 'make': 544,\n",
       " 'emergency': 304,\n",
       " 'general': 379,\n",
       " 'step': 842,\n",
       " 'fear': 330,\n",
       " 'compo': 195,\n",
       " 'slam': 810,\n",
       " 'away': 68,\n",
       " 'reach': 701,\n",
       " 'early': 294,\n",
       " 'development': 269,\n",
       " 'receive': 706,\n",
       " 'research': 727,\n",
       " 'grant': 392,\n",
       " 'king': 494,\n",
       " 'investigate': 465,\n",
       " 'economic': 297,\n",
       " 'losses': 540,\n",
       " 'agree': 20,\n",
       " 'fight': 338,\n",
       " 'families': 323,\n",
       " 'president': 657,\n",
       " 'join': 482,\n",
       " 'hail': 401,\n",
       " 'victory': 949,\n",
       " 'australians': 62,\n",
       " 'boss': 107,\n",
       " 'blaze': 94,\n",
       " 'fish': 349,\n",
       " 'flight': 351,\n",
       " 'wont': 986,\n",
       " 'worker': 988,\n",
       " 'guilty': 400,\n",
       " 'theft': 896,\n",
       " 'start': 837,\n",
       " 'forum': 363,\n",
       " 'regional': 716,\n",
       " 'need': 580,\n",
       " 'france': 364,\n",
       " 'drop': 287,\n",
       " 'french': 367,\n",
       " 'site': 809,\n",
       " 'price': 660,\n",
       " 'newcastle': 581,\n",
       " 'india': 450,\n",
       " 'gippsland': 382,\n",
       " 'remain': 720,\n",
       " 'guard': 399,\n",
       " 'improve': 447,\n",
       " 'child': 161,\n",
       " 'suicide': 856,\n",
       " 'dump': 292,\n",
       " 'terror': 892,\n",
       " 'praise': 653,\n",
       " 'reef': 712,\n",
       " 'score': 773,\n",
       " 'howard': 438,\n",
       " 'criticism': 232,\n",
       " 'crisis': 229,\n",
       " 'want': 959,\n",
       " 'illegal': 444,\n",
       " 'spot': 829,\n",
       " 'melbourne': 556,\n",
       " 'draw': 282,\n",
       " 'likely': 525,\n",
       " 'threaten': 899,\n",
       " 'finals': 343,\n",
       " 'stage': 834,\n",
       " 'iran': 468,\n",
       " 'military': 561,\n",
       " 'human': 439,\n",
       " 'rate': 700,\n",
       " 'rescue': 726,\n",
       " 'book': 103,\n",
       " 'date': 245,\n",
       " 'allow': 29,\n",
       " 'visit': 952,\n",
       " 'appeal': 41,\n",
       " 'post': 651,\n",
       " 'loss': 539,\n",
       " 'point': 639,\n",
       " 'airport': 23,\n",
       " 'close': 175,\n",
       " 'accident': 10,\n",
       " 'suffer': 854,\n",
       " 'damage': 241,\n",
       " 'apologise': 40,\n",
       " 'await': 66,\n",
       " 'sentence': 789,\n",
       " 'incident': 448,\n",
       " 'stab': 832,\n",
       " 'refuse': 715,\n",
       " 'bail': 71,\n",
       " 'robbery': 751,\n",
       " 'efforts': 301,\n",
       " 'zone': 999,\n",
       " 'challenge': 153,\n",
       " 'miner': 564,\n",
       " 'suspect': 868,\n",
       " 'hotel': 435,\n",
       " 'rally': 697,\n",
       " 'park': 613,\n",
       " 'teachers': 883,\n",
       " 'union': 933,\n",
       " 'ready': 702,\n",
       " 'labor': 498,\n",
       " 'business': 126,\n",
       " 'boat': 99,\n",
       " 'thursday': 901,\n",
       " 'church': 167,\n",
       " 'coalition': 181,\n",
       " 'election': 303,\n",
       " 'benefit': 84,\n",
       " 'roll': 754,\n",
       " 'disaster': 271,\n",
       " 'promise': 672,\n",
       " 'people': 620,\n",
       " 'trap': 920,\n",
       " 'pilot': 626,\n",
       " 'adelaide': 14,\n",
       " 'cannabis': 137,\n",
       " 'edge': 299,\n",
       " 'family': 324,\n",
       " 'office': 593,\n",
       " 'trade': 916,\n",
       " 'link': 528,\n",
       " 'camp': 131,\n",
       " 'asia': 50,\n",
       " 'good': 386,\n",
       " 'fatal': 328,\n",
       " 'condemn': 197,\n",
       " 'follow': 355,\n",
       " 'flood': 352,\n",
       " 'despite': 266,\n",
       " 'upset': 941,\n",
       " 'deal': 250,\n",
       " 'road': 748,\n",
       " 'tackle': 875,\n",
       " 'real': 703,\n",
       " 'fine': 345,\n",
       " 'school': 771,\n",
       " 'reopen': 723,\n",
       " 'injury': 459,\n",
       " 'native': 578,\n",
       " 'socceroos': 816,\n",
       " 'solar': 818,\n",
       " 'turn': 930,\n",
       " 'sport': 828,\n",
       " 'liberal': 519,\n",
       " 'play': 632,\n",
       " 'taxi': 881,\n",
       " 'increase': 449,\n",
       " 'expert': 312,\n",
       " 'dairy': 240,\n",
       " 'field': 337,\n",
       " 'days': 247,\n",
       " 'loom': 537,\n",
       " 'tough': 908,\n",
       " 'tourism': 910,\n",
       " 'woman': 983,\n",
       " 'truck': 926,\n",
       " 'destroy': 267,\n",
       " 'firm': 348,\n",
       " 'debate': 253,\n",
       " 'market': 549,\n",
       " 'seven': 793,\n",
       " 'years': 994,\n",
       " 'track': 915,\n",
       " 'speak': 824,\n",
       " 'battle': 77,\n",
       " 'power': 652,\n",
       " 'reporter': 725,\n",
       " 'accc': 7,\n",
       " 'petrol': 623,\n",
       " 'lower': 541,\n",
       " 'scrap': 775,\n",
       " 'hurt': 443,\n",
       " 'cash': 145,\n",
       " 'reform': 713,\n",
       " 'deliver': 262,\n",
       " 'list': 530,\n",
       " 'global': 384,\n",
       " 'confident': 201,\n",
       " 'press': 658,\n",
       " 'island': 473,\n",
       " 'dollar': 277,\n",
       " 'share': 796,\n",
       " 'week': 967,\n",
       " 'territory': 891,\n",
       " 'block': 95,\n",
       " 'beattie': 80,\n",
       " 'steal': 841,\n",
       " 'production': 668,\n",
       " 'league': 512,\n",
       " 'year': 993,\n",
       " 'cairns': 130,\n",
       " 'canberra': 133,\n",
       " 'brawl': 110,\n",
       " 'recall': 705,\n",
       " 'sink': 808,\n",
       " 'expand': 309,\n",
       " 'china': 163,\n",
       " 'africa': 19,\n",
       " 'residents': 730,\n",
       " 'sheep': 798,\n",
       " 'cattle': 148,\n",
       " 'export': 316,\n",
       " 'riot': 744,\n",
       " 'costello': 212,\n",
       " 'approve': 44,\n",
       " 'farm': 325,\n",
       " 'korea': 496,\n",
       " 'prove': 681,\n",
       " 'tigers': 902,\n",
       " 'chase': 158,\n",
       " 'complete': 194,\n",
       " 'tourist': 911,\n",
       " 'roads': 749,\n",
       " 'long': 535,\n",
       " 'dept': 265,\n",
       " 'rural': 760,\n",
       " 'news': 582,\n",
       " 'import': 446,\n",
       " 'great': 393,\n",
       " 'right': 743,\n",
       " 'cricket': 227,\n",
       " 'quarter': 687,\n",
       " 'horse': 432,\n",
       " 'hundreds': 440,\n",
       " 'indian': 451,\n",
       " 'indonesia': 453,\n",
       " 'travel': 921,\n",
       " 'industrial': 455,\n",
       " 'interview': 464,\n",
       " 'drown': 289,\n",
       " 'surf': 862,\n",
       " 'player': 633,\n",
       " 'pakistan': 611,\n",
       " 'return': 739,\n",
       " 'spark': 823,\n",
       " 'lake': 500,\n",
       " 'body': 100,\n",
       " 'sell': 786,\n",
       " 'bali': 72,\n",
       " 'life': 522,\n",
       " 'problems': 666,\n",
       " 'hunt': 441,\n",
       " 'city': 168,\n",
       " 'south': 820,\n",
       " 'forecast': 358,\n",
       " 'treatment': 922,\n",
       " 'coroner': 209,\n",
       " 'free': 366,\n",
       " 'prison': 661,\n",
       " 'suspend': 869,\n",
       " 'rugby': 758,\n",
       " 'super': 859,\n",
       " 'owners': 607,\n",
       " 'revamp': 740,\n",
       " 'comment': 185,\n",
       " 'care': 143,\n",
       " 'workers': 989,\n",
       " 'extend': 317,\n",
       " 'update': 939,\n",
       " 'rail': 694,\n",
       " 'weekend': 968,\n",
       " 'sector': 780,\n",
       " 'team': 884,\n",
       " 'round': 755,\n",
       " 'australias': 63,\n",
       " 'experts': 313,\n",
       " 'speed': 825,\n",
       " 'better': 86,\n",
       " 'milk': 562,\n",
       " 'shortage': 805,\n",
       " 'send': 788,\n",
       " 'best': 85,\n",
       " 'telstra': 889,\n",
       " 'timor': 904,\n",
       " 'drive': 284,\n",
       " 'contract': 206,\n",
       " 'gather': 377,\n",
       " 'researchers': 728,\n",
       " 'victoria': 947,\n",
       " 'confirm': 202,\n",
       " 'night': 583,\n",
       " 'propose': 676,\n",
       " 'game': 375,\n",
       " 'black': 91,\n",
       " 'teenager': 886,\n",
       " 'cancer': 135,\n",
       " 'figure': 339,\n",
       " 'reds': 710,\n",
       " 'title': 905,\n",
       " 'captain': 139,\n",
       " 'ship': 799,\n",
       " 'wheat': 974,\n",
       " 'assault': 52,\n",
       " 'bangladesh': 73,\n",
       " 'film': 341,\n",
       " 'eagle': 293,\n",
       " 'star': 836,\n",
       " 'leaders': 510,\n",
       " 'political': 642,\n",
       " 'japan': 478,\n",
       " 'fresh': 368,\n",
       " 'johnson': 481,\n",
       " 'hawk': 409,\n",
       " 'party': 615,\n",
       " 'beat': 79,\n",
       " 'hunter': 442,\n",
       " 'valley': 944,\n",
       " 'tour': 909,\n",
       " 'fall': 322,\n",
       " 'champion': 154,\n",
       " 'justice': 488,\n",
       " 'music': 575,\n",
       " 'hewitt': 418,\n",
       " 'killer': 492,\n",
       " 'ferry': 335,\n",
       " 'shop': 803,\n",
       " 'money': 569,\n",
       " 'volunteer': 953,\n",
       " 'save': 768,\n",
       " 'beach': 78,\n",
       " 'struggle': 849,\n",
       " 'alice': 26,\n",
       " 'discuss': 272,\n",
       " 'aussies': 58,\n",
       " 'lack': 499,\n",
       " 'brown': 118,\n",
       " 'beef': 81,\n",
       " 'shoot': 802,\n",
       " 'blue': 97,\n",
       " 'deadly': 249,\n",
       " 'grand': 390,\n",
       " 'crackdown': 221,\n",
       " 'criticise': 231,\n",
       " 'prompt': 673,\n",
       " 'swan': 871,\n",
       " 'bash': 76,\n",
       " 'swim': 872,\n",
       " 'federer': 332,\n",
       " 'pressure': 659,\n",
       " 'capture': 140,\n",
       " 'flow': 353,\n",
       " 'welfare': 970,\n",
       " 'perth': 621,\n",
       " 'director': 270,\n",
       " 'storm': 845,\n",
       " 'knight': 495,\n",
       " 'teacher': 882,\n",
       " 'tourists': 912,\n",
       " 'chopper': 165,\n",
       " 'role': 753,\n",
       " 'candidate': 136,\n",
       " 'level': 517,\n",
       " 'afghan': 17,\n",
       " 'grain': 389,\n",
       " 'alert': 25,\n",
       " 'shake': 795,\n",
       " 'cabinet': 129,\n",
       " 'budget': 119,\n",
       " 'blast': 93,\n",
       " 'media': 553,\n",
       " 'miners': 565,\n",
       " 'evidence': 308,\n",
       " 'coal': 180,\n",
       " 'boom': 104,\n",
       " 'property': 674,\n",
       " 'committee': 189,\n",
       " 'conference': 199,\n",
       " 'host': 434,\n",
       " 'drivers': 286,\n",
       " 'watch': 963,\n",
       " 'train': 918,\n",
       " 'abuse': 6,\n",
       " 'laws': 506,\n",
       " 'oppose': 601,\n",
       " 'squad': 831,\n",
       " 'heat': 414,\n",
       " 'historic': 424,\n",
       " 'hobart': 426,\n",
       " 'womens': 985,\n",
       " 'form': 362,\n",
       " 'uranium': 942,\n",
       " 'lawyer': 507,\n",
       " 'marine': 547,\n",
       " 'jones': 483,\n",
       " 'unit': 935,\n",
       " 'harbour': 407,\n",
       " 'pacific': 608,\n",
       " 'suspicious': 870,\n",
       " 'victim': 945,\n",
       " 'officer': 594,\n",
       " 'private': 662,\n",
       " 'queensland': 689,\n",
       " 'communities': 190,\n",
       " 'answer': 37,\n",
       " 'seize': 785,\n",
       " 'wall': 957,\n",
       " 'officials': 596,\n",
       " 'fiji': 340,\n",
       " 'town': 913,\n",
       " 'condition': 198,\n",
       " 'board': 98,\n",
       " 'wild': 977,\n",
       " 'half': 402,\n",
       " 'walk': 956,\n",
       " 'leader': 509,\n",
       " 'university': 937,\n",
       " 'southern': 821,\n",
       " 'closer': 176,\n",
       " 'latest': 504,\n",
       " 'success': 853,\n",
       " 'quake': 686,\n",
       " 'royal': 756,\n",
       " 'cruise': 237,\n",
       " 'sydney': 873,\n",
       " 'corruption': 210,\n",
       " 'allegations': 27,\n",
       " 'mackay': 542,\n",
       " 'response': 733,\n",
       " 'michael': 560,\n",
       " 'construction': 204,\n",
       " 'safe': 764,\n",
       " 'medical': 554,\n",
       " 'wednesday': 966,\n",
       " 'worry': 991,\n",
       " 'pass': 616,\n",
       " 'online': 599,\n",
       " 'explain': 314,\n",
       " 'rare': 699,\n",
       " 'commission': 186,\n",
       " 'rudd': 757,\n",
       " 'keen': 489,\n",
       " 'mother': 571,\n",
       " 'explosion': 315,\n",
       " 'hall': 403,\n",
       " 'crowd': 236,\n",
       " 'border': 106,\n",
       " 'admit': 15,\n",
       " 'count': 216,\n",
       " 'small': 812,\n",
       " 'terrorism': 893,\n",
       " 'transport': 919,\n",
       " 'couple': 218,\n",
       " 'whale': 973,\n",
       " 'children': 162,\n",
       " 'finish': 346,\n",
       " 'father': 329,\n",
       " 'bendigo': 83,\n",
       " 'deaths': 252,\n",
       " 'pool': 646,\n",
       " 'foreign': 359,\n",
       " 'islamic': 472,\n",
       " 'judge': 484,\n",
       " 'government': 387,\n",
       " 'resign': 731,\n",
       " 'custody': 238,\n",
       " 'official': 595,\n",
       " 'festival': 336,\n",
       " 'animal': 35,\n",
       " 'limit': 526,\n",
       " 'sexual': 794,\n",
       " 'halt': 404,\n",
       " 'convict': 208,\n",
       " 'roar': 750,\n",
       " 'breach': 111,\n",
       " 'townsville': 914,\n",
       " 'resume': 736,\n",
       " 'food': 356,\n",
       " 'heart': 413,\n",
       " 'resources': 732,\n",
       " 'abbott': 4,\n",
       " 'unveil': 938,\n",
       " 'memorial': 557,\n",
       " 'proposal': 675,\n",
       " 'russia': 761,\n",
       " 'chinese': 164,\n",
       " 'critical': 230,\n",
       " 'rethink': 737,\n",
       " 'term': 890,\n",
       " 'darling': 242,\n",
       " 'catch': 147,\n",
       " 'farmer': 326,\n",
       " 'refugee': 714,\n",
       " 'investment': 467,\n",
       " 'major': 543,\n",
       " 'rape': 698,\n",
       " 'survive': 867,\n",
       " 'accept': 8,\n",
       " 'problem': 665,\n",
       " 'remote': 722,\n",
       " 'scott': 774,\n",
       " 'brace': 109,\n",
       " 'strong': 848,\n",
       " 'company': 192,\n",
       " 'traffic': 917,\n",
       " 'baghdad': 70,\n",
       " 'video': 950,\n",
       " 'porn': 648,\n",
       " 'blow': 96,\n",
       " 'bird': 89,\n",
       " 'merger': 559,\n",
       " 'indonesian': 454,\n",
       " 'cyclone': 239,\n",
       " 'crow': 235,\n",
       " 'create': 223,\n",
       " 'base': 75,\n",
       " 'senate': 787,\n",
       " 'london': 534,\n",
       " 'street': 846,\n",
       " 'kick': 490,\n",
       " 'ponting': 645,\n",
       " 'fruit': 370,\n",
       " 'pipeline': 627,\n",
       " 'drum': 291,\n",
       " 'elderly': 302,\n",
       " 'chance': 155,\n",
       " 'bull': 121,\n",
       " 'extra': 318,\n",
       " 'mental': 558,\n",
       " 'olympics': 598,\n",
       " 'russian': 762,\n",
       " 'reserve': 729,\n",
       " 'tasmania': 879,\n",
       " 'bite': 90,\n",
       " 'lanka': 502,\n",
       " 'short': 804,\n",
       " 'commit': 188,\n",
       " 'package': 609,\n",
       " 'history': 425,\n",
       " 'patients': 617,\n",
       " 'happy': 406,\n",
       " 'september': 790,\n",
       " 'spring': 830,\n",
       " 'teens': 887,\n",
       " 'debt': 254,\n",
       " 'internet': 463,\n",
       " 'november': 586,\n",
       " 'israel': 474,\n",
       " 'escape': 307,\n",
       " 'forestry': 361,\n",
       " 'scheme': 770,\n",
       " 'parent': 612,\n",
       " 'holiday': 428,\n",
       " 'poor': 647,\n",
       " 'factory': 320,\n",
       " 'collapse': 183,\n",
       " 'approval': 43,\n",
       " 'bust': 128,\n",
       " 'reduce': 711,\n",
       " 'sight': 806,\n",
       " 'commissioner': 187,\n",
       " 'growers': 397,\n",
       " 'space': 822,\n",
       " 'possible': 650,\n",
       " 'bombers': 102,\n",
       " 'takeover': 876,\n",
       " 'bulldog': 122,\n",
       " 'anzac': 39,\n",
       " 'prize': 663,\n",
       " 'financial': 344,\n",
       " 'gang': 376,\n",
       " 'expansion': 310,\n",
       " 'energy': 305,\n",
       " 'june': 487,\n",
       " 'clarke': 170,\n",
       " 'compensation': 193,\n",
       " 'amid': 31,\n",
       " '2014': 1,\n",
       " 'april': 45,\n",
       " 'capital': 138,\n",
       " 'seekers': 784,\n",
       " 'thai': 895,\n",
       " 'hour': 436,\n",
       " 'social': 817,\n",
       " 'plea': 635,\n",
       " 'climate': 174,\n",
       " 'afghanistan': 18,\n",
       " 'data': 244,\n",
       " 'wallabies': 958,\n",
       " 'wife': 976,\n",
       " 'series': 791,\n",
       " 'outbreak': 606,\n",
       " 'asbestos': 49,\n",
       " 'john': 480,\n",
       " 'boys': 108,\n",
       " 'broncos': 117,\n",
       " 'throw': 900,\n",
       " 'peter': 622,\n",
       " 'bikie': 88,\n",
       " 'syria': 874,\n",
       " 'cast': 146,\n",
       " 'leak': 513,\n",
       " 'smith': 814,\n",
       " 'queen': 688,\n",
       " 'wrap': 992,\n",
       " 'bike': 87,\n",
       " 'mount': 572,\n",
       " 'monday': 568,\n",
       " 'outback': 605,\n",
       " 'july': 485,\n",
       " 'kimberley': 493,\n",
       " 'liberals': 520,\n",
       " 'grandstand': 391,\n",
       " 'origin': 604,\n",
       " 'iron': 471,\n",
       " 'turnbull': 931,\n",
       " 'october': 591,\n",
       " 'august': 56,\n",
       " 'trump': 927,\n",
       " 'george': 380,\n",
       " 'david': 246,\n",
       " 'tony': 907,\n",
       " 'tuesday': 929,\n",
       " 'carbon': 141,\n",
       " 'andrew': 32,\n",
       " 'gillard': 381,\n",
       " 'tsunami': 928,\n",
       " 'podcast': 638,\n",
       " '2013': 0,\n",
       " '2016': 3,\n",
       " '2015': 2,\n",
       " 'obama': 590,\n",
       " 'summary': 857}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_ \n",
    "# 뉴스기사의 제목들을 가져왔다. \n",
    "# 그리고 제목들의 불용어 제거, 동사를 1인칭으로 변환, 길이변환을 하였다.\n",
    "# 그리고 리스트안에 담겨져있던 자료들을 변환하기 위해 원래상태로 되돌렸다.\n",
    "# 즉 원래기사에서 불필요한 단어를 뺀 것이다. \n",
    "# TFIDF에서 함수를 집어넣은 것이다. \n",
    "# 여기까지가 토핑모델링의 준비단계인 것이다. \n",
    "# 문서에서 토픽이 무엇인지 정한만큼 각각의 문서가 어사인이 되어가는데 키워드가 무엇인지 뽑아내야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b1725e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation # 사이킷런에는 그게 구현이 되어져있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9818dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "# 이것에 대한 설명서이다.\n",
    "# 토픽의 갯수는 10개로 설정된 것이다. 가장 중요한 수치이다. \n",
    "# 교육메서드는 디폴트가 배치이다. 학습을 하는 과정에서 업데이트와 관련된 메서드이다. \n",
    "# 온라인이 좀 더 빠르다. 데이터가 큰 경우에 해당한다. 데이터가 적으면 배치가 좀 더 빠르다. \n",
    "# max iter는 에폭이라고 보면 된다. 성능을 높이고 싶다면 맥스이터를 높이게 된다. \n",
    "# n_job은 최대의 성능을 가지고 연산했으면 좋겠다면 -1로 설정하면 된다. 모든 cpu는 연산에 참여할 것이다. \n",
    "# 토픽은 10개이다. 1000개의 단어중에서 주요도가 높은 10개인 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7553bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1cff6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2013', '2014', '2015', '2016', 'abbott', 'aboriginal', 'abuse',\n",
       "       'accc', 'accept', 'access', 'accident', 'accuse', 'action',\n",
       "       'address', 'adelaide', 'admit', 'affect', 'afghan', 'afghanistan',\n",
       "       'africa', 'agree', 'agreement', 'ahead', 'airport', 'alcohol',\n",
       "       'alert', 'alice', 'allegations', 'allege', 'allow', 'ambulance',\n",
       "       'amid', 'andrew', 'anger', 'angry', 'animal', 'announce', 'answer',\n",
       "       'anti', 'anzac', 'apologise', 'appeal', 'appoint', 'approval',\n",
       "       'approve', 'april', 'army', 'arrest', 'arrive', 'asbestos', 'asia',\n",
       "       'asian', 'assault', 'asylum', 'attack', 'attempt', 'august',\n",
       "       'aussie', 'aussies', 'aust', 'australia', 'australian',\n",
       "       'australians', 'australias', 'authorities', 'avoid', 'await',\n",
       "       'award', 'away', 'baby', 'baghdad', 'bail', 'bali', 'bangladesh',\n",
       "       'bank', 'base', 'bash', 'battle', 'beach', 'beat', 'beattie',\n",
       "       'beef', 'begin', 'bendigo', 'benefit', 'best', 'better', 'bike',\n",
       "       'bikie', 'bird', 'bite', 'black', 'blame', 'blast', 'blaze',\n",
       "       'block', 'blow', 'blue', 'board', 'boat', 'body', 'bomb',\n",
       "       'bombers', 'book', 'boom', 'boost', 'border', 'boss', 'boys',\n",
       "       'brace', 'brawl', 'breach', 'break', 'bridge', 'bring', 'brisbane',\n",
       "       'british', 'broncos', 'brown', 'budget', 'build', 'bull',\n",
       "       'bulldog', 'burn', 'bush', 'bushfire', 'business', 'businesses',\n",
       "       'bust', 'cabinet', 'cairns', 'camp', 'campaign', 'canberra',\n",
       "       'cancel', 'cancer', 'candidate', 'cannabis', 'capital', 'captain',\n",
       "       'capture', 'carbon', 'card', 'care', 'case', 'cash', 'cast',\n",
       "       'catch', 'cattle', 'cause', 'celebrate', 'central', 'centre',\n",
       "       'challenge', 'champion', 'chance', 'change', 'charge', 'chase',\n",
       "       'check', 'chief', 'child', 'children', 'china', 'chinese',\n",
       "       'chopper', 'christmas', 'church', 'city', 'claim', 'clarke',\n",
       "       'clash', 'clean', 'clear', 'climate', 'close', 'closer', 'closure',\n",
       "       'club', 'coach', 'coal', 'coalition', 'coast', 'collapse', 'come',\n",
       "       'comment', 'commission', 'commissioner', 'commit', 'committee',\n",
       "       'communities', 'community', 'company', 'compensation', 'complete',\n",
       "       'compo', 'concern', 'condemn', 'condition', 'conference',\n",
       "       'confidence', 'confident', 'confirm', 'consider', 'construction',\n",
       "       'continue', 'contract', 'control', 'convict', 'coroner',\n",
       "       'corruption', 'cost', 'costello', 'council', 'councillor',\n",
       "       'councils', 'count', 'country', 'couple', 'court', 'crack',\n",
       "       'crackdown', 'crash', 'create', 'credit', 'creek', 'crew',\n",
       "       'cricket', 'crime', 'crisis', 'critical', 'criticise', 'criticism',\n",
       "       'crop', 'cross', 'crow', 'crowd', 'cruise', 'custody', 'cyclone',\n",
       "       'dairy', 'damage', 'darling', 'darwin', 'data', 'date', 'david',\n",
       "       'days', 'dead', 'deadly', 'deal', 'death', 'deaths', 'debate',\n",
       "       'debt', 'decide', 'decision', 'declare', 'defeat', 'defence',\n",
       "       'defend', 'delay', 'deliver', 'demand', 'deny', 'dept', 'despite',\n",
       "       'destroy', 'detention', 'development', 'director', 'disaster',\n",
       "       'discuss', 'disease', 'dismiss', 'dispute', 'doctor', 'dollar',\n",
       "       'domestic', 'double', 'doubt', 'downer', 'draw', 'drink', 'drive',\n",
       "       'driver', 'drivers', 'drop', 'drought', 'drown', 'drug', 'drum',\n",
       "       'dump', 'eagle', 'early', 'ease', 'east', 'economic', 'economy',\n",
       "       'edge', 'education', 'efforts', 'elderly', 'election', 'emergency',\n",
       "       'energy', 'england', 'escape', 'evidence', 'expand', 'expansion',\n",
       "       'expect', 'expert', 'experts', 'explain', 'explosion', 'export',\n",
       "       'extend', 'extra', 'face', 'factory', 'fail', 'fall', 'families',\n",
       "       'family', 'farm', 'farmer', 'farmers', 'fatal', 'father', 'fear',\n",
       "       'federal', 'federer', 'feed', 'feel', 'ferry', 'festival', 'field',\n",
       "       'fight', 'figure', 'fiji', 'film', 'final', 'finals', 'financial',\n",
       "       'fine', 'finish', 'firefighters', 'firm', 'fish', 'flag', 'flight',\n",
       "       'flood', 'flow', 'focus', 'follow', 'food', 'force', 'forecast',\n",
       "       'foreign', 'forest', 'forestry', 'form', 'forum', 'france',\n",
       "       'fraud', 'free', 'french', 'fresh', 'friday', 'fruit', 'fuel',\n",
       "       'fund', 'future', 'gain', 'game', 'gang', 'gather', 'gaza',\n",
       "       'general', 'george', 'gillard', 'gippsland', 'girl', 'global',\n",
       "       'gold', 'good', 'government', 'govt', 'grain', 'grand',\n",
       "       'grandstand', 'grant', 'great', 'green', 'group', 'grow',\n",
       "       'growers', 'growth', 'guard', 'guilty', 'hail', 'half', 'hall',\n",
       "       'halt', 'hand', 'happy', 'harbour', 'harvest', 'hawk', 'head',\n",
       "       'health', 'hear', 'heart', 'heat', 'heavy', 'help', 'heritage',\n",
       "       'hewitt', 'high', 'higher', 'highlight', 'highway', 'hill',\n",
       "       'historic', 'history', 'hobart', 'hold', 'holiday', 'home',\n",
       "       'honour', 'hope', 'horse', 'hospital', 'host', 'hotel', 'hour',\n",
       "       'house', 'howard', 'human', 'hundreds', 'hunt', 'hunter', 'hurt',\n",
       "       'illegal', 'impact', 'import', 'improve', 'incident', 'increase',\n",
       "       'india', 'indian', 'indigenous', 'indonesia', 'indonesian',\n",
       "       'industrial', 'industry', 'infrastructure', 'injure', 'injury',\n",
       "       'inquest', 'inquiry', 'international', 'internet', 'interview',\n",
       "       'investigate', 'investigation', 'investment', 'iran', 'iraq',\n",
       "       'iraqi', 'iron', 'islamic', 'island', 'israel', 'israeli', 'issue',\n",
       "       'jail', 'japan', 'japanese', 'john', 'johnson', 'join', 'jones',\n",
       "       'judge', 'july', 'jump', 'june', 'justice', 'keen', 'kick', 'kill',\n",
       "       'killer', 'kimberley', 'king', 'knight', 'korea', 'label', 'labor',\n",
       "       'lack', 'lake', 'land', 'lanka', 'late', 'latest', 'launch',\n",
       "       'laws', 'lawyer', 'lead', 'leader', 'leaders', 'leadership',\n",
       "       'league', 'leak', 'learn', 'leave', 'legal', 'level', 'levy',\n",
       "       'liberal', 'liberals', 'licence', 'life', 'lift', 'light',\n",
       "       'likely', 'limit', 'line', 'link', 'lions', 'list', 'live',\n",
       "       'lobby', 'local', 'london', 'long', 'look', 'loom', 'lose', 'loss',\n",
       "       'losses', 'lower', 'mackay', 'major', 'make', 'management',\n",
       "       'march', 'marine', 'mark', 'market', 'master', 'match', 'mayor',\n",
       "       'media', 'medical', 'meet', 'melbourne', 'memorial', 'mental',\n",
       "       'merger', 'michael', 'military', 'milk', 'million', 'miner',\n",
       "       'miners', 'minister', 'miss', 'monday', 'money', 'monitor',\n",
       "       'mother', 'mount', 'murder', 'murray', 'music', 'national',\n",
       "       'nationals', 'native', 'near', 'need', 'newcastle', 'news',\n",
       "       'night', 'north', 'northern', 'november', 'nuclear', 'number',\n",
       "       'nurse', 'obama', 'october', 'offer', 'office', 'officer',\n",
       "       'official', 'officials', 'olympic', 'olympics', 'online', 'open',\n",
       "       'oppose', 'opposition', 'order', 'origin', 'outback', 'outbreak',\n",
       "       'owners', 'pacific', 'package', 'pair', 'pakistan', 'parent',\n",
       "       'park', 'parliament', 'party', 'pass', 'patients', 'paul', 'peace',\n",
       "       'people', 'perth', 'peter', 'petrol', 'philippines', 'phone',\n",
       "       'pilot', 'pipeline', 'place', 'plan', 'plane', 'plant', 'play',\n",
       "       'player', 'players', 'plea', 'plead', 'pledge', 'podcast', 'point',\n",
       "       'police', 'policy', 'political', 'politics', 'poll', 'ponting',\n",
       "       'pool', 'poor', 'porn', 'port', 'possible', 'post', 'power',\n",
       "       'praise', 'predict', 'premier', 'prepare', 'president', 'press',\n",
       "       'pressure', 'price', 'prison', 'private', 'prize', 'probe',\n",
       "       'problem', 'problems', 'process', 'production', 'profit',\n",
       "       'program', 'project', 'promise', 'prompt', 'property', 'proposal',\n",
       "       'propose', 'protect', 'protection', 'protest', 'protesters',\n",
       "       'prove', 'public', 'pull', 'push', 'qantas', 'quake', 'quarter',\n",
       "       'queen', 'queensland', 'question', 'quit', 'race', 'raid', 'rail',\n",
       "       'rain', 'raise', 'rally', 'rape', 'rare', 'rate', 'reach', 'ready',\n",
       "       'real', 'rebel', 'recall', 'receive', 'record', 'recover',\n",
       "       'recovery', 'reds', 'reduce', 'reef', 'reform', 'refugee',\n",
       "       'refuse', 'regional', 'reject', 'release', 'relief', 'remain',\n",
       "       'remember', 'remote', 'reopen', 'report', 'reporter', 'rescue',\n",
       "       'research', 'researchers', 'reserve', 'residents', 'resign',\n",
       "       'resources', 'response', 'restrictions', 'result', 'resume',\n",
       "       'rethink', 'retire', 'return', 'revamp', 'reveal', 'review',\n",
       "       'right', 'riot', 'rise', 'risk', 'river', 'road', 'roads', 'roar',\n",
       "       'robbery', 'rock', 'role', 'roll', 'round', 'royal', 'rudd',\n",
       "       'rugby', 'rule', 'rural', 'russia', 'russian', 'sack', 'safe',\n",
       "       'safety', 'sale', 'sales', 'save', 'scare', 'scheme', 'school',\n",
       "       'scientists', 'score', 'scott', 'scrap', 'search', 'season',\n",
       "       'seat', 'second', 'sector', 'secure', 'security', 'seek',\n",
       "       'seekers', 'seize', 'sell', 'senate', 'send', 'sentence',\n",
       "       'september', 'series', 'service', 'seven', 'sexual', 'shake',\n",
       "       'share', 'shark', 'sheep', 'ship', 'shire', 'shock', 'shoot',\n",
       "       'shop', 'short', 'shortage', 'sight', 'sign', 'sink', 'site',\n",
       "       'slam', 'slow', 'small', 'smash', 'smith', 'smoke', 'socceroos',\n",
       "       'social', 'solar', 'soldier', 'south', 'southern', 'space',\n",
       "       'spark', 'speak', 'speed', 'spend', 'spill', 'sport', 'spot',\n",
       "       'spring', 'squad', 'stab', 'staff', 'stage', 'stand', 'star',\n",
       "       'start', 'state', 'station', 'stay', 'steal', 'step', 'stock',\n",
       "       'stop', 'storm', 'street', 'strike', 'strong', 'struggle',\n",
       "       'student', 'students', 'study', 'success', 'suffer', 'sugar',\n",
       "       'suicide', 'summary', 'summit', 'super', 'supply', 'support',\n",
       "       'surf', 'surge', 'surgery', 'surprise', 'survey', 'survive',\n",
       "       'suspect', 'suspend', 'suspicious', 'swan', 'swim', 'sydney',\n",
       "       'syria', 'tackle', 'takeover', 'talk', 'target', 'tasmania',\n",
       "       'tasmanian', 'taxi', 'teacher', 'teachers', 'team', 'teen',\n",
       "       'teenager', 'teens', 'tell', 'telstra', 'term', 'territory',\n",
       "       'terror', 'terrorism', 'test', 'thai', 'theft', 'thousands',\n",
       "       'threat', 'threaten', 'throw', 'thursday', 'tigers', 'time',\n",
       "       'timor', 'title', 'toll', 'tony', 'tough', 'tour', 'tourism',\n",
       "       'tourist', 'tourists', 'town', 'townsville', 'track', 'trade',\n",
       "       'traffic', 'train', 'transport', 'trap', 'travel', 'treatment',\n",
       "       'tree', 'trial', 'troop', 'truck', 'trump', 'tsunami', 'tuesday',\n",
       "       'turn', 'turnbull', 'underway', 'union', 'unions', 'unit', 'unite',\n",
       "       'university', 'unveil', 'update', 'upgrade', 'upset', 'uranium',\n",
       "       'urge', 'valley', 'victim', 'victims', 'victoria', 'victorian',\n",
       "       'victory', 'video', 'violence', 'visit', 'volunteer', 'vote',\n",
       "       'wait', 'walk', 'wall', 'wallabies', 'want', 'warn', 'warriors',\n",
       "       'waste', 'watch', 'water', 'weather', 'wednesday', 'week',\n",
       "       'weekend', 'welcome', 'welfare', 'west', 'western', 'whale',\n",
       "       'wheat', 'white', 'wife', 'wild', 'williams', 'wind', 'wine',\n",
       "       'witness', 'woes', 'woman', 'women', 'womens', 'wont', 'work',\n",
       "       'worker', 'workers', 'world', 'worry', 'wrap', 'year', 'years',\n",
       "       'young', 'youth', 'zealand', 'zimbabwe', 'zone'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms=vectorizer.get_feature_names_out()  # 모든 피쳐들이 나오게 된다. 1000개 가량이다.\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f60d392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00001533e-01, 1.00001269e-01, 1.00004179e-01, 1.00009783e-01,\n",
       "       1.00003277e-01, 1.00005085e-01, 1.00008228e-01, 1.00005611e-01,\n",
       "       1.00005647e-01, 1.00005891e-01, 1.00003852e-01, 1.00009243e-01,\n",
       "       2.21950370e+03, 1.00005136e-01, 1.00012732e-01, 1.00006707e-01,\n",
       "       1.00006758e-01, 1.00002416e-01, 1.00003071e-01, 1.00006781e-01,\n",
       "       1.00005235e-01, 1.00007553e-01, 1.00007442e-01, 1.00018055e-01,\n",
       "       1.00006315e-01, 1.00005805e-01, 1.00001334e-01, 1.00004811e-01,\n",
       "       1.00010464e-01, 1.00005187e-01, 5.68722498e+02, 1.00007276e-01,\n",
       "       1.00002793e-01, 1.00005508e-01, 1.00005284e-01, 1.00007886e-01,\n",
       "       1.00008474e-01, 1.00004393e-01, 1.00007032e-01, 1.00005811e-01,\n",
       "       1.10705722e+03, 1.00005659e-01, 1.00009460e-01, 1.00004993e-01,\n",
       "       1.00006201e-01, 1.00004432e-01, 1.00006869e-01, 3.93491018e+03,\n",
       "       1.00004439e-01, 1.00006127e-01, 1.00002532e-01, 1.00007665e-01,\n",
       "       1.00007779e-01, 1.00002561e-01, 1.00009086e-01, 1.32470301e+03,\n",
       "       1.00001940e-01, 1.24457881e+03, 1.00004953e-01, 1.00010636e-01,\n",
       "       1.00011883e-01, 1.00012006e-01, 1.00005896e-01, 1.00009714e-01,\n",
       "       1.00005043e-01, 1.00004833e-01, 1.00007366e-01, 1.00007767e-01,\n",
       "       1.00006193e-01, 1.00004646e-01, 1.00001459e-01, 1.00007333e-01,\n",
       "       1.00005805e-01, 1.00003900e-01, 3.27014925e+03, 1.00005085e-01,\n",
       "       1.00008372e-01, 1.00005530e-01, 2.17685570e+03, 1.00007129e-01,\n",
       "       1.00002219e-01, 1.00002345e-01, 1.94866808e+03, 1.00004519e-01,\n",
       "       1.00004416e-01, 1.00005631e-01, 1.00005198e-01, 1.00008650e-01,\n",
       "       6.09250309e+02, 1.00004119e-01, 6.97615733e+02, 1.00007545e-01,\n",
       "       1.00009667e-01, 1.00004687e-01, 1.00003675e-01, 1.00007838e-01,\n",
       "       1.00004445e-01, 1.00009406e-01, 1.00005813e-01, 1.00007171e-01,\n",
       "       1.00008829e-01, 1.00005406e-01, 1.00005395e-01, 1.00005575e-01,\n",
       "       1.00005483e-01, 1.00007744e-01, 1.00004477e-01, 1.00008117e-01,\n",
       "       1.00016027e-01, 1.00009225e-01, 1.00007595e-01, 1.00010167e-01,\n",
       "       1.00009553e-01, 1.00007900e-01, 1.00006766e-01, 1.00008224e-01,\n",
       "       1.00006828e-01, 1.00006682e-01, 1.00004448e-01, 1.00008654e-01,\n",
       "       1.00009551e-01, 1.00003932e-01, 1.00010210e-01, 1.00006359e-01,\n",
       "       1.00005366e-01, 1.00005628e-01, 1.00006413e-01, 1.00006606e-01,\n",
       "       1.00007788e-01, 1.00004732e-01, 1.00003498e-01, 1.00007864e-01,\n",
       "       1.00006915e-01, 1.00009358e-01, 1.00009070e-01, 1.00005508e-01,\n",
       "       1.00003463e-01, 1.00004291e-01, 1.00002799e-01, 1.00003660e-01,\n",
       "       1.00007227e-01, 1.00002434e-01, 1.00013691e-01, 1.00011244e-01,\n",
       "       1.00008619e-01, 1.00007621e-01, 1.00006272e-01, 1.00005196e-01,\n",
       "       1.00004982e-01, 1.00008643e-01, 1.00004788e-01, 1.82822735e+03,\n",
       "       1.00004404e-01, 1.00006314e-01, 1.00004282e-01, 1.00003762e-01,\n",
       "       5.87427366e+03, 1.00011903e-01, 1.00005327e-01, 1.00004429e-01,\n",
       "       1.96452594e+03, 1.00011981e-01, 1.00007397e-01, 1.00005706e-01,\n",
       "       1.00007593e-01, 1.00001467e-01, 1.00005664e-01, 1.00003015e-01,\n",
       "       1.00006986e-01, 1.00007858e-01, 1.00002810e-01, 1.00006043e-01,\n",
       "       1.00007414e-01, 1.00004505e-01, 2.20900707e+03, 1.00007603e-01,\n",
       "       1.00004956e-01, 1.00007073e-01, 1.96450444e+03, 1.00004958e-01,\n",
       "       1.00008092e-01, 1.00005339e-01, 1.00007221e-01, 1.00005762e-01,\n",
       "       1.00005745e-01, 1.00006117e-01, 1.00008988e-01, 1.00004875e-01,\n",
       "       1.00005312e-01, 1.00004297e-01, 1.00004522e-01, 1.00007013e-01,\n",
       "       1.00007525e-01, 1.00005520e-01, 1.00004516e-01, 1.00003565e-01,\n",
       "       1.00010161e-01, 1.00004129e-01, 1.26425836e+03, 1.00006115e-01,\n",
       "       1.00006351e-01, 1.00005420e-01, 1.00006535e-01, 1.00008837e-01,\n",
       "       1.00012710e-01, 1.00006045e-01, 1.09561384e+03, 1.00006643e-01,\n",
       "       1.00002853e-01, 1.00004444e-01, 1.00007667e-01, 1.00006438e-01,\n",
       "       1.00001326e-01, 1.00006782e-01, 1.00002079e-01, 1.00005380e-01,\n",
       "       1.00006907e-01, 1.00002707e-01, 1.00004890e-01, 1.00009486e-01,\n",
       "       1.00006082e-01, 1.00007467e-01, 1.00010433e-01, 1.00005729e-01,\n",
       "       1.00005633e-01, 1.30497769e+03, 1.00006909e-01, 2.07271258e+03,\n",
       "       1.00007953e-01, 1.00007327e-01, 1.00008450e-01, 1.00010772e-01,\n",
       "       1.00006182e-01, 1.00002980e-01, 1.00010060e-01, 1.00006221e-01,\n",
       "       1.00004783e-01, 1.00003344e-01, 1.00004973e-01, 1.00008886e-01,\n",
       "       1.00003265e-01, 1.00008267e-01, 1.00012981e-01, 1.00007898e-01,\n",
       "       1.00003771e-01, 1.00006677e-01, 2.68698566e+03, 1.00005256e-01,\n",
       "       1.00006883e-01, 1.00004761e-01, 1.00011527e-01, 1.00007310e-01,\n",
       "       1.00006749e-01, 1.00004853e-01, 1.00005366e-01, 5.56411599e+02,\n",
       "       1.53951710e+03, 1.00004223e-01, 7.70469195e+02, 1.00005137e-01,\n",
       "       1.00009395e-01, 1.00012986e-01, 1.00005173e-01, 1.00006041e-01,\n",
       "       1.00009202e-01, 1.00003961e-01, 1.00006201e-01, 1.00005541e-01,\n",
       "       1.00005887e-01, 1.00006637e-01, 1.00003315e-01, 1.00006000e-01,\n",
       "       1.00005550e-01, 1.00008396e-01, 1.00005258e-01, 1.00010973e-01,\n",
       "       1.00005877e-01, 1.74392391e+03, 1.00002683e-01, 1.00004251e-01,\n",
       "       1.00004424e-01, 1.00000542e-01, 1.00005420e-01, 1.00004307e-01,\n",
       "       1.00007014e-01, 1.00006236e-01, 1.00005891e-01, 1.00011184e-01,\n",
       "       1.00003908e-01, 1.00008063e-01, 1.00009575e-01, 1.00007854e-01,\n",
       "       1.00009885e-01, 1.00004440e-01, 1.00006847e-01, 1.00012130e-01,\n",
       "       1.00013785e-01, 1.00005278e-01, 1.00005876e-01, 1.00006813e-01,\n",
       "       1.00006596e-01, 1.00005907e-01, 1.00006250e-01, 1.00013889e-01,\n",
       "       1.00010417e-01, 1.00010669e-01, 1.00003908e-01, 1.00004529e-01,\n",
       "       1.00004019e-01, 1.00005468e-01, 1.00004028e-01, 1.00006477e-01,\n",
       "       1.00004024e-01, 1.00006053e-01, 1.00008215e-01, 1.00004272e-01,\n",
       "       1.00005969e-01, 1.00002897e-01, 1.00006308e-01, 1.00009662e-01,\n",
       "       1.00003355e-01, 2.11942654e+03, 1.00008290e-01, 1.00006493e-01,\n",
       "       1.00008172e-01, 1.00005001e-01, 1.00004776e-01, 1.00007117e-01,\n",
       "       1.00002989e-01, 1.00003430e-01, 1.00012705e-01, 1.00019911e-01,\n",
       "       4.72921320e+02, 6.68197253e+02, 7.65011845e+02, 1.00006925e-01,\n",
       "       1.00008165e-01, 1.00003109e-01, 1.00007393e-01, 6.33256334e+02,\n",
       "       1.49207633e+03, 1.00007333e-01, 3.65277978e+03, 1.00003693e-01,\n",
       "       1.00010404e-01, 1.00005007e-01, 1.00006773e-01, 1.00005502e-01,\n",
       "       1.00006162e-01, 1.00007235e-01, 1.00008501e-01, 1.00005650e-01,\n",
       "       1.00010866e-01, 1.00004691e-01, 1.00008486e-01, 1.00006909e-01,\n",
       "       1.00006674e-01, 1.00011319e-01, 1.00007692e-01, 1.00015018e-01,\n",
       "       1.00003617e-01, 1.00003328e-01, 1.00003815e-01, 1.00003082e-01,\n",
       "       1.00003304e-01, 1.00009043e-01, 1.00004387e-01, 1.00005635e-01,\n",
       "       1.00003400e-01, 1.00000816e-01, 1.00004676e-01, 1.00003429e-01,\n",
       "       1.00010103e-01, 1.00007209e-01, 1.00006415e-01, 1.00005910e-01,\n",
       "       1.00008126e-01, 1.00004816e-01, 1.00003019e-01, 1.00007096e-01,\n",
       "       1.00005171e-01, 1.00001477e-01, 1.00003925e-01, 1.00006331e-01,\n",
       "       1.00005612e-01, 1.00004318e-01, 1.00004025e-01, 8.72518642e+03,\n",
       "       1.00019450e-01, 1.00003842e-01, 1.00015389e-01, 1.00002139e-01,\n",
       "       1.00010469e-01, 1.00004534e-01, 1.00013080e-01, 1.00008028e-01,\n",
       "       1.00004246e-01, 9.73820011e+02, 1.00005135e-01, 1.00004343e-01,\n",
       "       1.00004428e-01, 1.00006981e-01, 6.88971272e+02, 1.00005460e-01,\n",
       "       1.00004798e-01, 1.00006008e-01, 1.00005398e-01, 9.67301269e+02,\n",
       "       1.00006012e-01, 1.00004253e-01, 1.00007500e-01, 1.00006648e-01,\n",
       "       1.00006600e-01, 1.00005012e-01, 1.00007676e-01, 1.00009869e-01,\n",
       "       1.00008831e-01, 1.00004232e-01, 1.00003172e-01, 1.00009906e-01,\n",
       "       1.00007695e-01, 1.00004074e-01, 1.00002595e-01, 1.00005544e-01,\n",
       "       1.00004765e-01, 1.00004882e-01, 1.00011346e-01, 2.89112888e+03,\n",
       "       1.00005729e-01, 5.67438229e+03, 1.00007034e-01, 1.00005984e-01,\n",
       "       1.00003884e-01, 1.00007212e-01, 1.00003662e-01, 1.00003295e-01,\n",
       "       1.00001200e-01, 1.00009479e-01, 1.00005425e-01, 1.17869691e+03,\n",
       "       1.00005498e-01, 1.00006627e-01, 1.00003606e-01, 6.85862161e+02,\n",
       "       1.00005491e-01, 1.18137405e+03, 1.00004596e-01, 1.00010306e-01,\n",
       "       7.97717019e+02, 1.00008034e-01, 1.00006970e-01, 1.00005837e-01,\n",
       "       1.00007603e-01, 1.00004239e-01, 1.00003931e-01, 5.60556783e+02,\n",
       "       1.00007116e-01, 1.00008766e-01, 1.00005066e-01, 1.00003449e-01,\n",
       "       1.00009138e-01, 1.00009045e-01, 1.00013644e-01, 1.00005676e-01,\n",
       "       1.00001322e-01, 1.00005691e-01, 1.76253776e+03, 9.42757758e+02,\n",
       "       5.62984850e+02, 1.00002096e-01, 4.95890115e+02, 1.00002605e-01,\n",
       "       1.00005728e-01, 1.00005323e-01, 1.00010592e-01, 1.00003592e-01,\n",
       "       1.00008328e-01, 1.00006633e-01, 1.00004277e-01, 1.00005336e-01,\n",
       "       1.00004561e-01, 1.00004094e-01, 1.00005146e-01, 1.00005868e-01,\n",
       "       1.00006603e-01, 1.00001866e-01, 1.00006313e-01, 1.00006052e-01,\n",
       "       1.00010027e-01, 1.00003066e-01, 1.00007433e-01, 1.00005745e-01,\n",
       "       1.00004186e-01, 1.00003672e-01, 1.00003088e-01, 1.00002741e-01,\n",
       "       1.00005492e-01, 1.00002804e-01, 1.00008075e-01, 1.00006814e-01,\n",
       "       1.00002922e-01, 1.00009095e-01, 1.00003921e-01, 1.00005721e-01,\n",
       "       1.00007005e-01, 1.00007321e-01, 1.00018317e-01, 1.00004458e-01,\n",
       "       1.00007513e-01, 1.00005794e-01, 1.00004720e-01, 1.00004520e-01,\n",
       "       1.00013814e-01, 1.00009971e-01, 1.00007753e-01, 1.00010385e-01,\n",
       "       1.00011280e-01, 1.00006708e-01, 6.51255442e+02, 1.00003465e-01,\n",
       "       1.00004617e-01, 5.73955998e+02, 1.00006823e-01, 1.00005684e-01,\n",
       "       2.15218301e+03, 1.00006230e-01, 1.00007010e-01, 1.48056749e+03,\n",
       "       1.00006173e-01, 1.00005689e-01, 1.00003029e-01, 5.48861811e+03,\n",
       "       1.00007436e-01, 1.00006948e-01, 1.00006704e-01, 1.00005226e-01,\n",
       "       1.00011627e-01, 5.90472486e+02, 1.00007457e-01, 1.00004922e-01,\n",
       "       1.00004965e-01, 1.00005506e-01, 1.00005558e-01, 1.00011767e-01,\n",
       "       1.00014598e-01, 7.25544705e+02, 1.00005573e-01, 1.00010684e-01,\n",
       "       1.00005490e-01, 1.00008485e-01, 1.00005587e-01, 1.00007185e-01,\n",
       "       1.00004306e-01, 1.00003023e-01, 1.00005779e-01, 1.00009560e-01,\n",
       "       1.00010769e-01, 1.00003530e-01, 1.00002503e-01, 1.00005089e-01,\n",
       "       1.00004188e-01, 1.00005587e-01, 1.00002103e-01, 1.00011968e-01,\n",
       "       1.00004506e-01, 1.00005792e-01, 1.00006583e-01, 1.00009158e-01,\n",
       "       1.00000296e-01, 1.00005374e-01, 1.00005264e-01, 1.00006686e-01,\n",
       "       1.00004699e-01, 1.00011170e-01, 1.00015054e-01, 1.39661493e+03,\n",
       "       1.00005344e-01, 1.00004550e-01, 1.00004057e-01, 1.00008980e-01,\n",
       "       1.00012133e-01, 1.00005280e-01, 1.00002878e-01, 1.00008669e-01,\n",
       "       1.00010347e-01, 1.00004999e-01, 1.00005240e-01, 1.00020824e-01,\n",
       "       1.00007318e-01, 1.32179602e+03, 1.00005144e-01, 1.00004757e-01,\n",
       "       1.00005834e-01, 1.00004249e-01, 1.00009686e-01, 6.71508900e+02,\n",
       "       1.00005434e-01, 1.00005773e-01, 1.00003382e-01, 1.00004376e-01,\n",
       "       1.00008096e-01, 1.00005440e-01, 1.00010242e-01, 1.00007314e-01,\n",
       "       1.00003563e-01, 1.00008041e-01, 1.00003259e-01, 1.00007685e-01,\n",
       "       1.00003599e-01, 1.00003804e-01, 1.00003468e-01, 1.00007339e-01,\n",
       "       1.00005301e-01, 1.00004905e-01, 1.00007878e-01, 1.00007563e-01,\n",
       "       1.00007373e-01, 1.00007089e-01, 1.00004531e-01, 1.00002993e-01,\n",
       "       1.00008271e-01, 1.00009552e-01, 1.00003627e-01, 1.00003347e-01,\n",
       "       1.00004069e-01, 1.00005571e-01, 1.00015048e-01, 1.00004614e-01,\n",
       "       1.00005464e-01, 1.00016824e-01, 1.62948838e+03, 1.00005908e-01,\n",
       "       1.00006196e-01, 9.21384620e+02, 1.00006585e-01, 1.00006027e-01,\n",
       "       1.00002206e-01, 1.00005656e-01, 1.00000376e-01, 1.91462233e+03,\n",
       "       1.00015313e-01, 1.00010784e-01, 1.00005089e-01, 1.00011503e-01,\n",
       "       1.00007217e-01, 5.46004837e+01, 1.00004347e-01, 1.00005737e-01,\n",
       "       1.00003326e-01, 1.00005748e-01, 1.00007459e-01, 1.00004489e-01,\n",
       "       1.00008682e-01, 1.00008052e-01, 1.00006161e-01, 1.00005794e-01,\n",
       "       1.00007275e-01, 1.00004080e-01, 1.00010723e-01, 1.00007067e-01,\n",
       "       1.00005726e-01, 1.00006396e-01, 1.00008162e-01, 1.00002346e-01,\n",
       "       1.00005614e-01, 1.00003699e-01, 6.89283972e+02, 1.00004018e-01,\n",
       "       1.00005014e-01, 1.00009438e-01, 1.00009522e-01, 1.00007898e-01,\n",
       "       1.00004783e-01, 1.14833137e+03, 1.00005944e-01, 1.00006114e-01,\n",
       "       1.00011359e-01, 1.00006856e-01, 1.00007449e-01, 1.00010378e-01,\n",
       "       1.00005507e-01, 1.00004105e-01, 1.00010858e-01, 1.00003424e-01,\n",
       "       1.00008733e-01, 1.00004106e-01, 1.00002219e-01, 1.00003000e-01,\n",
       "       1.00002386e-01, 7.72011746e+03, 1.00007749e-01, 1.00006542e-01,\n",
       "       1.00007034e-01, 1.00013378e-01, 1.00020106e-01, 1.00007474e-01,\n",
       "       1.00006745e-01, 1.00006426e-01, 1.00006403e-01, 1.18185386e+03,\n",
       "       1.00011604e-01, 1.00009966e-01, 1.00005382e-01, 1.00005841e-01,\n",
       "       1.00006535e-01, 1.00003054e-01, 1.00005868e-01, 1.00014395e-01,\n",
       "       1.00003804e-01, 1.00005869e-01, 1.00007811e-01, 1.00006986e-01,\n",
       "       1.00004927e-01, 1.00004448e-01, 1.00005429e-01, 1.00005541e-01,\n",
       "       1.00009034e-01, 1.00007039e-01, 1.00011309e-01, 1.00005155e-01,\n",
       "       1.00009108e-01, 1.00006347e-01, 1.00010347e-01, 1.00005774e-01,\n",
       "       1.00009673e-01, 1.00001821e-01, 1.00005347e-01, 1.00005186e-01,\n",
       "       1.00005467e-01, 9.49566890e+02, 1.00008354e-01, 1.00006137e-01,\n",
       "       1.00003630e-01, 1.00007380e-01, 1.00003755e-01, 1.00005789e-01,\n",
       "       1.00002617e-01, 1.00005463e-01, 1.00008439e-01, 1.00008644e-01,\n",
       "       1.00005497e-01, 1.00006020e-01, 2.85573571e+03, 1.00014191e-01,\n",
       "       1.00004221e-01, 1.00011911e-01, 1.00010525e-01, 2.41203082e+03,\n",
       "       1.00008071e-01, 1.00003193e-01, 1.00004245e-01, 1.00005070e-01,\n",
       "       1.00008004e-01, 1.00002985e-01, 1.00004399e-01, 1.00007003e-01,\n",
       "       1.00007303e-01, 1.00001654e-01, 1.00009083e-01, 2.55295074e+03,\n",
       "       1.00001600e-01, 1.00005777e-01, 1.00004957e-01, 1.00006282e-01,\n",
       "       1.00008595e-01, 1.76976381e+03, 1.00003713e-01, 1.00006680e-01,\n",
       "       1.00009560e-01, 1.00004038e-01, 1.00005276e-01, 1.00014798e-01,\n",
       "       1.00010217e-01, 1.00005170e-01, 1.00003883e-01, 1.00007475e-01,\n",
       "       1.00007253e-01, 1.00006025e-01, 1.00004850e-01, 1.00006982e-01,\n",
       "       6.86472064e+02, 7.18029838e+02, 1.00007686e-01, 1.00007832e-01,\n",
       "       1.00001278e-01, 9.93724417e+02, 1.00006151e-01, 1.00006726e-01,\n",
       "       1.00007132e-01, 1.00005820e-01, 1.00002110e-01, 1.19901555e+03,\n",
       "       1.00009147e-01, 1.00007364e-01, 1.00005595e-01, 1.00005083e-01,\n",
       "       1.00008322e-01, 1.00014433e-01, 1.00003568e-01, 1.00006248e-01,\n",
       "       1.00003435e-01, 1.00007708e-01, 1.00007790e-01, 1.00004678e-01,\n",
       "       1.00006863e-01, 1.00006560e-01, 1.00007776e-01, 1.00006358e-01,\n",
       "       1.00003195e-01, 1.00004468e-01, 1.00012488e-01, 1.00009304e-01,\n",
       "       1.00008290e-01, 1.00003115e-01, 1.00004542e-01, 6.76836974e+02,\n",
       "       1.00003417e-01, 1.00004652e-01, 1.00004320e-01, 1.00002730e-01,\n",
       "       1.00010534e-01, 1.00005802e-01, 1.00005069e-01, 1.00006060e-01,\n",
       "       1.00002926e-01, 1.00005309e-01, 1.00006897e-01, 1.00007284e-01,\n",
       "       2.26519687e+03, 1.00009322e-01, 1.00002356e-01, 1.00004033e-01,\n",
       "       1.00007073e-01, 1.00016250e-01, 1.00003488e-01, 1.00006504e-01,\n",
       "       1.00006536e-01, 1.00007511e-01, 1.00007643e-01, 1.00006168e-01,\n",
       "       1.00006978e-01, 1.00005731e-01, 1.00006488e-01, 1.00006506e-01,\n",
       "       2.06216592e+03, 1.00010560e-01, 1.00004474e-01, 1.00010581e-01,\n",
       "       1.00004923e-01, 1.00005408e-01, 1.16304544e+03, 2.72672652e+03,\n",
       "       1.00010793e-01, 1.00005390e-01, 1.00005317e-01, 1.00004749e-01,\n",
       "       1.00005987e-01, 1.00000443e-01, 1.00005284e-01, 1.00009565e-01,\n",
       "       1.00005148e-01, 1.00006963e-01, 1.00006454e-01, 1.00009986e-01,\n",
       "       1.00005118e-01, 1.00005631e-01, 1.00005251e-01, 1.00005346e-01,\n",
       "       1.00006049e-01, 1.25043775e+03, 1.00002135e-01, 1.00010439e-01,\n",
       "       1.00005458e-01, 8.39329070e+03, 1.00004229e-01, 1.00006081e-01,\n",
       "       1.00005496e-01, 1.00010475e-01, 1.00006097e-01, 1.00007032e-01,\n",
       "       1.00012418e-01, 1.00006579e-01, 8.54968801e+02, 4.74076177e+02,\n",
       "       1.00008369e-01, 1.00007817e-01, 1.00004755e-01, 1.00005129e-01,\n",
       "       1.00007652e-01, 3.72405150e+02, 1.00006952e-01, 1.00005222e-01,\n",
       "       1.00006150e-01, 1.00008761e-01, 1.00015486e-01, 1.00006130e-01,\n",
       "       1.00005256e-01, 1.00007211e-01, 1.00007749e-01, 1.00010298e-01,\n",
       "       1.00003299e-01, 1.00000504e-01, 1.00007148e-01, 1.00006990e-01,\n",
       "       1.00004477e-01, 1.00004317e-01, 1.00003262e-01, 1.00002385e-01,\n",
       "       1.00008297e-01, 1.00004990e-01, 1.00003622e-01, 1.00003073e-01,\n",
       "       1.00004505e-01, 1.00006675e-01, 1.00003626e-01, 1.00006115e-01,\n",
       "       1.00005917e-01, 1.00007000e-01, 1.00009053e-01, 1.00005093e-01,\n",
       "       1.00003361e-01, 1.00007142e-01, 1.00005901e-01, 1.00005639e-01,\n",
       "       1.00010612e-01, 1.00007794e-01, 1.00002479e-01, 1.00011601e-01,\n",
       "       1.00008272e-01, 1.44622973e+03, 1.00005317e-01, 1.00007695e-01,\n",
       "       7.75546476e+02, 1.66920961e+03, 1.00004774e-01, 1.00003164e-01,\n",
       "       1.00005812e-01, 1.00010895e-01, 1.00005596e-01, 1.00006516e-01,\n",
       "       1.00004694e-01, 1.00005974e-01, 2.68810187e+02, 1.00016575e-01,\n",
       "       1.00002848e-01, 1.00004217e-01, 1.00006111e-01, 1.00009328e-01,\n",
       "       1.00012967e-01, 1.00007620e-01, 1.00009712e-01, 1.00004047e-01,\n",
       "       1.00005084e-01, 1.00004393e-01, 2.79338503e+03, 1.00007191e-01,\n",
       "       1.00005840e-01, 1.00004789e-01, 1.00003619e-01, 1.00007750e-01,\n",
       "       1.00013760e-01, 1.00011052e-01, 1.32932654e+03, 1.00005824e-01,\n",
       "       1.00008809e-01, 2.20617465e+03, 1.00000639e-01, 1.00010154e-01,\n",
       "       1.00008130e-01, 1.00009226e-01, 8.00423365e+02, 1.00009112e-01,\n",
       "       1.00009826e-01, 1.00008421e-01, 1.00003830e-01, 1.00006492e-01,\n",
       "       1.00002919e-01, 1.30502320e+03, 1.00003515e-01, 1.00009830e-01,\n",
       "       1.00005962e-01, 1.00005736e-01, 1.00002806e-01, 1.00007487e-01,\n",
       "       1.00005758e-01, 1.00012914e-01, 2.45302012e+02, 1.00009814e-01,\n",
       "       1.27013248e+03, 1.00008355e-01, 1.00009339e-01, 1.00007786e-01,\n",
       "       1.00003256e-01, 5.58642204e+03, 1.00006381e-01, 1.00005905e-01,\n",
       "       1.00006945e-01, 1.00006124e-01, 1.00003111e-01, 1.00003064e-01])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_[0]        # 단어들의 중요도이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7bf86e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(comp, fn, n=5):\n",
    "    for idx, topic in enumerate(comp):\n",
    "        print(\"Topic %d:\" % (idx+1), [(fn[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "#         print(idx, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "32e96eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\n",
      "Topic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\n",
      "Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\n",
      "Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\n",
      "Topic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\n",
      "Topic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\n",
      "Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\n",
      "Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\n",
      "Topic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\n",
      "Topic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\n"
     ]
    }
   ],
   "source": [
    "get_topics(lda_model.components_, terms) # 토픽 상위 5개를 출력한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79ba27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공개된 소설이나 책을 이용하여 주제를 파악하는것에 유용하다. \n",
    "# 데이터를 준비하여 LDA를 적용하여 나름대로 요약하는 것이다. \n",
    "# 각 주제별로 준비하는 것이다. \n",
    "# 한국어로 토크나이즈를 한다면 명사, 동사를 기준으로 나누는 작업을 해 줘야 할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dbc53699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI는 두 문장의 유사성을 확인하여 관련이 있는지 모순되는지 중립인지를 확인하는 것이다.\n",
    "# 한국어 데이터셋이 있다. tpu를 써야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c2fd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f4fb167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('snli_1.0_train.ko.tsv', <http.client.HTTPMessage at 0x1f931167790>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\", filename=\"multinli.train.ko.tsv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\", filename=\"snli_1.0_train.ko.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8ff96c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xnli.dev.ko.tsv', <http.client.HTTPMessage at 0x1f931182ed0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\", filename=\"xnli.dev.ko.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0d8568ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xnli.test.ko.tsv', <http.client.HTTPMessage at 0x1f92ef6d110>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\", filename=\"xnli.test.ko.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a49ef4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep='\\t', quoting=3)\n",
    "train_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep='\\t', quoting=3)\n",
    "val_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep='\\t', quoting=3)\n",
    "test_data = pd.read_csv(\"xnli.test.ko.tsv\", sep='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87d1822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
       "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
       "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
       "      <td>아이들이 있다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sentence1                  sentence2     gold_label\n",
       "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
       "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
       "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment\n",
       "3          카메라에 웃고 손을 흔드는 아이들          그들은 부모님을 보고 웃고 있다        neutral\n",
       "4          카메라에 웃고 손을 흔드는 아이들                    아이들이 있다     entailment"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snli.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "02d9783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.</td>\n",
       "      <td>제품과 지리학은 크림 스키밍을 작동시키는 것이다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...</td>\n",
       "      <td>사람들이 기억하면 다음 수준으로 물건을 잃는다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.</td>\n",
       "      <td>우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어떻게 아세요? 이 모든 것이 다시 그들의 정보다.</td>\n",
       "      <td>이 정보는 그들의 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...</td>\n",
       "      <td>테니스화의 가격은 다양하다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0         개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.   \n",
       "1  시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...   \n",
       "2                  우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.   \n",
       "3                       어떻게 아세요? 이 모든 것이 다시 그들의 정보다.   \n",
       "4  그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...   \n",
       "\n",
       "                              sentence2  gold_label  \n",
       "0           제품과 지리학은 크림 스키밍을 작동시키는 것이다.     neutral  \n",
       "1            사람들이 기억하면 다음 수준으로 물건을 잃는다.  entailment  \n",
       "2  우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.  entailment  \n",
       "3                        이 정보는 그들의 것이다.  entailment  \n",
       "4                       테니스화의 가격은 다양하다.     neutral  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xnli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb80dad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 결합 후 섞기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_snli\u001b[38;5;241m.\u001b[39mappend(train_xnli)\n\u001b[0;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# 결합 후 섞기\n",
    "train_data = train_snli.append(train_xnli)\n",
    "train_data = train_data.sample(frac=1)\n",
    "# 콘트레디션은 서로 상반되는 것들이다.\n",
    "# 엔테일먼트가 관련있는 것이고 뉴트럴은 애매한 경우이다.\n",
    "# 이 자료들은 영국의 자료를 한국어로 번역한 것이다. 그래서 애매한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3376826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_and_duplciates(df):   # 중복제거하기\n",
    "  df = df.dropna()\n",
    "  df = df.drop_duplicates()\n",
    "  df = df.reset_index(drop=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b00234cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m훈련용 샘플 개수 :\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(train_data))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m검증용 샘플 개수 :\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(val_data))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m테스트용 샘플 개수 :\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(test_data))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print('훈련용 샘플 개수 :',len(train_data))\n",
    "print('검증용 샘플 개수 :',len(val_data))\n",
    "print('테스트용 샘플 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e82572",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "max_seq_len = 128\n",
    "sent1 = train_data['sentence1'].iloc[0]\n",
    "sent2 = train_data['sentence2'].iloc[0]\n",
    "\n",
    "print(sent1)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_result = tokenizer.encode_plus(sent1, sent2, max_length=max_seq_len, pad_to_max_length=True)\n",
    "print(encoding_result['input_ids'])\n",
    "# 어제하고 다른 점은 문장이 두 개가 들어간다는 점이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3eb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding_result['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73638472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list1, sent_list2, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "\n",
    "    for sent1, sent2 in tqdm(zip(sent_list1, sent_list2), total=len(sent_list1)):\n",
    "        encoding_result = tokenizer.encode_plus(sent1, sent2, max_length=max_seq_len, pad_to_max_length=True)\n",
    "\n",
    "        input_ids.append(encoding_result['input_ids'])\n",
    "        attention_masks.append(encoding_result['attention_mask'])\n",
    "        token_type_ids.append(encoding_result['token_type_ids'])\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95291425",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_examples_to_features(train_data['sentence1'], train_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_train[0][0]\n",
    "attention_mask = X_train[1][0]\n",
    "token_type_id = X_train[2][0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f089c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = convert_examples_to_features(val_data['sentence1'], val_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_val[0][0]\n",
    "attention_mask = X_val[1][0]\n",
    "token_type_id = X_val[2][0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = convert_examples_to_features(test_data['sentence1'], test_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbaa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data['gold_label'].tolist()\n",
    "val_label = val_data['gold_label'].tolist()\n",
    "test_label = test_data['gold_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_encode = preprocessing.LabelEncoder()\n",
    "idx_encode.fit(train_label)\n",
    "\n",
    "y_train = idx_encode.transform(train_label) # 주어진 고유한 정수로 변환\n",
    "y_val = idx_encode.transform(val_label) # 고유한 정수로 변환\n",
    "y_test = idx_encode.transform(test_label) # 고유한 정수로 변환\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
    "idx_label = {value: key for key, value in label_idx.items()}\n",
    "print(label_idx)\n",
    "print(idx_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3db677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TFBertForSequenceClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU 작동을 위한 코드 TPU 작동을 위한 코드\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f475c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = TFBertForSequenceClassification(\"klue/bert-base\", num_labels=3)\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
    "  early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001,\n",
    "    patience=2)\n",
    "\n",
    "  model.fit(\n",
    "    X_train, y_train, epochs=5, batch_size=32, validation_data = (X_val, y_val),\n",
    "    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80138874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test, batch_size=1024)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475329f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0be70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
